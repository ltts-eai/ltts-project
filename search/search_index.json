{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home page Welcome to EAI Project documentation website. Here at our documentation website, we aim to provide you with comprehensive information and resources related to embedded AI, frameworks, optimizations, and applications. Whether you are a developer, researcher, or enthusiast, you will find something useful here to enhance your understanding and skills. About Embedded AI Embedded AI refers to the integration of artificial intelligence algorithms and models into embedded systems such as microcontrollers, FPGAs, and SoCs. It enables intelligent decision-making and autonomous operation of the embedded devices without relying on cloud or network connectivity. Frameworks and Libraries We cover a wide range of frameworks and libraries for embedded AI development, including TensorFlow Lite, Keras, PyTorch, and more. These tools help simplify and accelerate the process of building and deploying AI models on embedded devices. Optimizations Optimizations are crucial for embedded AI applications, as they help improve the efficiency, speed, and accuracy of the models. We provide guidance on various optimization techniques such as quantization, pruning, compression, and hardware acceleration. Applications Embedded AI has a wide range of applications, from smart homes and IoT devices to healthcare and automotive. We showcase real-world examples of embedded AI applications and provide insights into their design, development, and deployment. (in progress) Hardware study (in progress)","title":"Home page"},{"location":"#home-page","text":"Welcome to EAI Project documentation website. Here at our documentation website, we aim to provide you with comprehensive information and resources related to embedded AI, frameworks, optimizations, and applications. Whether you are a developer, researcher, or enthusiast, you will find something useful here to enhance your understanding and skills.","title":"Home page"},{"location":"#about-embedded-ai","text":"Embedded AI refers to the integration of artificial intelligence algorithms and models into embedded systems such as microcontrollers, FPGAs, and SoCs. It enables intelligent decision-making and autonomous operation of the embedded devices without relying on cloud or network connectivity.","title":"About Embedded AI"},{"location":"#frameworks-and-libraries","text":"We cover a wide range of frameworks and libraries for embedded AI development, including TensorFlow Lite, Keras, PyTorch, and more. These tools help simplify and accelerate the process of building and deploying AI models on embedded devices.","title":"Frameworks and Libraries"},{"location":"#optimizations","text":"Optimizations are crucial for embedded AI applications, as they help improve the efficiency, speed, and accuracy of the models. We provide guidance on various optimization techniques such as quantization, pruning, compression, and hardware acceleration.","title":"Optimizations"},{"location":"#applications","text":"Embedded AI has a wide range of applications, from smart homes and IoT devices to healthcare and automotive. We showcase real-world examples of embedded AI applications and provide insights into their design, development, and deployment. (in progress)","title":"Applications"},{"location":"#hardware-study","text":"(in progress)","title":"Hardware study"},{"location":"page01/","text":"Introduction to AI What is Artificial Intelligence : Artificial intelligence is a branch of computer science concerned with creating machines that can think and make decisions independently of human intervention. Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition and machine vision. What is AI Algorithms? : The definition of an algorithm is \u201ca set of instructions to be followed in calculations or other operations.\u201d This applies to both mathematics and computer science. So, at the essential level, an AI algorithm is the programming that tells the computer how to learn to operate on its own. An AI algorithm is much more complex than what most people learn about in algebra, of course. A complex set of rules drive AI programs, determining their steps and their ability to learn. Without an algorithm, AI wouldn\u2019t exist. How does a AI algorithm work? : While a general algorithm can be simple, AI algorithms are by nature more complex. AI algorithms work by taking in training data that helps the algorithm to learn. How that data is acquired and is labeled marks the key difference between different types of AI algorithms. AI requires a foundation of specialized hardware and software for writing and training machine learning algorithms. No single programming language is synonymous with AI, but Python, R, Java, C++ and Julia have features popular with AI developers. At the core level, an AI algorithm takes in training data (labeled or unlabeled, supplied by developers, or acquired by the program itself) and uses that information to learn and grow. Then it completes its tasks, using the training data as a basis. Some types of AI algorithms can be taught to learn on their own and take in new data to change and refine their process. Cognitive skills that are more focused in AI Programming: LEARNING: This aspect of AI programming focuses on acquiring data and creating rules for how to turn it into actionable information. The rules, which are called algorithms, provide computing devices with step-by-step instructions for how to complete a specific task. REASONING: This aspect of AI programming focuses on choosing the right algorithm to reach a desired outcome. SELF-CORRECTION: This aspect of AI programming is designed to continually fine-tune algorithms and ensure they provide the most accurate results possible. CREATIVITY: This aspect of AI uses neural networks, rules-based systems, statistical methods and other AI techniques to generate new images, new text, new music and new ideas. Why AI is important? : AI is important for its potential to change how we live, work and play. It has been effectively used in business to automate tasks done by humans, including customer service work, lead generation, fraud detection and quality control. In a number of areas, AI can perform tasks much better than humans. Particularly when it comes to repetitive, detail-oriented tasks, such as analyzing large numbers of legal documents to ensure relevant fields are filled in properly, AI tools often complete jobs quickly and with relatively few errors. Because of the massive data sets it can process, AI can also give enterprises insights into their operations they might not have been aware of. The rapidly expanding population of generative AI tools will be important in fields ranging from education and marketing to product design. Types of Artificial Intelligence: There are three major categories of AI algorithms: 1. supervised learning, 2. unsupervised learning, 3. semi-supervised learning and 4. reinforcement learning. The key differences between these algorithms are in how they\u2019re trained, and how they function. Supervised Learning: In supervised learning, the machine is taught by example. The operator provides the machine learning algorithm with a known dataset that includes desired inputs and outputs, and the algorithm must find a method to determine how to arrive at those inputs and outputs. While the operator knows the correct answers to the problem, the algorithm identifies patterns in data, learns from observations and makes predictions. The algorithm makes predictions and is corrected by the operator \u2013 and this process continues until the algorithm achieves a high level of accuracy/performance. Under the umbrella of supervised learning fall: Classification, Regression and Forecasting. CLASSIFICATION: In classification tasks, the machine learning program must draw a conclusion from observed values and determine to what category new observations belong. For example, when filtering emails as \u2018spam\u2019 or \u2018not spam\u2019, the program must look at existing observational data and filter the emails accordingly. 2.REGRESION: In regression tasks, the machine learning program must estimate and understand the relationships among variables. Regression analysis focuses on one dependent variable and a series of other changing variables making it particularly useful for prediction and forecasting. 3.FORECASTING: Forecasting is the process of making predictions about the future based on the past and present data, and is commonly used to analyse trends. Unsupervised Learning: Here, the machine learning algorithm studies data to identify patterns. There is no answer key or human operator to provide instruction. Instead, the machine determines the correlations and relationships by analysing available data. In an unsupervised learning process, the machine learning algorithm is left to interpret large data sets and address that data accordingly. The algorithm tries to organise that data in some way to describe its structure. This might mean grouping the data into clusters or arranging it in a way that looks more organised.As it assesses more data, its ability to make decisions on that data gradually improves and becomes more refined. Under the umbrella of unsupervised learning, fall: 1. CLUSTERING: Clustering involves grouping sets of similar data (based on defined criteria). It\u2019s useful for segmenting data into several groups and performing analysis on each data set to find patterns. 2. DIMENSION REDUCTION: Dimension reduction reduces the number of variables being considered to find the exact information required. Semi-supervised Learning: Semi-supervised learning is similar to supervised learning, but instead uses both labelled and unlabelled data. Labelled data is essentially information that has meaningful tags so that the algorithm can understand the data, whilst unlabelled data lacks that information. By using this combination, machine learning algorithms can learn to label unlabelled data. Reinforcement Learning: Reinforcement learning focuses on regimented learning processes, where a machine learning algorithm is provided with a set of actions, parameters and end values. By defining the rules, the machine learning algorithm then tries to explore different options and possibilities, monitoring and evaluating each result to determine which one is optimal. Reinforcement learning teaches the machine trial and error. It learns from past experiences and begins to adapt its approach in response to the situation to achieve the best possible result.","title":"Introduction to AI"},{"location":"page01/#introduction-to-ai","text":"","title":"Introduction to AI"},{"location":"page01/#what-is-artificial-intelligence","text":"Artificial intelligence is a branch of computer science concerned with creating machines that can think and make decisions independently of human intervention. Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition and machine vision.","title":"What is Artificial Intelligence :"},{"location":"page01/#what-is-ai-algorithms","text":"The definition of an algorithm is \u201ca set of instructions to be followed in calculations or other operations.\u201d This applies to both mathematics and computer science. So, at the essential level, an AI algorithm is the programming that tells the computer how to learn to operate on its own. An AI algorithm is much more complex than what most people learn about in algebra, of course. A complex set of rules drive AI programs, determining their steps and their ability to learn. Without an algorithm, AI wouldn\u2019t exist.","title":"What is AI Algorithms? :"},{"location":"page01/#how-does-a-ai-algorithm-work","text":"While a general algorithm can be simple, AI algorithms are by nature more complex. AI algorithms work by taking in training data that helps the algorithm to learn. How that data is acquired and is labeled marks the key difference between different types of AI algorithms. AI requires a foundation of specialized hardware and software for writing and training machine learning algorithms. No single programming language is synonymous with AI, but Python, R, Java, C++ and Julia have features popular with AI developers. At the core level, an AI algorithm takes in training data (labeled or unlabeled, supplied by developers, or acquired by the program itself) and uses that information to learn and grow. Then it completes its tasks, using the training data as a basis. Some types of AI algorithms can be taught to learn on their own and take in new data to change and refine their process.","title":"How does a AI algorithm work? :"},{"location":"page01/#cognitive-skills-that-are-more-focused-in-ai-programming","text":"LEARNING: This aspect of AI programming focuses on acquiring data and creating rules for how to turn it into actionable information. The rules, which are called algorithms, provide computing devices with step-by-step instructions for how to complete a specific task. REASONING: This aspect of AI programming focuses on choosing the right algorithm to reach a desired outcome. SELF-CORRECTION: This aspect of AI programming is designed to continually fine-tune algorithms and ensure they provide the most accurate results possible. CREATIVITY: This aspect of AI uses neural networks, rules-based systems, statistical methods and other AI techniques to generate new images, new text, new music and new ideas.","title":"Cognitive skills that are more focused in AI Programming:"},{"location":"page01/#why-ai-is-important","text":"AI is important for its potential to change how we live, work and play. It has been effectively used in business to automate tasks done by humans, including customer service work, lead generation, fraud detection and quality control. In a number of areas, AI can perform tasks much better than humans. Particularly when it comes to repetitive, detail-oriented tasks, such as analyzing large numbers of legal documents to ensure relevant fields are filled in properly, AI tools often complete jobs quickly and with relatively few errors. Because of the massive data sets it can process, AI can also give enterprises insights into their operations they might not have been aware of. The rapidly expanding population of generative AI tools will be important in fields ranging from education and marketing to product design.","title":"Why AI is important? :"},{"location":"page01/#types-of-artificial-intelligence","text":"There are three major categories of AI algorithms: 1. supervised learning, 2. unsupervised learning, 3. semi-supervised learning and 4. reinforcement learning. The key differences between these algorithms are in how they\u2019re trained, and how they function.","title":"Types of Artificial Intelligence:"},{"location":"page01/#supervised-learning","text":"In supervised learning, the machine is taught by example. The operator provides the machine learning algorithm with a known dataset that includes desired inputs and outputs, and the algorithm must find a method to determine how to arrive at those inputs and outputs. While the operator knows the correct answers to the problem, the algorithm identifies patterns in data, learns from observations and makes predictions. The algorithm makes predictions and is corrected by the operator \u2013 and this process continues until the algorithm achieves a high level of accuracy/performance. Under the umbrella of supervised learning fall: Classification, Regression and Forecasting. CLASSIFICATION: In classification tasks, the machine learning program must draw a conclusion from observed values and determine to what category new observations belong. For example, when filtering emails as \u2018spam\u2019 or \u2018not spam\u2019, the program must look at existing observational data and filter the emails accordingly. 2.REGRESION: In regression tasks, the machine learning program must estimate and understand the relationships among variables. Regression analysis focuses on one dependent variable and a series of other changing variables making it particularly useful for prediction and forecasting. 3.FORECASTING: Forecasting is the process of making predictions about the future based on the past and present data, and is commonly used to analyse trends.","title":"Supervised Learning:"},{"location":"page01/#unsupervised-learning","text":"Here, the machine learning algorithm studies data to identify patterns. There is no answer key or human operator to provide instruction. Instead, the machine determines the correlations and relationships by analysing available data. In an unsupervised learning process, the machine learning algorithm is left to interpret large data sets and address that data accordingly. The algorithm tries to organise that data in some way to describe its structure. This might mean grouping the data into clusters or arranging it in a way that looks more organised.As it assesses more data, its ability to make decisions on that data gradually improves and becomes more refined. Under the umbrella of unsupervised learning, fall: 1. CLUSTERING: Clustering involves grouping sets of similar data (based on defined criteria). It\u2019s useful for segmenting data into several groups and performing analysis on each data set to find patterns. 2. DIMENSION REDUCTION: Dimension reduction reduces the number of variables being considered to find the exact information required.","title":"Unsupervised Learning:"},{"location":"page01/#semi-supervised-learning","text":"Semi-supervised learning is similar to supervised learning, but instead uses both labelled and unlabelled data. Labelled data is essentially information that has meaningful tags so that the algorithm can understand the data, whilst unlabelled data lacks that information. By using this combination, machine learning algorithms can learn to label unlabelled data.","title":"Semi-supervised Learning:"},{"location":"page01/#reinforcement-learning","text":"Reinforcement learning focuses on regimented learning processes, where a machine learning algorithm is provided with a set of actions, parameters and end values. By defining the rules, the machine learning algorithm then tries to explore different options and possibilities, monitoring and evaluating each result to determine which one is optimal. Reinforcement learning teaches the machine trial and error. It learns from past experiences and begins to adapt its approach in response to the situation to achieve the best possible result.","title":"Reinforcement Learning:"},{"location":"page02/","text":"Guideline to build framework 1.Define the scope and requirements of the framework: Before you start building the framework, you need to define the problem domain, the target hardware platform, and the specific use cases that the framework will support. This will help you to narrow down the scope of the project and identify the key features and functionalities that the framework needs to provide.[] 2.Design the architecture of the framework: Once you have defined the scope and requirements, you can start designing the architecture of the framework. This involves deciding on the programming languages, libraries, and tools that you will use, as well as the overall structure of the framework, such as the API, the data structures, and the algorithms. 3.Implement the core features of the framework: With the architecture in place, you can start implementing the core features of the framework, such as the data preprocessing, the model training, the inference engine, and the hardware abstraction layer. 4.Optimize the performance of the framework: To ensure that the framework runs efficiently on embedded and edge devices, you need to optimize the performance of the algorithms and the data structures. This may involve using specialized hardware accelerators, such as GPUs or FPGAs, or implementing custom optimization techniques, such as quantization or pruning. 5.Test and evaluate the framework: Once you have implemented the core features and optimized the performance, you need to test and evaluate the framework to ensure that it meets the requirements and performs as expected. This involves running a series of tests and benchmarks on real-world data and hardware platforms","title":"Guideline to build framework"},{"location":"page02/#guideline-to-build-framework","text":"","title":"Guideline to build framework"},{"location":"page02/#1define-the-scope-and-requirements-of-the-framework","text":"Before you start building the framework, you need to define the problem domain, the target hardware platform, and the specific use cases that the framework will support. This will help you to narrow down the scope of the project and identify the key features and functionalities that the framework needs to provide.[]","title":"1.Define the scope and requirements of the framework:"},{"location":"page02/#2design-the-architecture-of-the-framework","text":"Once you have defined the scope and requirements, you can start designing the architecture of the framework. This involves deciding on the programming languages, libraries, and tools that you will use, as well as the overall structure of the framework, such as the API, the data structures, and the algorithms.","title":"2.Design the architecture of the framework:"},{"location":"page02/#3implement-the-core-features-of-the-framework","text":"With the architecture in place, you can start implementing the core features of the framework, such as the data preprocessing, the model training, the inference engine, and the hardware abstraction layer.","title":"3.Implement the core features of the framework:"},{"location":"page02/#4optimize-the-performance-of-the-framework","text":"To ensure that the framework runs efficiently on embedded and edge devices, you need to optimize the performance of the algorithms and the data structures. This may involve using specialized hardware accelerators, such as GPUs or FPGAs, or implementing custom optimization techniques, such as quantization or pruning.","title":"4.Optimize the performance of the framework:"},{"location":"page02/#5test-and-evaluate-the-framework","text":"Once you have implemented the core features and optimized the performance, you need to test and evaluate the framework to ensure that it meets the requirements and performs as expected. This involves running a series of tests and benchmarks on real-world data and hardware platforms","title":"5.Test and evaluate the framework:"},{"location":"page03/","text":"EAI frameworks Comparision study between the available different frameworks: Tensorflow: TensorFlow is an open-source machine learning library developed by Google that provides a framework for building and training neural networks. It is widely used for a wide range of applications including image and speech recognition, natural language processing, recommendation systems, and many other tasks related to machine learning and deep learning. With TensorFlow, you can: Build and train neural networks: TensorFlow provides a high-level API for defining, training, and evaluating neural networks. You can create various types of neural network architectures, such as convolutional neural networks (CNNs) for image recognition, recurrent neural networks (RNNs) for sequence data, and transformers for natural language processing. Perform data preprocessing: TensorFlow has built-in functions for common data preprocessing tasks such as image and text data augmentation, normalization, and feature scaling. These capabilities help you prepare your data for machine learning tasks. Deploy machine learning models: TensorFlow allows you to deploy trained models to various platforms, including cloud servers, mobile devices, and embedded systems. This enables you to use your machine learning models in real-world applications. Perform transfer learning: TensorFlow supports transfer learning, which allows you to use pre-trained models as a starting point for training your own models. This can significantly speed up the training process and improve model performance, especially when you have limited data. Optimize performance: TensorFlow provides various optimization techniques, such as graph optimizations, model quantization, and hardware accelerations, to optimize the performance of your machine learning models for inference on different devices. Experiment with different machine learning techniques: TensorFlow offers a flexible and extensible platform for experimenting with different machine learning techniques and algorithms. You can implement and compare various approaches to see which one works best for your specific use case. TensorFlow Lite : TensorFlow Lite is a lightweight version of the TensorFlow framework that is designed specifically for mobile and embedded devices. It is widely used in various applications such as image recognition, speech recognition, and natural language processing. It is used for deploying machine learning models on mobile and edge devices. It is a mobile-optimized version of TensorFlow, the popular open-source machine learning framework. TFLite is designed to run models with low latency and a small memory footprint, which makes it ideal for mobile and embedded devices. TFLite provides several features to optimize and deploy machine learning models on mobile and edge devices, including model quantization, model compression, operator fusion, and GPU acceleration. It also supports a wide range of hardware, including Android and iOS devices, microcontrollers, and embedded systems. TFLite supports a variety of machine learning models, including neural networks and other types of models. It provides tools to convert models trained in popular machine learning frameworks such as TensorFlow, Keras, and PyTorch into the TFLite format, which can be deployed on mobile and edge devices. There are several types of optimizations that can be done with TensorFlow Lite (TFLite) to improve the performance and efficiency of machine learning models on mobile and edge devices. Some of the common optimizations are: 1. Quantization: This technique involves reducing the precision of model weights and activations from floating-point to fixed-point format. This can significantly reduce the model size and improve inference speed without sacrificing too much accuracy. 2. Model pruning: This technique involves removing redundant or less important model weights and neurons to reduce the model's size and improve its inference speed. 3. Model compression: This technique involves compressing the model's weights and activations using techniques such as Huffman coding or arithmetic coding to reduce its size and improve its inference speed. 4. Operator fusion: This technique involves combining multiple operators in the model into a single operation to reduce the number of memory accesses and improve inference speed. 5. GPU acceleration: This optimization technique involves utilizing the GPU to accelerate model inference by offloading computation from the CPU to the GPU. 6. Model caching: This technique involves caching the model's activations to reduce the number of memory accesses required during inference and improve inference speed. 7. Dynamic input shapes: This feature allows models to handle input tensors with varying sizes and shapes, which can reduce memory usage and improve inference speed. These optimizations can be applied individually or in combination to improve the performance and efficiency of machine learning models deployed on mobile and edge devices using TensorFlow Lite. Overall, TFLite makes it easy to deploy machine learning models on mobile and edge devices, allowing developers to create powerful and intelligent applications for a wide range of use cases. ( steps on how to intrepret a tf model ) Keras : Keras is a high-level neural networks API that can run on top of TensorFlow, CNTK, or Theano. It is well-suited for building embedded AI applications due to its simplicity and ease of use. PyTorch : PyTorch is a popular machine learning framework that is widely used for research and production applications, including embedded devices. It offers a dynamic computational graph and supports various neural network architectures. Caffe : Caffe is an open-source deep learning framework developed by the Berkeley Vision and Learning Center (BVLC) at the University of California, Berkeley. It is specifically designed for efficient and fast training and inference of deep neural networks, with a focus on convolutional neural networks (CNNs) commonly used for image recognition tasks. With Caffe, you can: Train and deploy deep neural networks: Caffe provides a simple and expressive domain-specific language (DSL) for defining neural network architectures in a concise and readable format. It supports a wide range of layer types, including convolutional layers, pooling layers, fully connected layers, and more, making it suitable for various deep learning tasks such as image classification, object detection, and segmentation. Optimize for efficiency: Caffe is optimized for speed and efficiency, making it well-suited for training and inference on both CPUs and GPUs. It includes optimizations such as memory reuse, parallelism, and efficient computation to accelerate the training and inference process, making it a popular choice for real-time and resource-constrained applications. Utilize pre-trained models: Caffe supports pre-trained models, allowing you to leverage existing models that have been trained on large datasets for tasks such as image classification or object detection. This can save a significant amount of time and computational resources compared to training models from scratch. Fine-tune models: Caffe provides the ability to fine-tune pre-trained models on your own dataset, which is useful for transfer learning, where you can leverage a pre-trained model's knowledge on a different task and adapt it to your specific task with limited data. Experiment with model architectures: Caffe's flexible DSL makes it easy to experiment with different model architectures and hyperparameters. You can quickly iterate and test different model configurations to find the best performing model for your specific task. Deploy models to production: Caffe provides tools for converting trained models into a format suitable for deployment in production environments, such as converting models to optimized representations for inference on embedded devices or integrating models into other software systems. Caffe2 : Caffe2 is a lightweight and modular deep learning framework that is designed for embedded and mobile devices. It is known for its high performance and is widely used in various applications such as object detection and image segmentation. OpenCV : OpenCV is an open-source computer vision library that includes a range of machine learning algorithms and can be used for embedded AI applications. It is widely used in various applications such as robotics, surveillance, and augmented reality. Arm NN : Arm NN is a software framework designed to optimize the performance of neural networks on Arm-based devices, including embedded systems. It is widely used in various applications such as image recognition and speech recognition. Edge Impulse : Edge Impulse is an end-to-end platform for building, training, and deploying machine learning models on microcontrollers, including Arduino boards. It offers good performance on Arduino boards for simpler use cases such as gesture recognition or vibration monitoring. EloquentTinyML : EloquentTinyML is a set of machine learning libraries that can be used on Arduino boards. It offers good performance for simpler use cases such as sensor data classification or voice recognition. uTensor : uTensor is an inference library that can be used on microcontrollers, including some Arduino boards. It offers good performance for simpler use cases such as object detection or sensor data classification. Alfes : Alfes is an open-source embedded AI framework designed for microcontrollers, IoT devices, and other resource-constrained systems. It supports a variety of machine learning algorithms and neural network architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks. One of the key features of Alfes is its lightweight design, which allows it to run on devices with limited processing power, memory, and storage capacity. It achieves this by using optimized implementations of machine learning algorithms and neural network architectures, as well as techniques such as quantization and compression to reduce the computational and memory requirements of models. Alfes also provides a simple, easy-to-use API for developers to build and deploy machine learning models on embedded systems. It supports both C and C++ programming languages, and can be integrated with popular development environments such as Arduino and PlatformIO. Some of the applications of Alfes include object detection and recognition, speech recognition, and anomaly detection. It has been used in a variety of projects, from smart home devices and wearables to robotics and industrial automation. ONNX ONNX (Open Neural Network Exchange) is an open-source model representation format that enables interoperability between different deep learning frameworks. It provides a standardized way to represent and exchange deep learning models between various deep learning frameworks, allowing models trained in one framework to be used in another framework without the need for extensive model retraining or reimplementation. With ONNX, you can: 1. Interoperability between deep learning frameworks: ONNX allows you to convert trained models from one deep learning framework to another, making it possible to use models trained in one framework in another framework without having to retrain or reimplement the model. This provides flexibility and allows practitioners to leverage the strengths of different deep learning frameworks for different tasks or environments. 2. Model deployment in multiple environments: ONNX models can be deployed in a variety of environments, including edge devices, cloud servers, and data centers. This allows you to develop models in one framework and deploy them in different environments as needed, making it easier to scale and deploy models to production. 3. Collaborative model development: ONNX enables collaboration among researchers and practitioners who use different deep learning frameworks. It allows sharing of models between different teams or organizations, making it easier to collaborate on model development and foster innovation in the field of deep learning. 4. Model optimization: ONNX provides tools for model optimization, including optimizations for inference performance and memory usage. This can help in deploying optimized models to resource-constrained environments, such as embedded devices or edge devices, where computational resources may be limited. 5. Model visualization and debugging: ONNX provides visualization tools that allow you to inspect and debug the structure of deep learning models, helping to understand the internal workings of complex models and diagnose issues during model development and deployment. 6. Ecosystem of supported deep learning frameworks: ONNX has a growing ecosystem of deep learning frameworks that support the format, including popular frameworks such as TensorFlow, PyTorch, Caffe2, and many more. This means that you can use ONNX to exchange models between these frameworks, opening up possibilities for model reuse and interoperability.","title":"EAI frameworks"},{"location":"page03/#eai-frameworks","text":"","title":"EAI frameworks"},{"location":"page03/#comparision-study-between-the-available-different-frameworks","text":"","title":"Comparision study between the available different frameworks:"},{"location":"page03/#tensorflow","text":"TensorFlow is an open-source machine learning library developed by Google that provides a framework for building and training neural networks. It is widely used for a wide range of applications including image and speech recognition, natural language processing, recommendation systems, and many other tasks related to machine learning and deep learning. With TensorFlow, you can: Build and train neural networks: TensorFlow provides a high-level API for defining, training, and evaluating neural networks. You can create various types of neural network architectures, such as convolutional neural networks (CNNs) for image recognition, recurrent neural networks (RNNs) for sequence data, and transformers for natural language processing. Perform data preprocessing: TensorFlow has built-in functions for common data preprocessing tasks such as image and text data augmentation, normalization, and feature scaling. These capabilities help you prepare your data for machine learning tasks. Deploy machine learning models: TensorFlow allows you to deploy trained models to various platforms, including cloud servers, mobile devices, and embedded systems. This enables you to use your machine learning models in real-world applications. Perform transfer learning: TensorFlow supports transfer learning, which allows you to use pre-trained models as a starting point for training your own models. This can significantly speed up the training process and improve model performance, especially when you have limited data. Optimize performance: TensorFlow provides various optimization techniques, such as graph optimizations, model quantization, and hardware accelerations, to optimize the performance of your machine learning models for inference on different devices. Experiment with different machine learning techniques: TensorFlow offers a flexible and extensible platform for experimenting with different machine learning techniques and algorithms. You can implement and compare various approaches to see which one works best for your specific use case.","title":"Tensorflow:"},{"location":"page03/#tensorflow-lite","text":"TensorFlow Lite is a lightweight version of the TensorFlow framework that is designed specifically for mobile and embedded devices. It is widely used in various applications such as image recognition, speech recognition, and natural language processing. It is used for deploying machine learning models on mobile and edge devices. It is a mobile-optimized version of TensorFlow, the popular open-source machine learning framework. TFLite is designed to run models with low latency and a small memory footprint, which makes it ideal for mobile and embedded devices. TFLite provides several features to optimize and deploy machine learning models on mobile and edge devices, including model quantization, model compression, operator fusion, and GPU acceleration. It also supports a wide range of hardware, including Android and iOS devices, microcontrollers, and embedded systems. TFLite supports a variety of machine learning models, including neural networks and other types of models. It provides tools to convert models trained in popular machine learning frameworks such as TensorFlow, Keras, and PyTorch into the TFLite format, which can be deployed on mobile and edge devices. There are several types of optimizations that can be done with TensorFlow Lite (TFLite) to improve the performance and efficiency of machine learning models on mobile and edge devices. Some of the common optimizations are: 1. Quantization: This technique involves reducing the precision of model weights and activations from floating-point to fixed-point format. This can significantly reduce the model size and improve inference speed without sacrificing too much accuracy. 2. Model pruning: This technique involves removing redundant or less important model weights and neurons to reduce the model's size and improve its inference speed. 3. Model compression: This technique involves compressing the model's weights and activations using techniques such as Huffman coding or arithmetic coding to reduce its size and improve its inference speed. 4. Operator fusion: This technique involves combining multiple operators in the model into a single operation to reduce the number of memory accesses and improve inference speed. 5. GPU acceleration: This optimization technique involves utilizing the GPU to accelerate model inference by offloading computation from the CPU to the GPU. 6. Model caching: This technique involves caching the model's activations to reduce the number of memory accesses required during inference and improve inference speed. 7. Dynamic input shapes: This feature allows models to handle input tensors with varying sizes and shapes, which can reduce memory usage and improve inference speed. These optimizations can be applied individually or in combination to improve the performance and efficiency of machine learning models deployed on mobile and edge devices using TensorFlow Lite. Overall, TFLite makes it easy to deploy machine learning models on mobile and edge devices, allowing developers to create powerful and intelligent applications for a wide range of use cases. ( steps on how to intrepret a tf model )","title":"TensorFlow Lite:"},{"location":"page03/#keras","text":"Keras is a high-level neural networks API that can run on top of TensorFlow, CNTK, or Theano. It is well-suited for building embedded AI applications due to its simplicity and ease of use.","title":"Keras:"},{"location":"page03/#pytorch","text":"PyTorch is a popular machine learning framework that is widely used for research and production applications, including embedded devices. It offers a dynamic computational graph and supports various neural network architectures.","title":"PyTorch:"},{"location":"page03/#caffe","text":"Caffe is an open-source deep learning framework developed by the Berkeley Vision and Learning Center (BVLC) at the University of California, Berkeley. It is specifically designed for efficient and fast training and inference of deep neural networks, with a focus on convolutional neural networks (CNNs) commonly used for image recognition tasks. With Caffe, you can: Train and deploy deep neural networks: Caffe provides a simple and expressive domain-specific language (DSL) for defining neural network architectures in a concise and readable format. It supports a wide range of layer types, including convolutional layers, pooling layers, fully connected layers, and more, making it suitable for various deep learning tasks such as image classification, object detection, and segmentation. Optimize for efficiency: Caffe is optimized for speed and efficiency, making it well-suited for training and inference on both CPUs and GPUs. It includes optimizations such as memory reuse, parallelism, and efficient computation to accelerate the training and inference process, making it a popular choice for real-time and resource-constrained applications. Utilize pre-trained models: Caffe supports pre-trained models, allowing you to leverage existing models that have been trained on large datasets for tasks such as image classification or object detection. This can save a significant amount of time and computational resources compared to training models from scratch. Fine-tune models: Caffe provides the ability to fine-tune pre-trained models on your own dataset, which is useful for transfer learning, where you can leverage a pre-trained model's knowledge on a different task and adapt it to your specific task with limited data. Experiment with model architectures: Caffe's flexible DSL makes it easy to experiment with different model architectures and hyperparameters. You can quickly iterate and test different model configurations to find the best performing model for your specific task. Deploy models to production: Caffe provides tools for converting trained models into a format suitable for deployment in production environments, such as converting models to optimized representations for inference on embedded devices or integrating models into other software systems.","title":"Caffe:"},{"location":"page03/#caffe2","text":"Caffe2 is a lightweight and modular deep learning framework that is designed for embedded and mobile devices. It is known for its high performance and is widely used in various applications such as object detection and image segmentation.","title":"Caffe2:"},{"location":"page03/#opencv","text":"OpenCV is an open-source computer vision library that includes a range of machine learning algorithms and can be used for embedded AI applications. It is widely used in various applications such as robotics, surveillance, and augmented reality.","title":"OpenCV:"},{"location":"page03/#arm-nn","text":"Arm NN is a software framework designed to optimize the performance of neural networks on Arm-based devices, including embedded systems. It is widely used in various applications such as image recognition and speech recognition.","title":"Arm NN:"},{"location":"page03/#edge-impulse","text":"Edge Impulse is an end-to-end platform for building, training, and deploying machine learning models on microcontrollers, including Arduino boards. It offers good performance on Arduino boards for simpler use cases such as gesture recognition or vibration monitoring.","title":"Edge Impulse:"},{"location":"page03/#eloquenttinyml","text":"EloquentTinyML is a set of machine learning libraries that can be used on Arduino boards. It offers good performance for simpler use cases such as sensor data classification or voice recognition.","title":"EloquentTinyML:"},{"location":"page03/#utensor","text":"uTensor is an inference library that can be used on microcontrollers, including some Arduino boards. It offers good performance for simpler use cases such as object detection or sensor data classification.","title":"uTensor:"},{"location":"page03/#alfes","text":"Alfes is an open-source embedded AI framework designed for microcontrollers, IoT devices, and other resource-constrained systems. It supports a variety of machine learning algorithms and neural network architectures, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory (LSTM) networks. One of the key features of Alfes is its lightweight design, which allows it to run on devices with limited processing power, memory, and storage capacity. It achieves this by using optimized implementations of machine learning algorithms and neural network architectures, as well as techniques such as quantization and compression to reduce the computational and memory requirements of models. Alfes also provides a simple, easy-to-use API for developers to build and deploy machine learning models on embedded systems. It supports both C and C++ programming languages, and can be integrated with popular development environments such as Arduino and PlatformIO. Some of the applications of Alfes include object detection and recognition, speech recognition, and anomaly detection. It has been used in a variety of projects, from smart home devices and wearables to robotics and industrial automation.","title":"Alfes:"},{"location":"page03/#onnx","text":"ONNX (Open Neural Network Exchange) is an open-source model representation format that enables interoperability between different deep learning frameworks. It provides a standardized way to represent and exchange deep learning models between various deep learning frameworks, allowing models trained in one framework to be used in another framework without the need for extensive model retraining or reimplementation. With ONNX, you can: 1. Interoperability between deep learning frameworks: ONNX allows you to convert trained models from one deep learning framework to another, making it possible to use models trained in one framework in another framework without having to retrain or reimplement the model. This provides flexibility and allows practitioners to leverage the strengths of different deep learning frameworks for different tasks or environments. 2. Model deployment in multiple environments: ONNX models can be deployed in a variety of environments, including edge devices, cloud servers, and data centers. This allows you to develop models in one framework and deploy them in different environments as needed, making it easier to scale and deploy models to production. 3. Collaborative model development: ONNX enables collaboration among researchers and practitioners who use different deep learning frameworks. It allows sharing of models between different teams or organizations, making it easier to collaborate on model development and foster innovation in the field of deep learning. 4. Model optimization: ONNX provides tools for model optimization, including optimizations for inference performance and memory usage. This can help in deploying optimized models to resource-constrained environments, such as embedded devices or edge devices, where computational resources may be limited. 5. Model visualization and debugging: ONNX provides visualization tools that allow you to inspect and debug the structure of deep learning models, helping to understand the internal workings of complex models and diagnose issues during model development and deployment. 6. Ecosystem of supported deep learning frameworks: ONNX has a growing ecosystem of deep learning frameworks that support the format, including popular frameworks such as TensorFlow, PyTorch, Caffe2, and many more. This means that you can use ONNX to exchange models between these frameworks, opening up possibilities for model reuse and interoperability.","title":"ONNX"},{"location":"page04/","text":"Interpreting TensorFlow lite framework Understand the TFLite file format: The TFLite file format is a binary file that contains the model's metadata, such as input and output shapes and data types, as well as the model's graph and weights. We need to understand the structure of this file format to be able to parse and interpret it. Parse the TFLite file: Write code to read and parse the TFLite file, extracting the metadata and the model's graph and weights. This involves using low-level programming techniques such as bit manipulation and memory management. Load the model graph: Once TFLite file is parsed, we can load the model's graph into memory. The graph consists of a set of nodes that represent mathematical operations, such as matrix multiplication and convolution, and edges that connect these nodes and represent the flow of data between them. Execute the model graph: With the model graph loaded into memory, you can execute it on the target hardware platform. This involves using hardware-specific optimizations, such as using vectorized instructions on CPUs or using specialized hardware accelerators, to speed up the computation. Post-processing: After executing the model graph, we may need to perform additional postprocessing steps to convert the output data into a format that is usable by the target application.","title":"Interpreting TensorFlow lite framework"},{"location":"page04/#interpreting-tensorflow-lite-framework","text":"Understand the TFLite file format: The TFLite file format is a binary file that contains the model's metadata, such as input and output shapes and data types, as well as the model's graph and weights. We need to understand the structure of this file format to be able to parse and interpret it. Parse the TFLite file: Write code to read and parse the TFLite file, extracting the metadata and the model's graph and weights. This involves using low-level programming techniques such as bit manipulation and memory management. Load the model graph: Once TFLite file is parsed, we can load the model's graph into memory. The graph consists of a set of nodes that represent mathematical operations, such as matrix multiplication and convolution, and edges that connect these nodes and represent the flow of data between them. Execute the model graph: With the model graph loaded into memory, you can execute it on the target hardware platform. This involves using hardware-specific optimizations, such as using vectorized instructions on CPUs or using specialized hardware accelerators, to speed up the computation. Post-processing: After executing the model graph, we may need to perform additional postprocessing steps to convert the output data into a format that is usable by the target application.","title":"Interpreting TensorFlow lite framework"},{"location":"page05/","text":"Optimization techniques Optimization is a critical component of machine learning as it enables algorithms to learn from data and improve their performance on a given task. In machine learning, optimization refers to the process of adjusting the parameters of a model in order to minimize the difference between the predicted and actual output for a given input. Optimization techniques for embedded devices are similar to those used in traditional machine learning, but they are specifically tailored to the limitations and requirements of embedded systems. Some common optimization techniques used for embedded devices include: Quantization: Pruning: Compression: Knowledge Distillation: Hardware Acceleration: Overall, optimization techniques for embedded devices are focused on reducing the size, complexity, and computational requirements of machine learning models, while still maintaining their accuracy and effectiveness.","title":"Optimization techniques"},{"location":"page05/#optimization-techniques","text":"Optimization is a critical component of machine learning as it enables algorithms to learn from data and improve their performance on a given task. In machine learning, optimization refers to the process of adjusting the parameters of a model in order to minimize the difference between the predicted and actual output for a given input. Optimization techniques for embedded devices are similar to those used in traditional machine learning, but they are specifically tailored to the limitations and requirements of embedded systems. Some common optimization techniques used for embedded devices include: Quantization: Pruning: Compression: Knowledge Distillation: Hardware Acceleration: Overall, optimization techniques for embedded devices are focused on reducing the size, complexity, and computational requirements of machine learning models, while still maintaining their accuracy and effectiveness.","title":"Optimization techniques"},{"location":"page06/","text":"Pruning Pruning is a technique used in machine learning to reduce the size and complexity of a trained model. It involves removing unnecessary connections or neurons from a neural network, which can significantly reduce its size and improve its speed, without sacrificing its accuracy. Pruning can be performed in different ways, such as magnitude-based pruning, where weights with the smallest magnitudes are pruned, or structured pruning, where entire layers or blocks of neurons are pruned based on their importance to the network. Pruning is usually performed after a model has been trained and can be used in conjunction with other optimization techniques, such as quantization and compression, to further reduce the size and complexity of a model. Pruning can also be applied iteratively, where a model is pruned and then retrained, to achieve even greater reductions in size and complexity. One advantage of pruning is that it can lead to models that are more efficient and easier to deploy in real-world applications, particularly on embedded devices with limited memory and processing power. Additionally, pruning can also help to reduce the risk of overfitting, where a model becomes too complex and performs poorly on new data. However, it is important to note that pruning can also lead to a decrease in accuracy if too many connections or neurons are removed from the model. Compression is a technique used in machine learning to reduce the storage requirements of trained models, without significantly impacting their accuracy or performance. It involves using algorithms to compress the weights and activations of a model, which can greatly reduce the amount of memory required to store the model. Deep Learning models these days require a significant amount of computing, memory, and power which becomes a bottleneck in the conditions where we need real-time inference or to run models on edge devices and browsers with limited computational resources. Energy efficiency is a major concern for current deep learning models. Pruning is one of the methods for inference to efficiently produce models smaller in size, more memory-efficient, more power-efficient and faster at inference with minimal loss in accuracy, other such techniques being weight sharing and quantization. Our first step is to get a couple of imports out of the way: Os and Zipfile will help us in assessing the size of the models. tensorflow_model_optimization for model pruning. load_model for loading a saved model. and of course tensorflow and keras. finally we'll able to visualize the models: import os import zipfile import tensorflow as tf import tensorflow_model_optimization as tfmot from tensorflow.keras.models import load_model from tensorflow import keras % load_ext tensorboard DATASET GENERATION For this experiment, we'll genertate a regression datset using sckit-learn. Thereafter, we split the datset into a training and test set: from sklearn.datasets import make_friedman1 X , y = make_friedman1 ( n_samples = 10000 , n_features = 10 , random_state = 0 ) from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = 42 ) MODEL WITHOUT PRUNING We\u2019ll create a simple neural network to predict the target variable y. Then check the mean squared error. After this, we\u2019ll compare this with the entire model pruned, and then with just the Dense layer pruned. Next, we step up a callback to stop training the model once it stops improving, after 30 epochs. early_stop = keras . callbacks . EarlyStopping ( monitor = \u2019 val_loss \u2019 , patience = 30 ) Let\u2019s print a summary of the model so that we can compare it with the summary of the pruned models. model = setup_model () model . summary () Let\u2019s compile the model and train it. model . compile ( optimizer = \u2019 adam \u2019 , loss = tf . keras . losses . mean_squared_error , metrics = [ \u2018 mae \u2019 , \u2018 mse \u2019 ]) model . fit ( X_train , y_train , epochs = 300 , validation_split = 0.2 , callbacks = early_stop , verbose = 0 ) Check the model summary. Compare this with the summary of the unpruned model.' model_to_prune . summary () We have to compile the model before we can fit it to the training and testing set. model_to_prune . compile ( optimizer = \u2019 adam \u2019 , loss = tf . keras . losses . mean_squared_error , metrics = [ \u2018 mae \u2019 , \u2018 mse \u2019 ]) Since we\u2019re applying pruning, we have to define a couple of pruning callbacks in addition to the early stopping callback. tfmot . sparsity . keras . UpdatePruningStep () ``` Updates pruning wrappers with the optimizer step . Failure to specify it will result in an error . ``` python tfmot . sparsity . keras . PruningSummaries () adds pruning summaries to the Tensorboard. log_dir = \u2018 . models \u2019 callbacks = [ tfmot . sparsity . keras . UpdatePruningStep (), # Log sparsity and other metrics in Tensorboard. tfmot . sparsity . keras . PruningSummaries ( log_dir = log_dir ), keras . callbacks . EarlyStopping ( monitor = \u2019 val_loss \u2019 , patience = 10 ) ] With that out of the way, we can now fit the model to the training set. model_to_prune . fit ( X_train , y_train , epochs = 100 , validation_split = 0.2 , callbacks = callbacks , verbose = 0 ) Upon checking the mean squared error for this model , we notice that it \u2019 s slightly higher than the one for the unpruned model . prune_predictions = model_to_prune . predict ( X_test ) print ( \u2018 Whole Model Pruned MSE % .4 f \u2019 % mean_squared_error ( y_test , prune_predictions . reshape ( 3300 ,))) Whole Model Pruned MSE 0.1830 Types of pruning Structured and Unstructured Pruning Individual parameters are pruned using an unstructured pruning approach. This results in a sparse neural network, which, while lower in terms of parameter count, may not be configured in a way that promotes speed improvements. Randomly zeroing out the parameters saves memory but may not necessarily improve computing performance because we end up conducting the same number of matrix multiplications as before. Because we set specific weights in the weight matrix to zero, this is also known as Weight Pruning. Single-shot pruning / one shot pruning A one-shot training and pruning framework that compresses a full neural network into a slimmer one with competitive performance by Only-Train-Once. OTO dramatically simplifies the complex multi-stage training pipelines of the existing pruning approaches, fits various architectures and applications, and hence is generic and efficient. The oneshot pruning approach works like iterative, but uses a prune rate of P=1.0. This means we prune the entire model in one shot reducing it down entirely to a size of [1, 1]. We run oneshot on all of the potential starting points we described in iterative (second approach), then we select the best performing oneshot model. A simple example of Single shot pruning where the threshold is set to 0.1 import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms # Define your neural network model class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=3) self.conv2 = nn.Conv2d(64, 64, kernel_size=3) self.fc1 = nn.Linear(64 * 5 * 5, 384) self.fc2 = nn.Linear(384, 10) def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = x.view(-1, 64 * 5 * 5) x = self.fc1(x) x = self.fc2(x) return x # Load your dataset and create data loaders transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] ) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2) # Train the initial model device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") net = Net().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) def train(net, dataloader, criterion, optimizer, device): net.train() running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() return running_loss for epoch in range(10): loss = train(net, trainloader, criterion, optimizer, device) print(f\"Epoch {epoch+1}: Loss = {loss}\") # Compute importance scores (e.g., magnitude-based pruning) def compute_importance_scores(net, dataloader, device): net.eval() importance_scores = [] for inputs, _ in dataloader: inputs = inputs.to(device) outputs = net(inputs) grad_outputs = torch.zeros_like(outputs).to(device) outputs.backward(grad_outputs) for param in net.parameters(): if param.grad is not None: importance_scores.append(torch.abs(param.grad)) return torch.cat(importance_scores) # Prune the network based on importance scores def prune_network(net, importance_scores, pruning_threshold): for name, module in net.named_modules(): if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear): importance_scores_layer = importance_scores[name] mask = importance_scores_layer > pruning_threshold module.weight.data *= mask.float() if module.bias is not None: module.bias.data *= mask.float() # Fine-tune the pruned model def fine_tune(net, dataloader, criterion, optimizer, device): net.train() for epoch in range(5): running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f\"Fine-tuning Epoch {epoch+1}: Loss = {running_loss}\") # Perform single-shot pruning pruning_threshold = 0.1 importance_scores = compute_importance_scores(net, trainloader, device) prune_network(net, importance_scores, pruning_threshold) fine_tune(net, trainloader, criterion, optimizer, device) This is a simple CNN model with CIFAR-10 dataset Iterative pruning Iterative Pruning or dynamic pruning repeats the process of pruning the network to some extent and retraining it until the desired pruning rate is obtained. A simple iterative approach can be thought of as starting with a model size and reducing it slowly until we reach an optimal size based on criteria for accuracy and speed performance. The iterative approach uses a pruning rate P=0.5 (50%). Here's a code snippet that demonstrates the iterative pruning process using PyTorch: import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms # Define your neural network model class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(784, 300) self.fc2 = nn.Linear(300, 100) self.fc3 = nn.Linear(100, 10) def forward(self, x): x = x.view(x.size(0), -1) x = self.fc1(x) x = self.fc2(x) x = self.fc3(x) return x # Load your dataset and create data loaders transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))] ) trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2) # Train the initial model device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") net = Net().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9) def train(net, dataloader, criterion, optimizer, device): net.train() running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() return running_loss for epoch in range(10): loss = train(net, trainloader, criterion, optimizer, device) print(f\"Epoch {epoch+1}: Loss = {loss}\") # Compute importance scores (e.g., magnitude-based pruning) def compute_importance_scores(net, dataloader, device): net.eval() importance_scores = [] for inputs, _ in dataloader: inputs = inputs.to(device) outputs = net(inputs) grad_outputs = torch.zeros_like(outputs).to(device) outputs.backward(grad_outputs) for param in net.parameters(): if param.grad is not None: importance_scores.append(torch.abs(param.grad)) return torch.cat(importance_scores) # Prune the network based on importance scores def prune_network(net, importance_scores, pruning_rate): flat_scores = importance_scores.flatten() k = int(len(flat_scores) * pruning_rate) threshold = torch.topk(flat_scores, k).values.min() for name, module in net.named_modules(): if isinstance(module, nn.Linear): importance_scores_layer = importance_scores[name] mask = importance_scores_layer > threshold module.weight.data *= mask.float() if module.bias is not None: module.bias.data *= mask.float() # Fine-tune the pruned model def fine_tune(net, dataloader, criterion, optimizer, device): net.train() for epoch in range(5): running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f\"Fine-tuning Epoch {epoch+1}: Loss = {running_loss}\") # Perform iterative pruning pruning_rate = 0.2 num_iterations = 5 for iteration in range(num_iterations): importance_scores = compute_importance_scores(net, trainloader, device) prune_network(net, importance_scores, pruning_rate) fine_tune(net, trainloader, criterion, optimizer, device) In this example, we use the MNIST dataset and a simple fully connected neural network model. Comparision of the accuracy of Single shot pruning and Iterative pruning in various models Advantages of Single-shot and Iterative Pruning The choice between single-shot pruning and iterative pruning depends on various factors, including the specific requirements of your task, the available computational resources, and the desired balance between model size and accuracy. Both approaches have their advantages and considerations: Single-shot pruning: Simplicity: Single-shot pruning involves pruning the network in a single pass, which can be simpler to implement compared to the iterative approach. Reduced computational cost: Since single-shot pruning requires only one pass, it can be computationally more efficient compared to iterative pruning, which involves multiple passes of pruning and fine-tuning. Fixed model size: Single-shot pruning provides a fixed and known model size after pruning, making it easier to deploy and manage the pruned model. Iterative pruning: Better accuracy retention: Iterative pruning allows for gradual pruning, which can help maintain or recover model accuracy more effectively. It provides an opportunity to fine-tune the model after each pruning step, reducing the impact on accuracy compared to single-shot pruning. More aggressive pruning: Iterative pruning allows for multiple iterations, enabling more aggressive pruning and potentially achieving higher compression rates. Adaptive pruning: With iterative pruning, you can adaptively adjust the pruning threshold or rate based on the model's performance during the pruning process, leading to potentially better results. Conclusion Single-shot pruning is a simpler and computationally more efficient approach, while iterative pruning offers better accuracy retention and the ability to achieve higher compression rates. The choice between them depends on the specific trade-offs and priorities for your particular use case. It's often recommended to experiment with both approaches and evaluate their impact on accuracy, model size, and computational requirements to determine the most suitable pruning method for your needs.","title":"Pruning"},{"location":"page06/#pruning","text":"Pruning is a technique used in machine learning to reduce the size and complexity of a trained model. It involves removing unnecessary connections or neurons from a neural network, which can significantly reduce its size and improve its speed, without sacrificing its accuracy. Pruning can be performed in different ways, such as magnitude-based pruning, where weights with the smallest magnitudes are pruned, or structured pruning, where entire layers or blocks of neurons are pruned based on their importance to the network. Pruning is usually performed after a model has been trained and can be used in conjunction with other optimization techniques, such as quantization and compression, to further reduce the size and complexity of a model. Pruning can also be applied iteratively, where a model is pruned and then retrained, to achieve even greater reductions in size and complexity. One advantage of pruning is that it can lead to models that are more efficient and easier to deploy in real-world applications, particularly on embedded devices with limited memory and processing power. Additionally, pruning can also help to reduce the risk of overfitting, where a model becomes too complex and performs poorly on new data. However, it is important to note that pruning can also lead to a decrease in accuracy if too many connections or neurons are removed from the model. Compression is a technique used in machine learning to reduce the storage requirements of trained models, without significantly impacting their accuracy or performance. It involves using algorithms to compress the weights and activations of a model, which can greatly reduce the amount of memory required to store the model. Deep Learning models these days require a significant amount of computing, memory, and power which becomes a bottleneck in the conditions where we need real-time inference or to run models on edge devices and browsers with limited computational resources. Energy efficiency is a major concern for current deep learning models. Pruning is one of the methods for inference to efficiently produce models smaller in size, more memory-efficient, more power-efficient and faster at inference with minimal loss in accuracy, other such techniques being weight sharing and quantization. Our first step is to get a couple of imports out of the way: Os and Zipfile will help us in assessing the size of the models. tensorflow_model_optimization for model pruning. load_model for loading a saved model. and of course tensorflow and keras. finally we'll able to visualize the models: import os import zipfile import tensorflow as tf import tensorflow_model_optimization as tfmot from tensorflow.keras.models import load_model from tensorflow import keras % load_ext tensorboard DATASET GENERATION For this experiment, we'll genertate a regression datset using sckit-learn. Thereafter, we split the datset into a training and test set: from sklearn.datasets import make_friedman1 X , y = make_friedman1 ( n_samples = 10000 , n_features = 10 , random_state = 0 ) from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = 42 ) MODEL WITHOUT PRUNING We\u2019ll create a simple neural network to predict the target variable y. Then check the mean squared error. After this, we\u2019ll compare this with the entire model pruned, and then with just the Dense layer pruned. Next, we step up a callback to stop training the model once it stops improving, after 30 epochs. early_stop = keras . callbacks . EarlyStopping ( monitor = \u2019 val_loss \u2019 , patience = 30 ) Let\u2019s print a summary of the model so that we can compare it with the summary of the pruned models. model = setup_model () model . summary () Let\u2019s compile the model and train it. model . compile ( optimizer = \u2019 adam \u2019 , loss = tf . keras . losses . mean_squared_error , metrics = [ \u2018 mae \u2019 , \u2018 mse \u2019 ]) model . fit ( X_train , y_train , epochs = 300 , validation_split = 0.2 , callbacks = early_stop , verbose = 0 ) Check the model summary. Compare this with the summary of the unpruned model.' model_to_prune . summary () We have to compile the model before we can fit it to the training and testing set. model_to_prune . compile ( optimizer = \u2019 adam \u2019 , loss = tf . keras . losses . mean_squared_error , metrics = [ \u2018 mae \u2019 , \u2018 mse \u2019 ]) Since we\u2019re applying pruning, we have to define a couple of pruning callbacks in addition to the early stopping callback. tfmot . sparsity . keras . UpdatePruningStep () ``` Updates pruning wrappers with the optimizer step . Failure to specify it will result in an error . ``` python tfmot . sparsity . keras . PruningSummaries () adds pruning summaries to the Tensorboard. log_dir = \u2018 . models \u2019 callbacks = [ tfmot . sparsity . keras . UpdatePruningStep (), # Log sparsity and other metrics in Tensorboard. tfmot . sparsity . keras . PruningSummaries ( log_dir = log_dir ), keras . callbacks . EarlyStopping ( monitor = \u2019 val_loss \u2019 , patience = 10 ) ] With that out of the way, we can now fit the model to the training set. model_to_prune . fit ( X_train , y_train , epochs = 100 , validation_split = 0.2 , callbacks = callbacks , verbose = 0 ) Upon checking the mean squared error for this model , we notice that it \u2019 s slightly higher than the one for the unpruned model . prune_predictions = model_to_prune . predict ( X_test ) print ( \u2018 Whole Model Pruned MSE % .4 f \u2019 % mean_squared_error ( y_test , prune_predictions . reshape ( 3300 ,))) Whole Model Pruned MSE 0.1830","title":"Pruning"},{"location":"page06/#types-of-pruning","text":"","title":"Types of pruning"},{"location":"page06/#structured-and-unstructured-pruning","text":"Individual parameters are pruned using an unstructured pruning approach. This results in a sparse neural network, which, while lower in terms of parameter count, may not be configured in a way that promotes speed improvements. Randomly zeroing out the parameters saves memory but may not necessarily improve computing performance because we end up conducting the same number of matrix multiplications as before. Because we set specific weights in the weight matrix to zero, this is also known as Weight Pruning.","title":"Structured and Unstructured Pruning"},{"location":"page06/#single-shot-pruning-one-shot-pruning","text":"A one-shot training and pruning framework that compresses a full neural network into a slimmer one with competitive performance by Only-Train-Once. OTO dramatically simplifies the complex multi-stage training pipelines of the existing pruning approaches, fits various architectures and applications, and hence is generic and efficient. The oneshot pruning approach works like iterative, but uses a prune rate of P=1.0. This means we prune the entire model in one shot reducing it down entirely to a size of [1, 1]. We run oneshot on all of the potential starting points we described in iterative (second approach), then we select the best performing oneshot model. A simple example of Single shot pruning where the threshold is set to 0.1 import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms # Define your neural network model class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=3) self.conv2 = nn.Conv2d(64, 64, kernel_size=3) self.fc1 = nn.Linear(64 * 5 * 5, 384) self.fc2 = nn.Linear(384, 10) def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = x.view(-1, 64 * 5 * 5) x = self.fc1(x) x = self.fc2(x) return x # Load your dataset and create data loaders transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] ) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2) # Train the initial model device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") net = Net().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4) def train(net, dataloader, criterion, optimizer, device): net.train() running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() return running_loss for epoch in range(10): loss = train(net, trainloader, criterion, optimizer, device) print(f\"Epoch {epoch+1}: Loss = {loss}\") # Compute importance scores (e.g., magnitude-based pruning) def compute_importance_scores(net, dataloader, device): net.eval() importance_scores = [] for inputs, _ in dataloader: inputs = inputs.to(device) outputs = net(inputs) grad_outputs = torch.zeros_like(outputs).to(device) outputs.backward(grad_outputs) for param in net.parameters(): if param.grad is not None: importance_scores.append(torch.abs(param.grad)) return torch.cat(importance_scores) # Prune the network based on importance scores def prune_network(net, importance_scores, pruning_threshold): for name, module in net.named_modules(): if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear): importance_scores_layer = importance_scores[name] mask = importance_scores_layer > pruning_threshold module.weight.data *= mask.float() if module.bias is not None: module.bias.data *= mask.float() # Fine-tune the pruned model def fine_tune(net, dataloader, criterion, optimizer, device): net.train() for epoch in range(5): running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f\"Fine-tuning Epoch {epoch+1}: Loss = {running_loss}\") # Perform single-shot pruning pruning_threshold = 0.1 importance_scores = compute_importance_scores(net, trainloader, device) prune_network(net, importance_scores, pruning_threshold) fine_tune(net, trainloader, criterion, optimizer, device) This is a simple CNN model with CIFAR-10 dataset","title":"Single-shot pruning / one shot pruning"},{"location":"page06/#iterative-pruning","text":"Iterative Pruning or dynamic pruning repeats the process of pruning the network to some extent and retraining it until the desired pruning rate is obtained. A simple iterative approach can be thought of as starting with a model size and reducing it slowly until we reach an optimal size based on criteria for accuracy and speed performance. The iterative approach uses a pruning rate P=0.5 (50%). Here's a code snippet that demonstrates the iterative pruning process using PyTorch: import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.transforms as transforms # Define your neural network model class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(784, 300) self.fc2 = nn.Linear(300, 100) self.fc3 = nn.Linear(100, 10) def forward(self, x): x = x.view(x.size(0), -1) x = self.fc1(x) x = self.fc2(x) x = self.fc3(x) return x # Load your dataset and create data loaders transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))] ) trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2) # Train the initial model device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") net = Net().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9) def train(net, dataloader, criterion, optimizer, device): net.train() running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() return running_loss for epoch in range(10): loss = train(net, trainloader, criterion, optimizer, device) print(f\"Epoch {epoch+1}: Loss = {loss}\") # Compute importance scores (e.g., magnitude-based pruning) def compute_importance_scores(net, dataloader, device): net.eval() importance_scores = [] for inputs, _ in dataloader: inputs = inputs.to(device) outputs = net(inputs) grad_outputs = torch.zeros_like(outputs).to(device) outputs.backward(grad_outputs) for param in net.parameters(): if param.grad is not None: importance_scores.append(torch.abs(param.grad)) return torch.cat(importance_scores) # Prune the network based on importance scores def prune_network(net, importance_scores, pruning_rate): flat_scores = importance_scores.flatten() k = int(len(flat_scores) * pruning_rate) threshold = torch.topk(flat_scores, k).values.min() for name, module in net.named_modules(): if isinstance(module, nn.Linear): importance_scores_layer = importance_scores[name] mask = importance_scores_layer > threshold module.weight.data *= mask.float() if module.bias is not None: module.bias.data *= mask.float() # Fine-tune the pruned model def fine_tune(net, dataloader, criterion, optimizer, device): net.train() for epoch in range(5): running_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(device), labels.to(device) optimizer.zero_grad() outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f\"Fine-tuning Epoch {epoch+1}: Loss = {running_loss}\") # Perform iterative pruning pruning_rate = 0.2 num_iterations = 5 for iteration in range(num_iterations): importance_scores = compute_importance_scores(net, trainloader, device) prune_network(net, importance_scores, pruning_rate) fine_tune(net, trainloader, criterion, optimizer, device) In this example, we use the MNIST dataset and a simple fully connected neural network model.","title":"Iterative pruning"},{"location":"page06/#comparision-of-the-accuracy-of-single-shot-pruning-and-iterative-pruning-in-various-models","text":"","title":"Comparision of the accuracy of Single shot pruning and Iterative pruning in various models"},{"location":"page06/#advantages-of-single-shot-and-iterative-pruning","text":"The choice between single-shot pruning and iterative pruning depends on various factors, including the specific requirements of your task, the available computational resources, and the desired balance between model size and accuracy. Both approaches have their advantages and considerations:","title":"Advantages of Single-shot and Iterative Pruning"},{"location":"page06/#single-shot-pruning","text":"Simplicity: Single-shot pruning involves pruning the network in a single pass, which can be simpler to implement compared to the iterative approach. Reduced computational cost: Since single-shot pruning requires only one pass, it can be computationally more efficient compared to iterative pruning, which involves multiple passes of pruning and fine-tuning. Fixed model size: Single-shot pruning provides a fixed and known model size after pruning, making it easier to deploy and manage the pruned model.","title":"Single-shot pruning:"},{"location":"page06/#iterative-pruning_1","text":"Better accuracy retention: Iterative pruning allows for gradual pruning, which can help maintain or recover model accuracy more effectively. It provides an opportunity to fine-tune the model after each pruning step, reducing the impact on accuracy compared to single-shot pruning. More aggressive pruning: Iterative pruning allows for multiple iterations, enabling more aggressive pruning and potentially achieving higher compression rates. Adaptive pruning: With iterative pruning, you can adaptively adjust the pruning threshold or rate based on the model's performance during the pruning process, leading to potentially better results.","title":"Iterative pruning:"},{"location":"page06/#conclusion","text":"Single-shot pruning is a simpler and computationally more efficient approach, while iterative pruning offers better accuracy retention and the ability to achieve higher compression rates. The choice between them depends on the specific trade-offs and priorities for your particular use case. It's often recommended to experiment with both approaches and evaluate their impact on accuracy, model size, and computational requirements to determine the most suitable pruning method for your needs.","title":"Conclusion"},{"location":"page07/","text":"Quantization This document aims to provide an overview of quantization, its importance, and its objectives in reducing data size, computational complexity, and memory requirements. By understanding quantization, you'll gain insights into a fundamental technique used in signal processing, data compression, and optimization. Definition of Quantization and Key Objectives: Quantization is a process that involves mapping a continuous range of values to a discrete set of values. The main objectives of quantization include: Data Size Reduction: Quantization reduces the number of bits required to represent a given set of data. By mapping data to a smaller set of discrete values, the size of the data representation is reduced, leading to efficient storage and transmission. Computational Complexity Reduction: Quantization reduces the precision or granularity of data, which can significantly reduce the computational complexity of processing operations. This is particularly important in computationally intensive tasks where high precision is not always necessary. Memory Requirement Optimization: By representing data with fewer bits, quantization helps optimize memory utilization. This is especially valuable in scenarios where memory resources are limited, such as in embedded systems or mobile devices. Trade-off between Accuracy and Efficiency: Quantization involves a trade-off between preserving data accuracy and achieving efficiency gains. The challenge lies in finding the right balance between reducing data size and maintaining an acceptable level of fidelity in the reconstructed signal. By achieving these objectives, quantization enables the efficient storage, transmission, and processing of data, making it a valuable technique in various fields. In the following sections of this document, we will explore quantization techniques, frameworks, and their applications in greater detail, providing insights into how quantization is implemented and its impact in specific domains. Importance of Quantization: Quantization plays a crucial role in a wide range of domains, including signal processing, data compression, machine learning, and embedded systems. Here are some reasons why quantization is important: Efficient Storage and Transmission: In many applications, data needs to be stored or transmitted efficiently. Quantization allows for the reduction of data size, enabling more compact representation and efficient utilization of storage space or network bandwidth. Computational Complexity Reduction: High-precision computations can be computationally intensive and resource-consuming. By reducing the precision of data through quantization, the computational complexity can be significantly reduced, leading to faster and more efficient processing. Memory Requirement Optimization: Memory usage is a critical consideration, especially in resource-constrained systems. Quantization reduces the memory requirements by representing data with fewer bits, allowing for optimized memory utilization. Hardware Implementation: In embedded systems or hardware accelerators, quantization enables the implementation of efficient algorithms using fixed-point arithmetic, which is more suitable for hardware implementations compared to floating-point operations. Basics of Quantization: Quantization is a fundamental concept in signal processing, data compression, and machine learning. It involves converting continuous and often analog data into a discrete representation. Let's explore the key concepts associated with quantization: Discretization of Continuous Data: Quantization involves dividing a continuous range of values into a finite number of discrete levels or bins. This allows us to represent the data using a limited set of values, resulting in a digital representation that can be processed, stored, or transmitted more efficiently. Representation of Discrete Levels: In quantization, the continuous data is mapped to the nearest discrete level within the defined set of quantization levels. Each level represents a range of values, and the data point is assigned to the closest level. The number of quantization levels determines the precision of the quantization. Quantization Error: Quantization introduces an inherent error called quantization error. It is the difference between the original value and its quantized representation. The quantization error arises because the discrete levels cannot precisely represent the infinite possibilities of the original continuous data. Minimizing quantization error is a key objective in quantization techniques. Impact on Fidelity: The quantization error has an impact on the fidelity or accuracy of the reconstructed data. Higher precision quantization (using more levels) can reduce the quantization error and preserve fidelity but requires more bits for representation. On the other hand, lower precision quantization (using fewer levels) reduces fidelity but offers higher efficiency in terms of storage, transmission, or computation. Role of Quantization: Signal Processing: In signal processing, quantization is used to convert analog signals into digital representations. It is a fundamental step in analog-to-digital conversion, enabling further processing and manipulation of signals in digital systems. Data Compression: Quantization plays a crucial role in data compression techniques. By reducing the precision of data, quantization allows for more efficient compression algorithms, as the reduced number of bits required for representation leads to smaller file sizes or reduced transmission bandwidth. Machine Learning: In machine learning, quantization is employed to optimize the deployment of models on resource-constrained devices. Quantizing the model parameters and activations reduces memory usage and computational complexity, enabling efficient inference on edge devices without sacrificing too much accuracy. Quantization is a powerful technique that strikes a balance between efficient utilization of resources and maintaining an acceptable level of fidelity in various domains. Understanding the basics of quantization helps in exploring more advanced quantization methods and their applications in signal processing, data compression, and machine learning tasks. Types of Quantization: Uniform Quantization: As the name implies, the quantized levels in the uniform quantization process are equally spaced. The uniform quantization is further categorized as mid-rise type uniform quantization and mid-tread type uniform quantization. Both the uniform quantization processes are symmetric about the respective axis. Mid-rise type uniform Quantization: Rise refers to the rising part. The origin of the discrete quantized signal lies in the middle of the rising part of the stair like graph, as shown below: Mid-tread type uniform Quantization: Tread refers to the flat part. The origin of the discrete quantized signal lies in the middle of the tread part of the stair like graph, as shown below: Quantization Frameworks Quantization frameworks include TensorFlow with TensorFlow Model Optimization Toolkit, PyTorch with TorchVision, and ONNX Runtime. These frameworks provide a comprehensive set of tools and resources to simplify the process of quantizing models and deploying them on resource-constrained devices. Framework 1: TensorFlow TensorFlow's quantization capabilities are not limited to specific model architectures. The TensorFlow Model Optimization Toolkit supports quantization for a wide range of models. By leveraging TensorFlow's quantization support, developers can optimize their models for deployment on devices with limited computational resources, such as mobile phones, edge devices, or embedded systems. The quantized models achieve a balance between accuracy and efficiency, enabling efficient inference while minimizing memory usage and computational requirements. Quantization Techniques: Post-training Quantization: Post-training quantization is a technique that involves converting pre-trained floating-point models into quantized formats. This technique allows models to be optimized without retraining. TensorFlow provides various methods for post-training quantization, including: Integer Quantization: Integer quantization is a widely used post-training quantization technique. It reduces the precision of model weights and activations by mapping them to a fixed set of integers. Integer quantization is performed on a per-layer or per-tensor basis, resulting in models that use fixed-point representations. This reduces model size and memory usage, enabling faster inference on devices with limited computational resources. Dynamic Range Quantization: Dynamic range quantization is a technique that quantizes the weights and activations of the model based on their dynamic range. It uses histogram information to determine the optimal quantization ranges for each layer or tensor. Dynamic range quantization allows for fine-grained precision adjustment, resulting in more efficient models while maintaining reasonable accuracy. Full Integer Quantization: Full integer quantization extends integer quantization by quantizing the weights, activations, and gradients to integers. It eliminates the need for floating-point operations during inference, leading to further performance improvements on hardware platforms that support integer operations. Quantization-Aware Training: Quantization-aware training is a technique that trains models with quantization effects considered. It aims to minimize the accuracy loss caused by quantization by incorporating quantization-aware algorithms and simulations during the training process. TensorFlow provides tools and APIs to enable quantization-aware training, which allows models to learn and adapt to the quantization process. Fake Quantization: Fake quantization is a key component of quantization-aware training. It simulates the effects of quantization during forward and backward passes of the training process. Fake quantization replaces the actual quantization operations with non-learnable quantization functions that approximate the behavior of quantization during training. This allows the model to adjust and optimize its weights and activations to better handle quantization effects. Fine-Tuning: After quantization-aware training, fine-tuning techniques can be applied to further refine the quantized model. Fine-tuning involves training the quantized model with additional data or adjusting hyperparameters to improve accuracy while preserving the benefits of quantization. Framework 2: PyTorch PyTorch offers built-in capabilities to support the quantization of models. It provides tools and functions through its TorchVision library, which focuses on computer vision tasks and includes various functionalities for model quantization. Quantization Techniques: PyTorch's quantization support includes two main techniques: post-training static quantization and quantization-aware training. Post-training Static Quantization: Post-training static quantization is a technique that converts pre-trained floating-point models into quantized formats. PyTorch's static quantization allows developers to quantize their models without retraining. This technique involves analyzing the model to determine optimal quantization parameters and then quantizing the weights and activations accordingly. The quantization process reduces the precision of the model parameters and activations, resulting in lower memory usage and improved inference efficiency on hardware platforms that support quantized operations. Quantization-Aware Training: PyTorch supports quantization-aware training, which allows models to be trained with quantization effects considered. By incorporating quantization-aware algorithms and simulations during the training process, models can learn to be robust to the effects of quantization. This technique minimizes the accuracy loss caused by quantization by training the model with quantization in mind, effectively optimizing the model to handle lower precision representations. Results of the Pytorch Models: Framework 3: ONNX Runtime ONNX Runtime supports the quantization of models in the ONNX format. It provides capabilities for converting and executing quantized models on hardware platforms that support quantized operations, such as CPUs, GPUs, and specialized accelerators. With ONNX Runtime, developers can take advantage of quantization techniques to optimize their models for deployment on resource-constrained devices. The ability to execute quantized models efficiently across different hardware platforms makes ONNX Runtime a valuable framework for deploying quantized models in diverse real-world scenarios. Quantization Techniques: Pre-processing is to transform a float32 model to prepare it for quantization. It consists of the following three optional steps:Symbolic shape inference. This is best suited for transformer models. Model optimization: This step uses ONNX Runtime native library to rewrite the computation graph, including merging computation nodes, eliminating redundancies to improve runtime efficiency.ONNX shape inference. Dynamic Quantization: Dynamic quantization calculates the quantization parameters (scale and zero point) for activations dynamically. These calculations increase the cost of inference, while usually achieve higher accuracy comparing to static ones. Python API for dynamic quantization is in module onnxruntime.quantization.quantize, function quantize_dynamic() Static Quantization: Static quantization method first runs the model using a set of inputs called calibration data. During these runs, we compute the quantization parameters for each activations. These quantization parameters are written as constants to the quantized model and used for all inputs. Our quantization tool supports three calibration methods: MinMax, Entropy and Percentile. Please refer to calibrate.py for details. Python API for static quantization is in module onnxruntime.quantization.quantize, function quantize_static(). Please refer to quantize.py for details. Quantization Debugging: Quantization is not a loss-less transformation. It may negatively affect a model\u2019s accuracy. A solution to this problem is to compare the weights and activations tensors of the original computation graph vs those of the quantized one, identify where they differ most, and avoid quantizing these tensors, or choose another quantization/calibration method. This is called quantization debugging. To facilitate this process, we provide Python APIs for matching weights and activation tensors between a float32 model and its quantized counterpart. API for debugging is in module onnxruntime.quantization.qdq_loss_debug, which has the following functions: Function create_weight_matching(). It takes a float32 model and its quantized model, and output a dictionary that matches the corresponding weights between these two models. Function modify_model_output_intermediate_tensors(). It takes a float32 or quantized model, and augment it to save all its activations. Function collect_activations(). It takes a model augmented by modify_model_output_intermediate_tensors(), and an input data reader, runs the augmented model to collect all the activations. Function create_activation_matching(). You can imagine that you run collect_activations(modify_model_output_intermediate_tensors()) on both the float32 and its quantized model, to collect two sets of activations. This function takes these two set of activations, and matches up corresponding ones, so that they can be easily compared by the user. Framework 4: Caffe: Caffe's focus on efficiency and modularity makes it a suitable framework for quantizing models, especially in computer vision applications. By utilizing Caffe's quantization capabilities, developers can achieve optimized models that strike a balance between memory usage, computational efficiency, and inference speed, making them well-suited for deployment on resource-constrained devices. Quantization Techniques: Weight Quantization: Caffe supports weight quantization, where the precision of the model's weights is reduced. This involves converting the weights, which are typically stored as floating-point values, into lower precision representations such as fixed-point or integer formats. By quantizing the weights, the memory footprint of the model is reduced, enabling more efficient storage and faster computations. Activation Quantization: Caffe also supports activation quantization, which reduces the precision of the model's activations during inference. Similar to weight quantization, activation quantization involves converting the floating-point activations into lower precision formats. This reduces memory usage and computational requirements, leading to faster inference on devices with limited resources. Dynamic Fixed Point (DFP) Quantization: Caffe provides the Dynamic Fixed Point (DFP) quantization method. DFP quantization allows for the specification of different bit precisions for weights and activations, giving flexibility in optimizing the model for different hardware platforms. It supports fine-grained control over precision, enabling efficient utilization of available resources while maintaining reasonable accuracy. Network-Specific Quantization: Caffe allows for network-specific quantization, where quantization parameters can be customized for specific layers or parts of the network. This flexibility enables developers to optimize different sections of the model based on their requirements and resource constraints. For example, layers that are more critical for accuracy can be quantized with higher precision, while less critical layers can be quantized more aggressively. By leveraging these quantization techniques in Caffe, developers can achieve models that are optimized for deployment on resource-constrained devices. The reduced precision of weights and activations significantly reduces memory usage, enhances computational efficiency, and enables faster inference while maintaining reasonable accuracy. Quantizations Comparision of frameworks:","title":"Quantization"},{"location":"page07/#quantization","text":"This document aims to provide an overview of quantization, its importance, and its objectives in reducing data size, computational complexity, and memory requirements. By understanding quantization, you'll gain insights into a fundamental technique used in signal processing, data compression, and optimization.","title":"Quantization"},{"location":"page07/#definition-of-quantization-and-key-objectives","text":"Quantization is a process that involves mapping a continuous range of values to a discrete set of values. The main objectives of quantization include: Data Size Reduction: Quantization reduces the number of bits required to represent a given set of data. By mapping data to a smaller set of discrete values, the size of the data representation is reduced, leading to efficient storage and transmission. Computational Complexity Reduction: Quantization reduces the precision or granularity of data, which can significantly reduce the computational complexity of processing operations. This is particularly important in computationally intensive tasks where high precision is not always necessary. Memory Requirement Optimization: By representing data with fewer bits, quantization helps optimize memory utilization. This is especially valuable in scenarios where memory resources are limited, such as in embedded systems or mobile devices. Trade-off between Accuracy and Efficiency: Quantization involves a trade-off between preserving data accuracy and achieving efficiency gains. The challenge lies in finding the right balance between reducing data size and maintaining an acceptable level of fidelity in the reconstructed signal. By achieving these objectives, quantization enables the efficient storage, transmission, and processing of data, making it a valuable technique in various fields. In the following sections of this document, we will explore quantization techniques, frameworks, and their applications in greater detail, providing insights into how quantization is implemented and its impact in specific domains.","title":"Definition of Quantization and Key Objectives:"},{"location":"page07/#importance-of-quantization","text":"Quantization plays a crucial role in a wide range of domains, including signal processing, data compression, machine learning, and embedded systems. Here are some reasons why quantization is important: Efficient Storage and Transmission: In many applications, data needs to be stored or transmitted efficiently. Quantization allows for the reduction of data size, enabling more compact representation and efficient utilization of storage space or network bandwidth. Computational Complexity Reduction: High-precision computations can be computationally intensive and resource-consuming. By reducing the precision of data through quantization, the computational complexity can be significantly reduced, leading to faster and more efficient processing. Memory Requirement Optimization: Memory usage is a critical consideration, especially in resource-constrained systems. Quantization reduces the memory requirements by representing data with fewer bits, allowing for optimized memory utilization. Hardware Implementation: In embedded systems or hardware accelerators, quantization enables the implementation of efficient algorithms using fixed-point arithmetic, which is more suitable for hardware implementations compared to floating-point operations.","title":"Importance of Quantization:"},{"location":"page07/#basics-of-quantization","text":"Quantization is a fundamental concept in signal processing, data compression, and machine learning. It involves converting continuous and often analog data into a discrete representation. Let's explore the key concepts associated with quantization: Discretization of Continuous Data: Quantization involves dividing a continuous range of values into a finite number of discrete levels or bins. This allows us to represent the data using a limited set of values, resulting in a digital representation that can be processed, stored, or transmitted more efficiently. Representation of Discrete Levels: In quantization, the continuous data is mapped to the nearest discrete level within the defined set of quantization levels. Each level represents a range of values, and the data point is assigned to the closest level. The number of quantization levels determines the precision of the quantization. Quantization Error: Quantization introduces an inherent error called quantization error. It is the difference between the original value and its quantized representation. The quantization error arises because the discrete levels cannot precisely represent the infinite possibilities of the original continuous data. Minimizing quantization error is a key objective in quantization techniques. Impact on Fidelity: The quantization error has an impact on the fidelity or accuracy of the reconstructed data. Higher precision quantization (using more levels) can reduce the quantization error and preserve fidelity but requires more bits for representation. On the other hand, lower precision quantization (using fewer levels) reduces fidelity but offers higher efficiency in terms of storage, transmission, or computation.","title":"Basics of Quantization:"},{"location":"page07/#role-of-quantization","text":"Signal Processing: In signal processing, quantization is used to convert analog signals into digital representations. It is a fundamental step in analog-to-digital conversion, enabling further processing and manipulation of signals in digital systems. Data Compression: Quantization plays a crucial role in data compression techniques. By reducing the precision of data, quantization allows for more efficient compression algorithms, as the reduced number of bits required for representation leads to smaller file sizes or reduced transmission bandwidth. Machine Learning: In machine learning, quantization is employed to optimize the deployment of models on resource-constrained devices. Quantizing the model parameters and activations reduces memory usage and computational complexity, enabling efficient inference on edge devices without sacrificing too much accuracy. Quantization is a powerful technique that strikes a balance between efficient utilization of resources and maintaining an acceptable level of fidelity in various domains. Understanding the basics of quantization helps in exploring more advanced quantization methods and their applications in signal processing, data compression, and machine learning tasks.","title":"Role of Quantization:"},{"location":"page07/#types-of-quantization","text":"Uniform Quantization: As the name implies, the quantized levels in the uniform quantization process are equally spaced. The uniform quantization is further categorized as mid-rise type uniform quantization and mid-tread type uniform quantization. Both the uniform quantization processes are symmetric about the respective axis. Mid-rise type uniform Quantization: Rise refers to the rising part. The origin of the discrete quantized signal lies in the middle of the rising part of the stair like graph, as shown below: Mid-tread type uniform Quantization: Tread refers to the flat part. The origin of the discrete quantized signal lies in the middle of the tread part of the stair like graph, as shown below:","title":"Types of Quantization:"},{"location":"page07/#quantization-frameworks","text":"Quantization frameworks include TensorFlow with TensorFlow Model Optimization Toolkit, PyTorch with TorchVision, and ONNX Runtime. These frameworks provide a comprehensive set of tools and resources to simplify the process of quantizing models and deploying them on resource-constrained devices.","title":"Quantization Frameworks"},{"location":"page07/#framework-1-tensorflow","text":"TensorFlow's quantization capabilities are not limited to specific model architectures. The TensorFlow Model Optimization Toolkit supports quantization for a wide range of models. By leveraging TensorFlow's quantization support, developers can optimize their models for deployment on devices with limited computational resources, such as mobile phones, edge devices, or embedded systems. The quantized models achieve a balance between accuracy and efficiency, enabling efficient inference while minimizing memory usage and computational requirements.","title":"Framework 1: TensorFlow"},{"location":"page07/#quantization-techniques","text":"","title":"Quantization Techniques:"},{"location":"page07/#post-training-quantization","text":"Post-training quantization is a technique that involves converting pre-trained floating-point models into quantized formats. This technique allows models to be optimized without retraining. TensorFlow provides various methods for post-training quantization, including: Integer Quantization: Integer quantization is a widely used post-training quantization technique. It reduces the precision of model weights and activations by mapping them to a fixed set of integers. Integer quantization is performed on a per-layer or per-tensor basis, resulting in models that use fixed-point representations. This reduces model size and memory usage, enabling faster inference on devices with limited computational resources. Dynamic Range Quantization: Dynamic range quantization is a technique that quantizes the weights and activations of the model based on their dynamic range. It uses histogram information to determine the optimal quantization ranges for each layer or tensor. Dynamic range quantization allows for fine-grained precision adjustment, resulting in more efficient models while maintaining reasonable accuracy. Full Integer Quantization: Full integer quantization extends integer quantization by quantizing the weights, activations, and gradients to integers. It eliminates the need for floating-point operations during inference, leading to further performance improvements on hardware platforms that support integer operations.","title":"Post-training Quantization:"},{"location":"page07/#quantization-aware-training","text":"Quantization-aware training is a technique that trains models with quantization effects considered. It aims to minimize the accuracy loss caused by quantization by incorporating quantization-aware algorithms and simulations during the training process. TensorFlow provides tools and APIs to enable quantization-aware training, which allows models to learn and adapt to the quantization process. Fake Quantization: Fake quantization is a key component of quantization-aware training. It simulates the effects of quantization during forward and backward passes of the training process. Fake quantization replaces the actual quantization operations with non-learnable quantization functions that approximate the behavior of quantization during training. This allows the model to adjust and optimize its weights and activations to better handle quantization effects. Fine-Tuning: After quantization-aware training, fine-tuning techniques can be applied to further refine the quantized model. Fine-tuning involves training the quantized model with additional data or adjusting hyperparameters to improve accuracy while preserving the benefits of quantization.","title":"Quantization-Aware Training:"},{"location":"page07/#framework-2-pytorch","text":"PyTorch offers built-in capabilities to support the quantization of models. It provides tools and functions through its TorchVision library, which focuses on computer vision tasks and includes various functionalities for model quantization.","title":"Framework 2: PyTorch"},{"location":"page07/#quantization-techniques_1","text":"PyTorch's quantization support includes two main techniques: post-training static quantization and quantization-aware training.","title":"Quantization Techniques:"},{"location":"page07/#post-training-static-quantization","text":"Post-training static quantization is a technique that converts pre-trained floating-point models into quantized formats. PyTorch's static quantization allows developers to quantize their models without retraining. This technique involves analyzing the model to determine optimal quantization parameters and then quantizing the weights and activations accordingly. The quantization process reduces the precision of the model parameters and activations, resulting in lower memory usage and improved inference efficiency on hardware platforms that support quantized operations.","title":"Post-training Static Quantization:"},{"location":"page07/#quantization-aware-training_1","text":"PyTorch supports quantization-aware training, which allows models to be trained with quantization effects considered. By incorporating quantization-aware algorithms and simulations during the training process, models can learn to be robust to the effects of quantization. This technique minimizes the accuracy loss caused by quantization by training the model with quantization in mind, effectively optimizing the model to handle lower precision representations.","title":"Quantization-Aware Training:"},{"location":"page07/#results-of-the-pytorch-models","text":"","title":"Results of the Pytorch Models:"},{"location":"page07/#framework-3-onnx-runtime","text":"ONNX Runtime supports the quantization of models in the ONNX format. It provides capabilities for converting and executing quantized models on hardware platforms that support quantized operations, such as CPUs, GPUs, and specialized accelerators. With ONNX Runtime, developers can take advantage of quantization techniques to optimize their models for deployment on resource-constrained devices. The ability to execute quantized models efficiently across different hardware platforms makes ONNX Runtime a valuable framework for deploying quantized models in diverse real-world scenarios.","title":"Framework 3: ONNX Runtime"},{"location":"page07/#quantization-techniques_2","text":"Pre-processing is to transform a float32 model to prepare it for quantization. It consists of the following three optional steps:Symbolic shape inference. This is best suited for transformer models.","title":"Quantization Techniques:"},{"location":"page07/#model-optimization","text":"This step uses ONNX Runtime native library to rewrite the computation graph, including merging computation nodes, eliminating redundancies to improve runtime efficiency.ONNX shape inference.","title":"Model optimization:"},{"location":"page07/#dynamic-quantization","text":"Dynamic quantization calculates the quantization parameters (scale and zero point) for activations dynamically. These calculations increase the cost of inference, while usually achieve higher accuracy comparing to static ones. Python API for dynamic quantization is in module onnxruntime.quantization.quantize, function quantize_dynamic()","title":"Dynamic Quantization:"},{"location":"page07/#static-quantization","text":"Static quantization method first runs the model using a set of inputs called calibration data. During these runs, we compute the quantization parameters for each activations. These quantization parameters are written as constants to the quantized model and used for all inputs. Our quantization tool supports three calibration methods: MinMax, Entropy and Percentile. Please refer to calibrate.py for details. Python API for static quantization is in module onnxruntime.quantization.quantize, function quantize_static(). Please refer to quantize.py for details.","title":"Static Quantization:"},{"location":"page07/#quantization-debugging","text":"Quantization is not a loss-less transformation. It may negatively affect a model\u2019s accuracy. A solution to this problem is to compare the weights and activations tensors of the original computation graph vs those of the quantized one, identify where they differ most, and avoid quantizing these tensors, or choose another quantization/calibration method. This is called quantization debugging. To facilitate this process, we provide Python APIs for matching weights and activation tensors between a float32 model and its quantized counterpart. API for debugging is in module onnxruntime.quantization.qdq_loss_debug, which has the following functions: Function create_weight_matching(). It takes a float32 model and its quantized model, and output a dictionary that matches the corresponding weights between these two models. Function modify_model_output_intermediate_tensors(). It takes a float32 or quantized model, and augment it to save all its activations. Function collect_activations(). It takes a model augmented by modify_model_output_intermediate_tensors(), and an input data reader, runs the augmented model to collect all the activations. Function create_activation_matching(). You can imagine that you run collect_activations(modify_model_output_intermediate_tensors()) on both the float32 and its quantized model, to collect two sets of activations. This function takes these two set of activations, and matches up corresponding ones, so that they can be easily compared by the user.","title":"Quantization Debugging:"},{"location":"page07/#framework-4-caffe","text":"Caffe's focus on efficiency and modularity makes it a suitable framework for quantizing models, especially in computer vision applications. By utilizing Caffe's quantization capabilities, developers can achieve optimized models that strike a balance between memory usage, computational efficiency, and inference speed, making them well-suited for deployment on resource-constrained devices.","title":"Framework 4: Caffe:"},{"location":"page07/#quantization-techniques_3","text":"","title":"Quantization Techniques:"},{"location":"page07/#weight-quantization","text":"Caffe supports weight quantization, where the precision of the model's weights is reduced. This involves converting the weights, which are typically stored as floating-point values, into lower precision representations such as fixed-point or integer formats. By quantizing the weights, the memory footprint of the model is reduced, enabling more efficient storage and faster computations.","title":"Weight Quantization:"},{"location":"page07/#activation-quantization","text":"Caffe also supports activation quantization, which reduces the precision of the model's activations during inference. Similar to weight quantization, activation quantization involves converting the floating-point activations into lower precision formats. This reduces memory usage and computational requirements, leading to faster inference on devices with limited resources.","title":"Activation Quantization:"},{"location":"page07/#dynamic-fixed-point-dfp-quantization","text":"Caffe provides the Dynamic Fixed Point (DFP) quantization method. DFP quantization allows for the specification of different bit precisions for weights and activations, giving flexibility in optimizing the model for different hardware platforms. It supports fine-grained control over precision, enabling efficient utilization of available resources while maintaining reasonable accuracy.","title":"Dynamic Fixed Point (DFP) Quantization:"},{"location":"page07/#network-specific-quantization","text":"Caffe allows for network-specific quantization, where quantization parameters can be customized for specific layers or parts of the network. This flexibility enables developers to optimize different sections of the model based on their requirements and resource constraints. For example, layers that are more critical for accuracy can be quantized with higher precision, while less critical layers can be quantized more aggressively. By leveraging these quantization techniques in Caffe, developers can achieve models that are optimized for deployment on resource-constrained devices. The reduced precision of weights and activations significantly reduces memory usage, enhances computational efficiency, and enables faster inference while maintaining reasonable accuracy.","title":"Network-Specific Quantization:"},{"location":"page07/#quantizations-comparision-of-frameworks","text":"","title":"Quantizations Comparision of frameworks:"},{"location":"page08/","text":"Knowledge Distillation Knowledge Distillation is a technique used in machine learning to transfer the knowledge from a large, complex model to a smaller, simpler model. It involves training the smaller model to mimic the output of the larger model, by learning from the soft targets (probabilistic predictions) generated by the larger model, rather than the hard targets (actual labels) in the training data. The main idea behind knowledge distillation is that a large, complex model has learned to capture the underlying patterns and structure in the data, which can be transferred to a smaller, simpler model through the soft targets generated by the larger model. By training the smaller model to mimic the output of the larger model, the smaller model can achieve comparable performance to the larger model, while using fewer resources and being more computationally efficient. Knowledge Distillation is typically performed after a large, complex model has been trained and can be used to create a smaller model that is optimized for deployment in scenarios with limited computational resources, such as mobile devices or IoT devices. In addition to reducing the size and complexity of a model, knowledge distillation can also help to reduce overfitting, as the smaller model is trained to learn from the soft targets generated by the larger model, which can be more robust to noisy or incomplete data. Overall, knowledge distillation is a powerful technique for transferring knowledge from a large, complex model to a smaller, simpler model, while maintaining the performance and accuracy of the original model. The different kinds of knowledge in a teacher model 1. Response-based knowledge As shown in Figure 2, response-based knowledge focuses on the final output layer of the teacher model. The hypothesis is that the student model will learn to mimic the predictions of the teacher model. As illustrated in Figure 3, This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss is minimized over training, the student model will become better at making the same predictions as the teacher. In the context of computer vision tasks like image classification, the soft targets comprise the response-based knowledge. Soft targets represent the probability distribution over the output classes and typically estimated using a softmax function. Each soft target\u2019s contribution to the knowledge is modulated using a parameter called temperature. Response-based knowledge distillation based on soft targets is usually used in the context of supervised learning. 2. Feature-based knowledge A trained teacher model also captures knowledge of the data in its intermediate layers, which is especially pertinent for deep neural networks. The intermediate layers learn to discriminate specific features and this knowledge can be used to train a student model. As shown in Figure 4, the goal is to train the student model to learn the same feature activations as the teacher model. The distillation loss function achieves this by minimizing the difference between the feature activations of the teacher and the student models. 3. Relation-based knowledge In addition to knowledge represented in the output layers and the intermediate layers of a neural network, knowledge that captures the relationship between feature maps can also be used to train a student model. This form of knowledge, termed as relation-based knowledge is depicted in Figure 5. This relationship can be modeled as correlation between feature maps, graphs, similarity matrix, feature embeddings, or probabilistic distributions based on feature representations. Training There are three principal types of methods for training student and teacher models, namely offline, online and self distillation. The categorization of the distillation training methods depends on whether the teacher model is modified at the same time as the student model or not 1. Offline distillation Offline distillation is the most common method, where a pre-trained teacher model is used to guide the student model. In this scheme, the teacher model is first pre-trained on a training dataset, and then knowledge from the teacher model is distilled to train the student model. Given the recent advances in deep learning, a wide variety of pre-trained neural network models are openly available that can serve as the teacher depending on the use case. Offline distillation is an established technique in deep learning and easier to implement. 2. Online distillation In offline distillation, the pre-trained teacher model is usually a large capacity deep neural network. For several use cases, a pre-trained model may not be available for offline distillation. To address this limitation, online distillation can be used where both the teacher and student models are updated simultaneously in a single end-to-end training process. Online distillation can be operationalized using parallel computing thus making it a highly efficient method. 3. Self-distillation As shown in Figure 6, in self-distillation, the same model is used for the teacher and the student models. For instance, knowledge from deeper layers of a deep neural network can be used to train the shallow layers. It can be considered a special case of online distillation, and instantiated in several ways. Knowledge from earlier epochs of the teacher model can be transferred to its later epochs to train the student model. Architecture The design of the student-teacher network architecture is critical for efficient knowledge acquisition and distillation. Typically, there is a model capacity gap between the more complex teacher model and the simpler student model. This structural gap can be reduced through optimizing knowledge transfer via efficient student-teacher architectures. Transferring knowledge from deep neural networks is not straightforward due to their depth as well as breadth. The most common architectures for knowledge transfer include a student model that is: a shallower version of the teacher model with fewer layers and fewer neurons per layer, a quantized version of the teacher model, a smaller network with efficient basic operations, a smaller networks with optimized global network architecture, the same model as the teacher. In addition to the above methods, recent advances like neural architecture search can also be employed for designing an optimal student model architecture given a particular teacher model.","title":"Knowledge Distillation"},{"location":"page08/#knowledge-distillation","text":"Knowledge Distillation is a technique used in machine learning to transfer the knowledge from a large, complex model to a smaller, simpler model. It involves training the smaller model to mimic the output of the larger model, by learning from the soft targets (probabilistic predictions) generated by the larger model, rather than the hard targets (actual labels) in the training data. The main idea behind knowledge distillation is that a large, complex model has learned to capture the underlying patterns and structure in the data, which can be transferred to a smaller, simpler model through the soft targets generated by the larger model. By training the smaller model to mimic the output of the larger model, the smaller model can achieve comparable performance to the larger model, while using fewer resources and being more computationally efficient. Knowledge Distillation is typically performed after a large, complex model has been trained and can be used to create a smaller model that is optimized for deployment in scenarios with limited computational resources, such as mobile devices or IoT devices. In addition to reducing the size and complexity of a model, knowledge distillation can also help to reduce overfitting, as the smaller model is trained to learn from the soft targets generated by the larger model, which can be more robust to noisy or incomplete data. Overall, knowledge distillation is a powerful technique for transferring knowledge from a large, complex model to a smaller, simpler model, while maintaining the performance and accuracy of the original model.","title":"Knowledge Distillation"},{"location":"page08/#the-different-kinds-of-knowledge-in-a-teacher-model","text":"","title":"The different kinds of knowledge in a teacher model"},{"location":"page08/#1-response-based-knowledge","text":"As shown in Figure 2, response-based knowledge focuses on the final output layer of the teacher model. The hypothesis is that the student model will learn to mimic the predictions of the teacher model. As illustrated in Figure 3, This can be achieved by using a loss function, termed the distillation loss, that captures the difference between the logits of the student and the teacher model respectively. As this loss is minimized over training, the student model will become better at making the same predictions as the teacher. In the context of computer vision tasks like image classification, the soft targets comprise the response-based knowledge. Soft targets represent the probability distribution over the output classes and typically estimated using a softmax function. Each soft target\u2019s contribution to the knowledge is modulated using a parameter called temperature. Response-based knowledge distillation based on soft targets is usually used in the context of supervised learning.","title":"1. Response-based knowledge"},{"location":"page08/#2-feature-based-knowledge","text":"A trained teacher model also captures knowledge of the data in its intermediate layers, which is especially pertinent for deep neural networks. The intermediate layers learn to discriminate specific features and this knowledge can be used to train a student model. As shown in Figure 4, the goal is to train the student model to learn the same feature activations as the teacher model. The distillation loss function achieves this by minimizing the difference between the feature activations of the teacher and the student models.","title":"2. Feature-based knowledge"},{"location":"page08/#3-relation-based-knowledge","text":"In addition to knowledge represented in the output layers and the intermediate layers of a neural network, knowledge that captures the relationship between feature maps can also be used to train a student model. This form of knowledge, termed as relation-based knowledge is depicted in Figure 5. This relationship can be modeled as correlation between feature maps, graphs, similarity matrix, feature embeddings, or probabilistic distributions based on feature representations.","title":"3. Relation-based knowledge"},{"location":"page08/#training","text":"There are three principal types of methods for training student and teacher models, namely offline, online and self distillation. The categorization of the distillation training methods depends on whether the teacher model is modified at the same time as the student model or not","title":"Training"},{"location":"page08/#1-offline-distillation","text":"Offline distillation is the most common method, where a pre-trained teacher model is used to guide the student model. In this scheme, the teacher model is first pre-trained on a training dataset, and then knowledge from the teacher model is distilled to train the student model. Given the recent advances in deep learning, a wide variety of pre-trained neural network models are openly available that can serve as the teacher depending on the use case. Offline distillation is an established technique in deep learning and easier to implement.","title":"1. Offline distillation"},{"location":"page08/#2-online-distillation","text":"In offline distillation, the pre-trained teacher model is usually a large capacity deep neural network. For several use cases, a pre-trained model may not be available for offline distillation. To address this limitation, online distillation can be used where both the teacher and student models are updated simultaneously in a single end-to-end training process. Online distillation can be operationalized using parallel computing thus making it a highly efficient method.","title":"2. Online distillation"},{"location":"page08/#3-self-distillation","text":"As shown in Figure 6, in self-distillation, the same model is used for the teacher and the student models. For instance, knowledge from deeper layers of a deep neural network can be used to train the shallow layers. It can be considered a special case of online distillation, and instantiated in several ways. Knowledge from earlier epochs of the teacher model can be transferred to its later epochs to train the student model.","title":"3. Self-distillation"},{"location":"page08/#architecture","text":"The design of the student-teacher network architecture is critical for efficient knowledge acquisition and distillation. Typically, there is a model capacity gap between the more complex teacher model and the simpler student model. This structural gap can be reduced through optimizing knowledge transfer via efficient student-teacher architectures. Transferring knowledge from deep neural networks is not straightforward due to their depth as well as breadth. The most common architectures for knowledge transfer include a student model that is: a shallower version of the teacher model with fewer layers and fewer neurons per layer, a quantized version of the teacher model, a smaller network with efficient basic operations, a smaller networks with optimized global network architecture, the same model as the teacher. In addition to the above methods, recent advances like neural architecture search can also be employed for designing an optimal student model architecture given a particular teacher model.","title":"Architecture"},{"location":"page09/","text":"Compression Compression is a technique used in machine learning to reduce the storage requirements of trained models, without significantly impacting their accuracy or performance. It involves using algorithms to compress the weights and activations of a model, which can greatly reduce the amount of memory required to store the model. There are different types of compression algorithms used in machine learning, such as: Weight sharing: This technique involves sharing the same weight value between multiple connections in the neural network, which can greatly reduce the number of weights required to store the model. Huffman coding: This technique involves encoding weights using a variable-length code, where frequently occurring weights are represented by shorter codes, and less frequent weights are represented by longer codes. Singular value decomposition (SVD): This technique involves factorizing the weight matrix of a model into smaller matrices, which can reduce the number of weights required to store the model. Pruning: This technique involves removing unnecessary connections or neurons from a model, which can reduce its size and improve its speed, as mentioned in the previous answer. Compression techniques can be applied to a model after it has been trained and can be used in conjunction with other optimization techniques, such as quantization and pruning, to further reduce the size and complexity of a model. Compression can also be applied iteratively, where a model is compressed and then retrained, to achieve even greater reductions in size and complexity. Overall, compression is a useful technique for reducing the storage requirements of machine learning models, particularly in scenarios where memory is limited, such as on embedded devices or in cloud-based applications with high storage costs.","title":"Compression"},{"location":"page09/#compression","text":"Compression is a technique used in machine learning to reduce the storage requirements of trained models, without significantly impacting their accuracy or performance. It involves using algorithms to compress the weights and activations of a model, which can greatly reduce the amount of memory required to store the model. There are different types of compression algorithms used in machine learning, such as:","title":"Compression"},{"location":"page09/#weight-sharing","text":"This technique involves sharing the same weight value between multiple connections in the neural network, which can greatly reduce the number of weights required to store the model.","title":"Weight sharing:"},{"location":"page09/#huffman-coding","text":"This technique involves encoding weights using a variable-length code, where frequently occurring weights are represented by shorter codes, and less frequent weights are represented by longer codes.","title":"Huffman coding:"},{"location":"page09/#singular-value-decomposition-svd","text":"This technique involves factorizing the weight matrix of a model into smaller matrices, which can reduce the number of weights required to store the model.","title":"Singular value decomposition (SVD):"},{"location":"page09/#pruning","text":"This technique involves removing unnecessary connections or neurons from a model, which can reduce its size and improve its speed, as mentioned in the previous answer. Compression techniques can be applied to a model after it has been trained and can be used in conjunction with other optimization techniques, such as quantization and pruning, to further reduce the size and complexity of a model. Compression can also be applied iteratively, where a model is compressed and then retrained, to achieve even greater reductions in size and complexity. Overall, compression is a useful technique for reducing the storage requirements of machine learning models, particularly in scenarios where memory is limited, such as on embedded devices or in cloud-based applications with high storage costs.","title":"Pruning:"},{"location":"page10/","text":"Hardware Acceleration Hardware acceleration is a technique used in machine learning to speed up the training and inference of neural networks by using specialized hardware, such as GPUs (Graphics Processing Units), TPUs (Tensor Processing Units), or FPGAs (Field Programmable Gate Arrays). These hardware devices are designed specifically for performing matrix operations, which are a fundamental component of neural network computations. The main advantage of hardware acceleration is that it can greatly reduce the time required to train and run neural networks, as these specialized hardware devices can perform matrix operations much faster than general-purpose CPUs. This allows for faster experimentation and iteration of models, which can greatly accelerate the development and deployment of machine learning applications. Hardware acceleration can also enable the deployment of machine learning models on resource-constrained devices, such as mobile phones or IoT devices, by offloading the compute-intensive tasks to specialized hardware. This can enable real-time inference and processing of data, which can be critical in certain applications, such as self-driving cars or industrial automation. Overall, hardware acceleration is a powerful technique for accelerating the training and inference of neural networks, and is an essential component of many modern machine learning frameworks and applications.","title":"Hardware Acceleration"},{"location":"page10/#hardware-acceleration","text":"Hardware acceleration is a technique used in machine learning to speed up the training and inference of neural networks by using specialized hardware, such as GPUs (Graphics Processing Units), TPUs (Tensor Processing Units), or FPGAs (Field Programmable Gate Arrays). These hardware devices are designed specifically for performing matrix operations, which are a fundamental component of neural network computations. The main advantage of hardware acceleration is that it can greatly reduce the time required to train and run neural networks, as these specialized hardware devices can perform matrix operations much faster than general-purpose CPUs. This allows for faster experimentation and iteration of models, which can greatly accelerate the development and deployment of machine learning applications. Hardware acceleration can also enable the deployment of machine learning models on resource-constrained devices, such as mobile phones or IoT devices, by offloading the compute-intensive tasks to specialized hardware. This can enable real-time inference and processing of data, which can be critical in certain applications, such as self-driving cars or industrial automation. Overall, hardware acceleration is a powerful technique for accelerating the training and inference of neural networks, and is an essential component of many modern machine learning frameworks and applications.","title":"Hardware Acceleration"},{"location":"page11/","text":"PyTorch PyTorch is an open source machine learning (ML) framework based on the Python programming language and the Torch library. Torch is an open source ML library used for creating deep neural networks PyTorch provides the following key features: Tensor computation. Similar to NumPy array -- an open source library of Python that adds support for large, multidimensional arrays -- tensors are generic n-dimensional arrays used for arbitrary numeric computation and are accelerated by graphics processing units. These multidimensional structures can be operated on and manipulated with application program interfaces (APIs). TorchScript. This is the production environment of PyTorch that enables users to seamlessly transition between modes. TorchScript optimizes functionality, speed, ease of use and flexibility. Dynamic graph computation. This feature lets users change network behavior on the fly, rather than waiting for all the code to be executed. Automatic differentiation. This technique is used for creating and training neural networks. It numerically computes the derivative of a function by making backward passes in neural networks. Python support. Because PyTorch is based on Python, it can be used with popular libraries and packages such as NumPy, SciPy, Numba and Cynthon. PRODUCTION READY With TorchScript, PyTorch provides ease-of-use and flexibility in eager mode, while seamlessly transitioning to graph mode for speed, optimization, and functionality in C++ runtime environments. TorchServe is an easy to use tool for deploying PyTorch models at scale. It is cloud and environment agnostic and supports features such as multi-model serving, logging, metrics and the creation of RESTful endpoints for application integration. DISTRIBUTED TRAINING Optimize performance in both research and production by taking advantage of native support for asynchronous execution of collective operations and peer-to-peer communication that is accessible from Python and C++. MOBILE (EXPERIMENTAL) PyTorch supports an end-to-end workflow from Python to deployment on iOS and Android. It extends the PyTorch API to cover common preprocessing and integration tasks needed for incorporating ML in mobile applications. ROBUST ECOSYSTEM An active community of researchers and developers have built a rich ecosystem of tools and libraries for extending PyTorch and supporting development in areas from computer vision to reinforcement learning. NATIVE ONNX SUPPORT Export models in the standard ONNX (Open Neural Network Exchange) format for direct access to ONNX-compatible platforms, runtimes, visualizers, and more. C++ FRONT-END The C++ frontend is a pure C++ interface to PyTorch that follows the design and architecture of the established Python frontend. It is intended to enable research in high performance, low latency and bare metal C++ applications CLOUD SUPPORT PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling through prebuilt images, large scale training on GPUs, ability to run models in a production scale environment, and more. PyTorch vs TensorFlow Both TensorFlow and PyTorch offer useful abstractions that ease the development of models by reducing boilerplate code. They differ because PyTorch has a more \"pythonic\" approach and is object-oriented, while TensorFlow offers a variety of options. PyTorch is used for many deep learning projects today, and its popularity is increasing among AI researchers, although of the three main frameworks, it is the least popular. Trends show that this may change soon. When researchers want flexibility, debugging capabilities, and short training duration, they choose PyTorch. It runs on Linux, macOS, and Windows. TensorFlow is the favorite tool of many industry professionals and researchers. TensorFlow offers better visualization, which allows developers to debug better and track the training process. PyTorch, however, provides only limited visualization. TensorFlow also beats PyTorch in deploying trained models to production, thanks to the TensorFlow Serving framework. PyTorch offers no such framework, so developers need to use Django or Flask as a back-end server. In the area of data parallelism, PyTorch gains optimal performance by relying on native support for asynchronous execution through Python. However, with TensorFlow, you must manually code and optimize every operation run on a specific device to allow distributed training. PyTorch vs Keras Keras is better suited for developers who want a plug-and-play framework that lets them build, train, and evaluate their models quickly. Keras also offers more deployment options and easier model export. However, remember that PyTorch is faster than Keras and has better debugging capabilities. Both platforms enjoy sufficient levels of popularity that they offer plenty of learning resources. Keras has excellent access to reusable code and tutorials, while PyTorch has outstanding community support and active development. Keras is the best when working with small datasets, rapid prototyping, and multiple back-end support. It\u2019s the most popular framework thanks to its comparative simplicity. It runs on Linux, MacOS, and Windows","title":"PyTorch"},{"location":"page11/#pytorch","text":"PyTorch is an open source machine learning (ML) framework based on the Python programming language and the Torch library. Torch is an open source ML library used for creating deep neural networks","title":"PyTorch"},{"location":"page11/#pytorch-provides-the-following-key-features","text":"Tensor computation. Similar to NumPy array -- an open source library of Python that adds support for large, multidimensional arrays -- tensors are generic n-dimensional arrays used for arbitrary numeric computation and are accelerated by graphics processing units. These multidimensional structures can be operated on and manipulated with application program interfaces (APIs). TorchScript. This is the production environment of PyTorch that enables users to seamlessly transition between modes. TorchScript optimizes functionality, speed, ease of use and flexibility. Dynamic graph computation. This feature lets users change network behavior on the fly, rather than waiting for all the code to be executed. Automatic differentiation. This technique is used for creating and training neural networks. It numerically computes the derivative of a function by making backward passes in neural networks. Python support. Because PyTorch is based on Python, it can be used with popular libraries and packages such as NumPy, SciPy, Numba and Cynthon. PRODUCTION READY With TorchScript, PyTorch provides ease-of-use and flexibility in eager mode, while seamlessly transitioning to graph mode for speed, optimization, and functionality in C++ runtime environments. TorchServe is an easy to use tool for deploying PyTorch models at scale. It is cloud and environment agnostic and supports features such as multi-model serving, logging, metrics and the creation of RESTful endpoints for application integration. DISTRIBUTED TRAINING Optimize performance in both research and production by taking advantage of native support for asynchronous execution of collective operations and peer-to-peer communication that is accessible from Python and C++. MOBILE (EXPERIMENTAL) PyTorch supports an end-to-end workflow from Python to deployment on iOS and Android. It extends the PyTorch API to cover common preprocessing and integration tasks needed for incorporating ML in mobile applications. ROBUST ECOSYSTEM An active community of researchers and developers have built a rich ecosystem of tools and libraries for extending PyTorch and supporting development in areas from computer vision to reinforcement learning. NATIVE ONNX SUPPORT Export models in the standard ONNX (Open Neural Network Exchange) format for direct access to ONNX-compatible platforms, runtimes, visualizers, and more. C++ FRONT-END The C++ frontend is a pure C++ interface to PyTorch that follows the design and architecture of the established Python frontend. It is intended to enable research in high performance, low latency and bare metal C++ applications CLOUD SUPPORT PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling through prebuilt images, large scale training on GPUs, ability to run models in a production scale environment, and more.","title":"PyTorch provides the following key features:"},{"location":"page11/#pytorch-vs-tensorflow","text":"Both TensorFlow and PyTorch offer useful abstractions that ease the development of models by reducing boilerplate code. They differ because PyTorch has a more \"pythonic\" approach and is object-oriented, while TensorFlow offers a variety of options. PyTorch is used for many deep learning projects today, and its popularity is increasing among AI researchers, although of the three main frameworks, it is the least popular. Trends show that this may change soon. When researchers want flexibility, debugging capabilities, and short training duration, they choose PyTorch. It runs on Linux, macOS, and Windows. TensorFlow is the favorite tool of many industry professionals and researchers. TensorFlow offers better visualization, which allows developers to debug better and track the training process. PyTorch, however, provides only limited visualization. TensorFlow also beats PyTorch in deploying trained models to production, thanks to the TensorFlow Serving framework. PyTorch offers no such framework, so developers need to use Django or Flask as a back-end server. In the area of data parallelism, PyTorch gains optimal performance by relying on native support for asynchronous execution through Python. However, with TensorFlow, you must manually code and optimize every operation run on a specific device to allow distributed training.","title":"PyTorch vs TensorFlow"},{"location":"page11/#pytorch-vs-keras","text":"Keras is better suited for developers who want a plug-and-play framework that lets them build, train, and evaluate their models quickly. Keras also offers more deployment options and easier model export. However, remember that PyTorch is faster than Keras and has better debugging capabilities. Both platforms enjoy sufficient levels of popularity that they offer plenty of learning resources. Keras has excellent access to reusable code and tutorials, while PyTorch has outstanding community support and active development. Keras is the best when working with small datasets, rapid prototyping, and multiple back-end support. It\u2019s the most popular framework thanks to its comparative simplicity. It runs on Linux, MacOS, and Windows","title":"PyTorch vs Keras"},{"location":"page12/","text":"ONNX ONNX, or Open Neural Network Exchange, is an open-source standard for representing deep learning models. It was developed by Facebook and Microsoft in order to make it easier for researchers and engineers to move models between different deep-learning frameworks and hardware platforms. it allows models to be easily exported from one framework, such as PyTorch, and imported into another framework, such as TensorFlow. This can be especially useful for researchers who want to try out different frameworks for training and deploying their models ONNX also provides a set of tools for optimizing and quantizing models, which can help to reduce the memory and computational requirements of the model. This can be especially useful for deploying models on edge devices and other resource-constrained environments. Another important feature of ONNX is that it is supported by a wide range of companies and organizations. This includes not only Facebook and Microsoft, but also companies like Amazon, NVIDIA, and Intel. This wide range of support ensures that ONNX will continue to be actively developed and maintained, making it a robust and stable standard for representing deep learning models. -ONNX Runtime is an open-source inference engine for executing ONNX (Open Neural Network Exchange) models. It is designed to be high-performance and lightweight, making it well-suited for deployment on a wide range of hardware platforms, including edge devices, servers, and cloud services. -The ONNX Runtime provides a C++ API, a C# API, and a Python API for executing ONNX models. It also provides support for multiple backends, including CUDA and OpenCL, which allows it to run on a wide range of hardware platforms, such as NVIDIA GPUs and Intel CPUs. -ONNX Runtime can be very useful since you can use models in inference with a single framework no matter what hardware you are going to use. So without having to actually rewrite the code depending on whether we want to use a CPU, GPU, FPGA or whatever. -ONNX Runtime also provides support for a wide range of models, including both traditional machine learning models and deep learning models. -One of the main advantages of ONNX Runtime is its performance. It uses various techniques such as Just-In-Time (JIT) compilation, kernel fusion and subgraph partitioning to optimize the performance of the model. It also supports thread pooling and inter-node communication for distributed deployment which makes it a suitable choice for large-scale deployment. -One of the main advantages of ONNX Runtime is its performance. It uses various techniques such as Just-In-Time (JIT) compilation, kernel fusion and subgraph partitioning to optimize the performance of the model. It also supports thread pooling and inter-node communication for distributed deployment which makes it a suitable choice for large-scale deployment. FOR EXAMPLE: Let us look at an example scenario where your goal is to deploy your trained model via an iOS app. You are comfortable putting together neural network models in Keras, an easy-to-use interface for the TensorFlow framework. Your journey starts by understanding the hardware through which your app gets used by the user. Modern Apple devices come with Apple Neural Engine (ANE) as part of the chip. The A15 Bionic chip on iPhone 14 comes with 6-core CPU, 5-core GPU, and a 16-core Neural Engine (ANE). On this device, your deep learning model can work on CPU only, CPU and GPU, or all computing engines (CPU, GPU, and ANE). ANE is the fastest way to run your model, especially for heavy applications. On iOS, CoreML is the framework that is optimized for deep learning inference. Also, CoreML is built into the operating system which means there is no need for you to compile, link, or ship your ML binary libraries with the app. Now that your target framework is defined, you need to find a way to convert your trained model into Core ML. This task can be easily achieved via ONNX - convert your Keras model to ONNX using the tf2onnx library and convert ONNX to Core ML model using the onnx-coreml library in python. The Core ML model is now ready to be deployed into your app. BUILD ONNX FROM SOURCE Before building from source uninstall any existing versions of onnx pip uninstall onnx. c++17 or higher C++ compiler version is required to build ONNX from source on Windows. For other platforms, please use C++11 or higher versions. Windows If you are building ONNX from source, it is recommended that you also build Protobuf locally as a static library. The version distributed with conda-forge is a DLL, but ONNX expects it to be a static library. Building protobuf locally also lets you control the version of protobuf. The tested and recommended version is 3.20.2. The instructions in this README assume you are using Visual Studio. It is recommended that you run all the commands from a shell started from \"x64 Native Tools Command Prompt for VS 2019\" and keep the build system generator for cmake (e.g., cmake -G \"Visual Studio 16 2019\") consistent while building protobuf as well as ONNX. CONVERSION STEPS -> All frameworks have their own way to convert their models to ONNX. But there are some common steps that you need to follow.only saving the model weighs we ll not be able to convert to ONNX,the model architecture is required and really important to convert your model to onnx. -> Model weights are the weights of the different layers which are used to compute the output of the model. So, they are equally important to successfully convert the model. input names and output names -> we will need to define the input names and the output names of our model. These metadata are used to describe the inputs and outputs of your model. ONNX will trace the different layers of your model in order to create a graph of theses layers. -> While tracing the layers, ONNX will also need an input sample to understand how the model is working and what operators are used to compute the outputs. -> The selected sample will be the input of the first layer of the model and is also used to define the input shape of the model. dynamic axes ->Then, ONNX requires to know the dynamic axes of your model. Most of the time during the conversion, you will use a batch size of 1. ->But if you want to have a model that can take a batch of N samples, you will need to define the dynamic axes of your model to accept a batch of N samples. conversion evaluation Finally, we need to evaluate the converted model to ensure that it is a sustainable ONNX model and it is working as expected. There are two separate steps to evaluate the converted model. The first step is to use the ONNX\u2019s API to check the model\u2019s validity. This is done by calling the onnx.checker.check_model function. This will verify the model\u2019s structure and confirm if the model has a valid ONNX scheme or not. Each node in the model is evaluated by checking the inputs and outputs of the node. The second step is to compare the output of the converted model with the output of the original model. This is done by comparing both outputs with the numpy.testing.assert_allclose function. This function will compare the two outputs and will raise an error if the two outputs are not equal, based on the rtol and atol parameters. conversion of a pytorch model to onnx import torch import torchvision from PIL import Image import numpy as np !pip3 install onnx ~ here we will load the pretrained ResNet18: resnet = torchvision.models.resnet18(pretrained=True) ~ We will download an example image from PyTorhc import urllib url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\") # Notebook Link will be in description urllib.request.urlretrieve(url, filename) from torchvision import transforms inp_image = Image.open('/content/dog.jpg') preprocess = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) input_tensor = preprocess(inp_image) inp_batch = input_tensor.unsqueeze(0) device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") inp_batch.to(device) resnet.to(device) with torch.no_grad(): output = resnet(inp_batch) print(output[0]) ~ Output of shape 1000, confidence scores for each of the imagenet classes import torch.onnx torch.onnx.export(resnet, inp_batch, \"resnet18.onnx\", export_params=True, opset_version=10)","title":"ONNX"},{"location":"page12/#onnx","text":"ONNX, or Open Neural Network Exchange, is an open-source standard for representing deep learning models. It was developed by Facebook and Microsoft in order to make it easier for researchers and engineers to move models between different deep-learning frameworks and hardware platforms. it allows models to be easily exported from one framework, such as PyTorch, and imported into another framework, such as TensorFlow. This can be especially useful for researchers who want to try out different frameworks for training and deploying their models ONNX also provides a set of tools for optimizing and quantizing models, which can help to reduce the memory and computational requirements of the model. This can be especially useful for deploying models on edge devices and other resource-constrained environments. Another important feature of ONNX is that it is supported by a wide range of companies and organizations. This includes not only Facebook and Microsoft, but also companies like Amazon, NVIDIA, and Intel. This wide range of support ensures that ONNX will continue to be actively developed and maintained, making it a robust and stable standard for representing deep learning models. -ONNX Runtime is an open-source inference engine for executing ONNX (Open Neural Network Exchange) models. It is designed to be high-performance and lightweight, making it well-suited for deployment on a wide range of hardware platforms, including edge devices, servers, and cloud services. -The ONNX Runtime provides a C++ API, a C# API, and a Python API for executing ONNX models. It also provides support for multiple backends, including CUDA and OpenCL, which allows it to run on a wide range of hardware platforms, such as NVIDIA GPUs and Intel CPUs. -ONNX Runtime can be very useful since you can use models in inference with a single framework no matter what hardware you are going to use. So without having to actually rewrite the code depending on whether we want to use a CPU, GPU, FPGA or whatever. -ONNX Runtime also provides support for a wide range of models, including both traditional machine learning models and deep learning models. -One of the main advantages of ONNX Runtime is its performance. It uses various techniques such as Just-In-Time (JIT) compilation, kernel fusion and subgraph partitioning to optimize the performance of the model. It also supports thread pooling and inter-node communication for distributed deployment which makes it a suitable choice for large-scale deployment. -One of the main advantages of ONNX Runtime is its performance. It uses various techniques such as Just-In-Time (JIT) compilation, kernel fusion and subgraph partitioning to optimize the performance of the model. It also supports thread pooling and inter-node communication for distributed deployment which makes it a suitable choice for large-scale deployment. FOR EXAMPLE: Let us look at an example scenario where your goal is to deploy your trained model via an iOS app. You are comfortable putting together neural network models in Keras, an easy-to-use interface for the TensorFlow framework. Your journey starts by understanding the hardware through which your app gets used by the user. Modern Apple devices come with Apple Neural Engine (ANE) as part of the chip. The A15 Bionic chip on iPhone 14 comes with 6-core CPU, 5-core GPU, and a 16-core Neural Engine (ANE). On this device, your deep learning model can work on CPU only, CPU and GPU, or all computing engines (CPU, GPU, and ANE). ANE is the fastest way to run your model, especially for heavy applications. On iOS, CoreML is the framework that is optimized for deep learning inference. Also, CoreML is built into the operating system which means there is no need for you to compile, link, or ship your ML binary libraries with the app. Now that your target framework is defined, you need to find a way to convert your trained model into Core ML. This task can be easily achieved via ONNX - convert your Keras model to ONNX using the tf2onnx library and convert ONNX to Core ML model using the onnx-coreml library in python. The Core ML model is now ready to be deployed into your app.","title":"ONNX"},{"location":"page12/#build-onnx-from-source","text":"Before building from source uninstall any existing versions of onnx pip uninstall onnx. c++17 or higher C++ compiler version is required to build ONNX from source on Windows. For other platforms, please use C++11 or higher versions. Windows If you are building ONNX from source, it is recommended that you also build Protobuf locally as a static library. The version distributed with conda-forge is a DLL, but ONNX expects it to be a static library. Building protobuf locally also lets you control the version of protobuf. The tested and recommended version is 3.20.2. The instructions in this README assume you are using Visual Studio. It is recommended that you run all the commands from a shell started from \"x64 Native Tools Command Prompt for VS 2019\" and keep the build system generator for cmake (e.g., cmake -G \"Visual Studio 16 2019\") consistent while building protobuf as well as ONNX.","title":"BUILD ONNX FROM SOURCE"},{"location":"page12/#conversion-steps","text":"-> All frameworks have their own way to convert their models to ONNX. But there are some common steps that you need to follow.only saving the model weighs we ll not be able to convert to ONNX,the model architecture is required and really important to convert your model to onnx. -> Model weights are the weights of the different layers which are used to compute the output of the model. So, they are equally important to successfully convert the model.","title":"CONVERSION STEPS"},{"location":"page12/#input-names-and-output-names","text":"-> we will need to define the input names and the output names of our model. These metadata are used to describe the inputs and outputs of your model. ONNX will trace the different layers of your model in order to create a graph of theses layers. -> While tracing the layers, ONNX will also need an input sample to understand how the model is working and what operators are used to compute the outputs. -> The selected sample will be the input of the first layer of the model and is also used to define the input shape of the model.","title":"input names and output names"},{"location":"page12/#dynamic-axes","text":"->Then, ONNX requires to know the dynamic axes of your model. Most of the time during the conversion, you will use a batch size of 1. ->But if you want to have a model that can take a batch of N samples, you will need to define the dynamic axes of your model to accept a batch of N samples.","title":"dynamic axes"},{"location":"page12/#conversion-evaluation","text":"Finally, we need to evaluate the converted model to ensure that it is a sustainable ONNX model and it is working as expected. There are two separate steps to evaluate the converted model. The first step is to use the ONNX\u2019s API to check the model\u2019s validity. This is done by calling the onnx.checker.check_model function. This will verify the model\u2019s structure and confirm if the model has a valid ONNX scheme or not. Each node in the model is evaluated by checking the inputs and outputs of the node. The second step is to compare the output of the converted model with the output of the original model. This is done by comparing both outputs with the numpy.testing.assert_allclose function. This function will compare the two outputs and will raise an error if the two outputs are not equal, based on the rtol and atol parameters. conversion of a pytorch model to onnx import torch import torchvision from PIL import Image import numpy as np !pip3 install onnx ~ here we will load the pretrained ResNet18: resnet = torchvision.models.resnet18(pretrained=True) ~ We will download an example image from PyTorhc import urllib url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\") # Notebook Link will be in description urllib.request.urlretrieve(url, filename) from torchvision import transforms inp_image = Image.open('/content/dog.jpg') preprocess = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) input_tensor = preprocess(inp_image) inp_batch = input_tensor.unsqueeze(0) device =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") inp_batch.to(device) resnet.to(device) with torch.no_grad(): output = resnet(inp_batch) print(output[0]) ~ Output of shape 1000, confidence scores for each of the imagenet classes import torch.onnx torch.onnx.export(resnet, inp_batch, \"resnet18.onnx\", export_params=True, opset_version=10)","title":"conversion evaluation"},{"location":"page14/","text":"Optimization Techniques for PyTorch Framework 1. Pytorch GPU acceleration: Pytorch GPU acceleration is a method of using the GPU to speed up the training and inference of deep learning models. This is done by taking advantage of the parallel processing capabilities of the GPU to run multiple operations simultaneously. This allows for faster training and inference of deep learning models, as the GPU can process multiple operations at the same time. Pytorch also provides a number of other features such as distributed training, which allows for training on multiple GPUs, and automatic mixed precision, which allows for faster training and inference of deep learning models. How to Use Pytorch GPU Acceleration: Using Pytorch GPU acceleration is relatively straightforward. First, you need to install the Pytorch library on your machine. Once installed, you can then use the GPU to speed up the training and inference of deep learning models. To do this, you need to specify the GPU as the device when creating the model. You can also use the distributed training feature to train on multiple GPUs. Additionally, you can use the automatic mixed precision feature to further improve the performance of deep learning models.PyTorch provides a .to() method that allows you to move your model and data to the GPU. You can specify the device (e.g., 'cuda') as an argument to the .to() method to move the model and data to the GPU. For example: import torch #Create a model model = MyModel () #Move model to GPU model . to ( 'cuda' ) #Create input data input_data = torch . randn ( 64 , 3 , 32 , 32 ) #Move input data to GPU input_data = input_data . to ( 'cuda' ) --->PyTorch integrates with CUDA, which is a parallel computing platform and programming model for NVIDIA GPUs. Many popular deep learning libraries, such as cuDNN, cuBLAS, and cuFFT, are CUDA-enabled and can be used with PyTorch to accelerate computation on the GPU. You can install these libraries separately and PyTorch will automatically leverage them when available. import torch import torch.nn as nn import torch.optim as optim #Create a model model = MyModel () #Move model to GPU model . to ( 'cuda' ) #Define loss function criterion = nn . CrossEntropyLoss () . to ( 'cuda' ) #Define optimizer optimizer = optim . SGD ( model . parameters (), lr = 0.001 ) #Move optimizer to GPU optimizer = optimizer . to ( 'cuda' ) --->GPUs are optimized for batch processing, where multiple inputs can be processed in parallel. When processing large datasets, batching the data and processing it in batches can significantly accelerate the computation on the GPU. PyTorch provides the ability to batch the data using the 'torch.utils.data.DataLoader' class, which can be used in combination with the 'torch.nn.Module' and 'torch.optim' modules for efficient batch processing on the GPU. import torch from torch.utils.data import DataLoader #Create a DataLoader for batch processing dataloader = DataLoader ( dataset , batch_size = 64 , shuffle = True ) #Iterate over batches for batch in dataloader : #Move batch data to GPU input_data , labels = batch input_data = input_data . to ( 'cuda' ) labels = labels . to ( 'cuda' ) #Perform forward and backward pass on GPU outputs = model ( input_data ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () optimizer . zero_grad () --->GPU memory is a valuable resource, and it's important to manage it effectively. Deep learning models with large numbers of parameters or large input data can quickly consume a lot of GPU memory. You can monitor the GPU memory usage using the 'torch.cuda.max_memory_allocated()' and 'torch.cuda.memory_allocated()' functions, and optimize your code to minimize unnecessary memory consumption, such as avoiding redundant computations or unnecessary copies between CPU and GPU --->PyTorch supports training deep learning models on multiple GPUs in parallel, which can further accelerate the training process. You can use PyTorch's 'torch.nn.parallel.DistributedDataParallel' module to parallelize the training process across multiple GPUs. This module automatically handles data parallelism and gradient synchronization across GPUs, allowing you to scale up the training process to multiple GPUs. import torch import torch.nn as nn import torch.optim as optim import torch.nn.parallel #Create a model model = MyModel () #Parallelize model across multiple GPUs model = nn . parallel . DistributedDataParallel ( model ) #Move model to GPU model . to ( 'cuda' ) #Define loss function criterion = nn . CrossEntropyLoss () #Define optimizer optimizer = optim . SGD ( model . parameters (), lr = 0.001 ) #Move optimizer to GPU optimizer = optimizer . to ( 'cuda' ) --->Performing GPU-accelerated inference: import torch #Load pre-trained model model = torch . load ( 'model.pth' ) #Move model to GPU model . to ( 'cuda' ) #Create input data input_data = torch . randn ( 64 , 3 , 32 , 32 ) . to ( 'cuda' ) #Perform inference on GPU outputs = model ( input_data ) GPU acceleration is a powerful technique that can significantly speed up the training and inference process of deep learning models in PyTorch. By leveraging the capabilities of GPUs effectively, you can achieve faster and more efficient deep learning workflows. 2.Batch Normalization Batch normalization is an optimization technique used in deep neural networks to improve training stability and convergence. It works by normalizing the inputs to each layer in the network during training, which helps to mitigate the \"internal covariate shift\" problem. In PyTorch, batch normalization can be implemented as an optimization technique during the training of deep neural networks using the 'nn.BatchNorm' classes provided by the 'torch.nn' module. Batch normalization helps in normalizing the inputs to a neural network layer by scaling and shifting them during training, which can improve the overall performance and convergence of the model. Here's an overview of how you can use batch normalization in PyTorch for optimization: 1.Define your neural network architecture: Define your neural network architecture using PyTorch's 'nn.Module' class. This typically involves defining the layers of your network using classes like 'nn.Linear', 'nn.Conv2d', etc., and specifying their activation functions, dropout, and other parameters. 2.Add batch normalization layers: Add batch normalization layers after the linear or convolutional layers in your neural network. You can use the 'nn.BatchNorm' classes, such as 'nn.BatchNorm1d' for fully connected layers or 'nn.BatchNorm2d' for convolutional layers, depending on the dimensionality of your inputs. For example, you can add batch normalization to a fully connected layer like this: import torch import torch.nn as nn class MyNet ( nn . Module ): def __init__ ( self ): super ( MyNet , self ) . __init__ () self . fc1 = nn . Linear ( 100 , 256 ) self . bn1 = nn . BatchNorm1d ( 256 ) #Add batch normalization after fc1 def forward ( self , x ): x = self . fc1 ( x ) x = self . bn1 ( x ) x = F . relu ( x ) return x 3.Initialize batch normalization layers: After adding batch normalization layers to your network, you need to initialize their parameters. You can do this using the 'initialize()' method provided by PyTorch, or you can use custom initialization methods. For example, you can initialize the batch normalization layers in the above example like this: def init_weights ( m ): if isinstance ( m , nn . BatchNorm1d ): nn . init . constant_ ( m . weight , 1 ) nn . init . constant_ ( m . bias , 0 ) net = MyNet () net . apply ( init_weights ) # Initialize batch normalization layers 4.Enable training mode: Before training your neural network, you need to set the batch normalization layers to training mode using the 'train()' method. This ensures that the running statistics (e.g., running mean and running variance) used for normalization are updated during training. net . train () #Set the network to training mode 5.Update during optimization: During the optimization step, you can use any optimizer provided by PyTorch, such as 'torch.optim.SGD', 'torch.optim.Adam', etc., to update the parameters of your neural network, including the batch normalization layers. For example, if using SGD as the optimizer, you can update the network parameters like this: optimizer = torch . optim . SGD ( net . parameters (), lr = 0.01 ) # Inside the training loop optimizer . zero_grad () outputs = net ( inputs ) loss = criterion ( outputs , targets ) loss . backward () optimizer . step () # Update the network parameters, including batch normalization layers Batch normalization in PyTorch can be used as an optimization technique during training by adding batch normalization layers to your neural network, initializing their parameters, setting them to training mode during training, updating them during optimization, and setting them to inference mode during inference or evaluation. 3.Weight initialization Weight initialization is an important step in optimizing neural networks in PyTorch, as it can have a significant impact on the convergence and performance of the model during training. Properly initializing the weights can help prevent issues such as vanishing or exploding gradients, which can hinder the training process and result in poor model performance. In PyTorch, weight initialization is typically done when defining the architecture of the neural network or after the network is created. You can initialize the weights of the network using the appropriate initialization method for your specific use case. Here's an example of how weight initialization can be used in PyTorch for optimization: import torch import torch.nn as nn import torch.nn.init as init # Define a custom neural network class class MyNet ( nn . Module ): def __init__ ( self ): super ( MyNet , self ) . __init__ () self . fc1 = nn . Linear ( 784 , 256 ) # Fully connected layer with input size 784 and output size 256 self . fc2 = nn . Linear ( 256 , 128 ) # Fully connected layer with input size 256 and output size 128 self . fc3 = nn . Linear ( 128 , 10 ) # Fully connected layer with input size 128 and output size 10 # Initialize weights using Xavier initialization init . xavier_uniform_ ( self . fc1 . weight ) init . xavier_uniform_ ( self . fc2 . weight ) init . xavier_uniform_ ( self . fc3 . weight ) def forward ( self , x ): # Define the forward pass of the network x = torch . relu ( self . fc1 ( x )) x = torch . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x # Create an instance of the custom neural network net = MyNet () # Use the initialized weights during training # ... In this example, the init.xavier_uniform_() function is used to initialize the weights of the fully connected layers in the neural network using Xavier initialization. The net object is an instance of the MyNet class, and the initialized weights are used during the training process to optimize the network's performance. Note that the choice of weight initialization method may depend on the specific architecture of the neural network, the activation functions used, and the type of problem being solved. Experimenting with different weight initialization methods can be an important part of optimizing the training process and improving the performance of neural networks in PyTorch. 4.Gradient Clippping gradient clipping is a commonly used technique for optimization in PyTorch, as well as in other deep learning frameworks. Gradient clipping helps prevent the issue of exploding gradients, which can occur during the training process when gradients become very large and cause the model's parameters to be updated excessively, leading to instability and poor convergence. By limiting the magnitude of gradients, gradient clipping can help stabilize the training process and prevent the model from diverging or getting stuck in local optima. PyTorch provides built-in functions, such as 'torch.nn.utils.clip_grad_norm_()' and 'torch.nn.utils.clip_grad_value_()' , that allow you to clip gradients during the training loop. Here's an example of how you can use gradient clipping in PyTorch: import torch import torch.nn as nn import torch.optim as optim # Define your neural network model class MyNet ( nn . Module ): def __init__ ( self ): super ( MyNet , self ) . __init__ () self . fc1 = nn . Linear ( 784 , 256 ) self . fc2 = nn . Linear ( 256 , 128 ) self . fc3 = nn . Linear ( 128 , 10 ) # ... #Instantiate your model and optimizer model = MyNet () optimizer = optim . SGD ( model . parameters (), lr = 0.01 ) #Training loop for epoch in range ( num_epochs ): for batch_idx , ( data , target ) in enumerate ( train_loader ): #Zero the gradients optimizer . zero_grad () #Forward pass output = model ( data ) loss = loss_function ( output , target ) #Backward pass loss . backward () #Clip gradients max_grad_norm = 1.0 nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = max_grad_norm ) #Update weights optimizer . step () In this example, 'nn.utils.clip_grad_norm_()' is used to clip the gradients of the model's parameters based on their norm. The max_norm parameter specifies the maximum norm value allowed for gradients. If the norm of the gradients exceeds this threshold, the gradients are scaled down so that their norm becomes equal to the threshold, effectively limiting the magnitude of the gradients. It's important to note that the specific value of the gradient clipping threshold ('max_grad_norm' in the example) should be chosen carefully through experimentation, as it may affect the convergence and performance of the model. Too small a value may result in gradients being overly clipped and slow down training, while too large a value may not effectively mitigate exploding gradients. It's recommended to tune this hyperparameter to find the optimal value for your specific model and problem. There are several benefits to using gradient clipping in machine learning: --->Improved stability: Gradient clipping can help stabilize the training process by preventing large gradient updates that can cause the model weights to diverge or oscillate, leading to more stable and reliable convergence during training. --->Better generalization: By controlling the magnitude of gradients, gradient clipping can help prevent overfitting, as large gradients can lead to over-optimization on the training data. By limiting the gradients, gradient clipping can encourage the model to learn more general features that are applicable to unseen data. --->Faster convergence: In some cases, gradient clipping can help accelerate the convergence of the optimization process by preventing large gradients that can cause the optimization algorithm to take large steps and overshoot the optimal solution. This can result in faster training times and improved efficiency. 5.Learning Rate Scheduler One solution to help the algorithm converge quickly to an optimum is to use a learning rate scheduler . A learning rate scheduler adjusts the learning rate according to a pre-defined schedule during the training process. Usually, the learning rate is set to a higher value at the beginning of the training to allow faster convergence. As the training progresses, the learning rate is reduced to enable convergence to the optimum and thus leading to better performance. Reducing the learning rate over the training process is also known as annealing or decay. The amount of different learning rate schedulers can be overwhelming. An overview of how different pre-defined learning rate schedulers in PyTorch adjust the learning rate during training: ~StepLR The StepLR reduces the learning rate by a multiplicative factor after every predefined number of training steps. from torch.optim.lr_scheduler import StepLR scheduler = StepLR ( optimizer , step_size = 4 , #Period of learning rate decay gamma = 0.5 ) #Multiplicative factor of learning rate decay ~MultiStepLR The MultiStepLR \u2014 similarly to the StepLR \u2014 also reduces the learning rate by a multiplicative factor but after each pre-defined milestone. from torch.optim.lr_scheduler import MultiStepLR scheduler = MultiStepLR ( optimizer , milestones = [ 8 , 24 , 28 ], # List of epoch indices gamma = 0.5 ) # Multiplicative factor of learning rate decay ~ConstantLR The ConstantLR reduces learning rate by a multiplicative factor until the number of training steps reaches a pre-defined milestone. from torch.optim.lr_scheduler import ConstantLR scheduler = ConstantLR ( optimizer , factor = 0.5 , # The number we multiply learning rate until the milestone. total_iters = 8 ) # The number of steps that the scheduler decays the learning rate As you might have already noticed, if your starting factor is smaller than 1, this learning rate scheduler increases the learning rate over the course of the training process instead of decreasing it. ~LinearLR The LinearLR \u2014 similarly to the ConstantLR\u2014 also reduces the learning rate by a multiplicative factor at the beginning of the training. But it linearly increases the learning rate over a defined number of training steps until it reaches its originally set learning rate. from torch.optim.lr_scheduler import LinearLR scheduler = LinearLR ( optimizer , start_factor = 0.5 , # The number we multiply learning rate in the first epoch total_iters = 8 ) # The number of iterations that multiplicative factor reaches to 1 If your starting factor is smaller than 1, this learning rate scheduler also increases the learning rate over the course of the training process instead of decreasing it. ~ExponentialLR The ExponentialLR reduces learning rate by a multiplicative factor at every training step. from torch.optim.lr_scheduler import ExponentialLR scheduler = ExponentialLR ( optimizer , gamma = 0.5 ) # Multiplicative factor of learning rate decay. ~PolynomialLR The PolynomialLR reduces learning rate by using a polynomial function for a defined number of steps. from torch.optim.lr_scheduler import PolynomialLR scheduler = PolynomialLR ( optimizer , total_iters = 8 , # The number of steps that the scheduler decays the learning rate. power = 1 ) # The power of the polynomial. ~CosineAnnealingLR The CosineAnnealingLR reduces learning rate by a cosine function. While you could technically schedule the learning rate adjustments to follow multiple periods, the idea is to decay the learning rate over half a period for the maximum number of iterations. from torch.optim.lr_scheduler import CosineAnnealingLR scheduler = CosineAnnealingLR ( optimizer , T_max = 32 , # Maximum number of iterations. eta_min = 1e-4 ) # Minimum learning rate. ~CosineAnnealingWarmRestartsLR The CosineAnnealingWarmRestarts is similar to the cosine annealing schedule. However, it allows you to restart the LR schedule with the initial LR at, e.g., each epoch. from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts scheduler = CosineAnnealingWarmRestarts ( optimizer , T_0 = 8 , # Number of iterations for the first restart T_mult = 1 , # A factor increases TiTi\u200b after a restart eta_min = 1e-4 ) # Minimum learning rate This is called a warm restart and was introduced in 2017 [1]. Increasing the LR causes the model to diverge. However, this intentional divergence enables the model to escape local minima and find an even better global minimum. ~CyclicLR The CyclicLR adjusted the learning rate according to a cyclical learning rate policy, which is based on the concept of warm restarts which we just discussed in the previous section. In PyTorch there are three built-in policies. from torch.optim.lr_scheduler import CyclicLR scheduler = CyclicLR ( optimizer , base_lr = 0.0001 , # Initial learning rate which is the lower boundary in the cycle for each parameter group max_lr = 1e-3 , # Upper learning rate boundaries in the cycle for each parameter group step_size_up = 4 , # Number of training iterations in the increasing half of a cycle mode = \"triangular\" ) ~OneCycleLR The OneCycleLR reduces learning rate according to the 1cycle learning rate policy. In contrast to many other learning rate schedulers, the learning rate is not only decreased over the training process. Instead, the learning rate increases from an initial learning rate to some maximum learning rate and then decreases again. from torch.optim.lr_scheduler import OneCycleLR scheduler = OneCycleLR ( optimizer , max_lr = 1e-3 , # Upper learning rate boundaries in the cycle for each parameter group steps_per_epoch = 8 , # The number of steps per epoch to train for. epochs = 4 , # The number of epochs to train for. anneal_strategy = 'cos' ) # Specifies the annealing strategy ~ReduceLROnPlateauLR The ReduceLROnPlateau reduces the learning rate by when the metric has stopped improving. As you can guess, this is difficult to visualize because the learning rate reduction timing depends on your model, data, and hyperparameters. ~Custom Learning Rate Schedulers with Lambda Functions If the built-in learning rate schedulers don\u2019t fit your needs, you have the possibility to define a scheduler with lambda functions. The lambda function is a function that returns a multiplicative factor based on the epoch value. The LambdaLR adjusts the learning rate by applying the multiplicative factor from the lambda function to the initial LR. lr_epoch [ t ] = lr_initial * lambda ( epoch )","title":"Optimization Techniques for PyTorch Framework"},{"location":"page14/#optimization-techniques-for-pytorch-framework","text":"","title":"Optimization Techniques for PyTorch Framework"},{"location":"page14/#1-pytorch-gpu-acceleration","text":"Pytorch GPU acceleration is a method of using the GPU to speed up the training and inference of deep learning models. This is done by taking advantage of the parallel processing capabilities of the GPU to run multiple operations simultaneously. This allows for faster training and inference of deep learning models, as the GPU can process multiple operations at the same time. Pytorch also provides a number of other features such as distributed training, which allows for training on multiple GPUs, and automatic mixed precision, which allows for faster training and inference of deep learning models.","title":"1. Pytorch GPU acceleration:"},{"location":"page14/#how-to-use-pytorch-gpu-acceleration","text":"Using Pytorch GPU acceleration is relatively straightforward. First, you need to install the Pytorch library on your machine. Once installed, you can then use the GPU to speed up the training and inference of deep learning models. To do this, you need to specify the GPU as the device when creating the model. You can also use the distributed training feature to train on multiple GPUs. Additionally, you can use the automatic mixed precision feature to further improve the performance of deep learning models.PyTorch provides a .to() method that allows you to move your model and data to the GPU. You can specify the device (e.g., 'cuda') as an argument to the .to() method to move the model and data to the GPU. For example: import torch #Create a model model = MyModel () #Move model to GPU model . to ( 'cuda' ) #Create input data input_data = torch . randn ( 64 , 3 , 32 , 32 ) #Move input data to GPU input_data = input_data . to ( 'cuda' ) --->PyTorch integrates with CUDA, which is a parallel computing platform and programming model for NVIDIA GPUs. Many popular deep learning libraries, such as cuDNN, cuBLAS, and cuFFT, are CUDA-enabled and can be used with PyTorch to accelerate computation on the GPU. You can install these libraries separately and PyTorch will automatically leverage them when available. import torch import torch.nn as nn import torch.optim as optim #Create a model model = MyModel () #Move model to GPU model . to ( 'cuda' ) #Define loss function criterion = nn . CrossEntropyLoss () . to ( 'cuda' ) #Define optimizer optimizer = optim . SGD ( model . parameters (), lr = 0.001 ) #Move optimizer to GPU optimizer = optimizer . to ( 'cuda' ) --->GPUs are optimized for batch processing, where multiple inputs can be processed in parallel. When processing large datasets, batching the data and processing it in batches can significantly accelerate the computation on the GPU. PyTorch provides the ability to batch the data using the 'torch.utils.data.DataLoader' class, which can be used in combination with the 'torch.nn.Module' and 'torch.optim' modules for efficient batch processing on the GPU. import torch from torch.utils.data import DataLoader #Create a DataLoader for batch processing dataloader = DataLoader ( dataset , batch_size = 64 , shuffle = True ) #Iterate over batches for batch in dataloader : #Move batch data to GPU input_data , labels = batch input_data = input_data . to ( 'cuda' ) labels = labels . to ( 'cuda' ) #Perform forward and backward pass on GPU outputs = model ( input_data ) loss = criterion ( outputs , labels ) loss . backward () optimizer . step () optimizer . zero_grad () --->GPU memory is a valuable resource, and it's important to manage it effectively. Deep learning models with large numbers of parameters or large input data can quickly consume a lot of GPU memory. You can monitor the GPU memory usage using the 'torch.cuda.max_memory_allocated()' and 'torch.cuda.memory_allocated()' functions, and optimize your code to minimize unnecessary memory consumption, such as avoiding redundant computations or unnecessary copies between CPU and GPU --->PyTorch supports training deep learning models on multiple GPUs in parallel, which can further accelerate the training process. You can use PyTorch's 'torch.nn.parallel.DistributedDataParallel' module to parallelize the training process across multiple GPUs. This module automatically handles data parallelism and gradient synchronization across GPUs, allowing you to scale up the training process to multiple GPUs. import torch import torch.nn as nn import torch.optim as optim import torch.nn.parallel #Create a model model = MyModel () #Parallelize model across multiple GPUs model = nn . parallel . DistributedDataParallel ( model ) #Move model to GPU model . to ( 'cuda' ) #Define loss function criterion = nn . CrossEntropyLoss () #Define optimizer optimizer = optim . SGD ( model . parameters (), lr = 0.001 ) #Move optimizer to GPU optimizer = optimizer . to ( 'cuda' ) --->Performing GPU-accelerated inference: import torch #Load pre-trained model model = torch . load ( 'model.pth' ) #Move model to GPU model . to ( 'cuda' ) #Create input data input_data = torch . randn ( 64 , 3 , 32 , 32 ) . to ( 'cuda' ) #Perform inference on GPU outputs = model ( input_data ) GPU acceleration is a powerful technique that can significantly speed up the training and inference process of deep learning models in PyTorch. By leveraging the capabilities of GPUs effectively, you can achieve faster and more efficient deep learning workflows.","title":"How to Use Pytorch GPU Acceleration:"},{"location":"page14/#2batch-normalization","text":"Batch normalization is an optimization technique used in deep neural networks to improve training stability and convergence. It works by normalizing the inputs to each layer in the network during training, which helps to mitigate the \"internal covariate shift\" problem. In PyTorch, batch normalization can be implemented as an optimization technique during the training of deep neural networks using the 'nn.BatchNorm' classes provided by the 'torch.nn' module. Batch normalization helps in normalizing the inputs to a neural network layer by scaling and shifting them during training, which can improve the overall performance and convergence of the model. Here's an overview of how you can use batch normalization in PyTorch for optimization: 1.Define your neural network architecture: Define your neural network architecture using PyTorch's 'nn.Module' class. This typically involves defining the layers of your network using classes like 'nn.Linear', 'nn.Conv2d', etc., and specifying their activation functions, dropout, and other parameters. 2.Add batch normalization layers: Add batch normalization layers after the linear or convolutional layers in your neural network. You can use the 'nn.BatchNorm' classes, such as 'nn.BatchNorm1d' for fully connected layers or 'nn.BatchNorm2d' for convolutional layers, depending on the dimensionality of your inputs. For example, you can add batch normalization to a fully connected layer like this: import torch import torch.nn as nn class MyNet ( nn . Module ): def __init__ ( self ): super ( MyNet , self ) . __init__ () self . fc1 = nn . Linear ( 100 , 256 ) self . bn1 = nn . BatchNorm1d ( 256 ) #Add batch normalization after fc1 def forward ( self , x ): x = self . fc1 ( x ) x = self . bn1 ( x ) x = F . relu ( x ) return x 3.Initialize batch normalization layers: After adding batch normalization layers to your network, you need to initialize their parameters. You can do this using the 'initialize()' method provided by PyTorch, or you can use custom initialization methods. For example, you can initialize the batch normalization layers in the above example like this: def init_weights ( m ): if isinstance ( m , nn . BatchNorm1d ): nn . init . constant_ ( m . weight , 1 ) nn . init . constant_ ( m . bias , 0 ) net = MyNet () net . apply ( init_weights ) # Initialize batch normalization layers 4.Enable training mode: Before training your neural network, you need to set the batch normalization layers to training mode using the 'train()' method. This ensures that the running statistics (e.g., running mean and running variance) used for normalization are updated during training. net . train () #Set the network to training mode 5.Update during optimization: During the optimization step, you can use any optimizer provided by PyTorch, such as 'torch.optim.SGD', 'torch.optim.Adam', etc., to update the parameters of your neural network, including the batch normalization layers. For example, if using SGD as the optimizer, you can update the network parameters like this: optimizer = torch . optim . SGD ( net . parameters (), lr = 0.01 ) # Inside the training loop optimizer . zero_grad () outputs = net ( inputs ) loss = criterion ( outputs , targets ) loss . backward () optimizer . step () # Update the network parameters, including batch normalization layers Batch normalization in PyTorch can be used as an optimization technique during training by adding batch normalization layers to your neural network, initializing their parameters, setting them to training mode during training, updating them during optimization, and setting them to inference mode during inference or evaluation.","title":"2.Batch Normalization"},{"location":"page14/#3weight-initialization","text":"Weight initialization is an important step in optimizing neural networks in PyTorch, as it can have a significant impact on the convergence and performance of the model during training. Properly initializing the weights can help prevent issues such as vanishing or exploding gradients, which can hinder the training process and result in poor model performance. In PyTorch, weight initialization is typically done when defining the architecture of the neural network or after the network is created. You can initialize the weights of the network using the appropriate initialization method for your specific use case. Here's an example of how weight initialization can be used in PyTorch for optimization: import torch import torch.nn as nn import torch.nn.init as init # Define a custom neural network class class MyNet ( nn . Module ): def __init__ ( self ): super ( MyNet , self ) . __init__ () self . fc1 = nn . Linear ( 784 , 256 ) # Fully connected layer with input size 784 and output size 256 self . fc2 = nn . Linear ( 256 , 128 ) # Fully connected layer with input size 256 and output size 128 self . fc3 = nn . Linear ( 128 , 10 ) # Fully connected layer with input size 128 and output size 10 # Initialize weights using Xavier initialization init . xavier_uniform_ ( self . fc1 . weight ) init . xavier_uniform_ ( self . fc2 . weight ) init . xavier_uniform_ ( self . fc3 . weight ) def forward ( self , x ): # Define the forward pass of the network x = torch . relu ( self . fc1 ( x )) x = torch . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x # Create an instance of the custom neural network net = MyNet () # Use the initialized weights during training # ... In this example, the init.xavier_uniform_() function is used to initialize the weights of the fully connected layers in the neural network using Xavier initialization. The net object is an instance of the MyNet class, and the initialized weights are used during the training process to optimize the network's performance. Note that the choice of weight initialization method may depend on the specific architecture of the neural network, the activation functions used, and the type of problem being solved. Experimenting with different weight initialization methods can be an important part of optimizing the training process and improving the performance of neural networks in PyTorch.","title":"3.Weight initialization"},{"location":"page14/#4gradient-clippping","text":"gradient clipping is a commonly used technique for optimization in PyTorch, as well as in other deep learning frameworks. Gradient clipping helps prevent the issue of exploding gradients, which can occur during the training process when gradients become very large and cause the model's parameters to be updated excessively, leading to instability and poor convergence. By limiting the magnitude of gradients, gradient clipping can help stabilize the training process and prevent the model from diverging or getting stuck in local optima. PyTorch provides built-in functions, such as 'torch.nn.utils.clip_grad_norm_()' and 'torch.nn.utils.clip_grad_value_()' , that allow you to clip gradients during the training loop. Here's an example of how you can use gradient clipping in PyTorch: import torch import torch.nn as nn import torch.optim as optim # Define your neural network model class MyNet ( nn . Module ): def __init__ ( self ): super ( MyNet , self ) . __init__ () self . fc1 = nn . Linear ( 784 , 256 ) self . fc2 = nn . Linear ( 256 , 128 ) self . fc3 = nn . Linear ( 128 , 10 ) # ... #Instantiate your model and optimizer model = MyNet () optimizer = optim . SGD ( model . parameters (), lr = 0.01 ) #Training loop for epoch in range ( num_epochs ): for batch_idx , ( data , target ) in enumerate ( train_loader ): #Zero the gradients optimizer . zero_grad () #Forward pass output = model ( data ) loss = loss_function ( output , target ) #Backward pass loss . backward () #Clip gradients max_grad_norm = 1.0 nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = max_grad_norm ) #Update weights optimizer . step () In this example, 'nn.utils.clip_grad_norm_()' is used to clip the gradients of the model's parameters based on their norm. The max_norm parameter specifies the maximum norm value allowed for gradients. If the norm of the gradients exceeds this threshold, the gradients are scaled down so that their norm becomes equal to the threshold, effectively limiting the magnitude of the gradients. It's important to note that the specific value of the gradient clipping threshold ('max_grad_norm' in the example) should be chosen carefully through experimentation, as it may affect the convergence and performance of the model. Too small a value may result in gradients being overly clipped and slow down training, while too large a value may not effectively mitigate exploding gradients. It's recommended to tune this hyperparameter to find the optimal value for your specific model and problem. There are several benefits to using gradient clipping in machine learning: --->Improved stability: Gradient clipping can help stabilize the training process by preventing large gradient updates that can cause the model weights to diverge or oscillate, leading to more stable and reliable convergence during training. --->Better generalization: By controlling the magnitude of gradients, gradient clipping can help prevent overfitting, as large gradients can lead to over-optimization on the training data. By limiting the gradients, gradient clipping can encourage the model to learn more general features that are applicable to unseen data. --->Faster convergence: In some cases, gradient clipping can help accelerate the convergence of the optimization process by preventing large gradients that can cause the optimization algorithm to take large steps and overshoot the optimal solution. This can result in faster training times and improved efficiency.","title":"4.Gradient Clippping"},{"location":"page14/#5learning-rate-scheduler","text":"One solution to help the algorithm converge quickly to an optimum is to use a learning rate scheduler . A learning rate scheduler adjusts the learning rate according to a pre-defined schedule during the training process. Usually, the learning rate is set to a higher value at the beginning of the training to allow faster convergence. As the training progresses, the learning rate is reduced to enable convergence to the optimum and thus leading to better performance. Reducing the learning rate over the training process is also known as annealing or decay. The amount of different learning rate schedulers can be overwhelming. An overview of how different pre-defined learning rate schedulers in PyTorch adjust the learning rate during training: ~StepLR The StepLR reduces the learning rate by a multiplicative factor after every predefined number of training steps. from torch.optim.lr_scheduler import StepLR scheduler = StepLR ( optimizer , step_size = 4 , #Period of learning rate decay gamma = 0.5 ) #Multiplicative factor of learning rate decay ~MultiStepLR The MultiStepLR \u2014 similarly to the StepLR \u2014 also reduces the learning rate by a multiplicative factor but after each pre-defined milestone. from torch.optim.lr_scheduler import MultiStepLR scheduler = MultiStepLR ( optimizer , milestones = [ 8 , 24 , 28 ], # List of epoch indices gamma = 0.5 ) # Multiplicative factor of learning rate decay ~ConstantLR The ConstantLR reduces learning rate by a multiplicative factor until the number of training steps reaches a pre-defined milestone. from torch.optim.lr_scheduler import ConstantLR scheduler = ConstantLR ( optimizer , factor = 0.5 , # The number we multiply learning rate until the milestone. total_iters = 8 ) # The number of steps that the scheduler decays the learning rate As you might have already noticed, if your starting factor is smaller than 1, this learning rate scheduler increases the learning rate over the course of the training process instead of decreasing it. ~LinearLR The LinearLR \u2014 similarly to the ConstantLR\u2014 also reduces the learning rate by a multiplicative factor at the beginning of the training. But it linearly increases the learning rate over a defined number of training steps until it reaches its originally set learning rate. from torch.optim.lr_scheduler import LinearLR scheduler = LinearLR ( optimizer , start_factor = 0.5 , # The number we multiply learning rate in the first epoch total_iters = 8 ) # The number of iterations that multiplicative factor reaches to 1 If your starting factor is smaller than 1, this learning rate scheduler also increases the learning rate over the course of the training process instead of decreasing it. ~ExponentialLR The ExponentialLR reduces learning rate by a multiplicative factor at every training step. from torch.optim.lr_scheduler import ExponentialLR scheduler = ExponentialLR ( optimizer , gamma = 0.5 ) # Multiplicative factor of learning rate decay. ~PolynomialLR The PolynomialLR reduces learning rate by using a polynomial function for a defined number of steps. from torch.optim.lr_scheduler import PolynomialLR scheduler = PolynomialLR ( optimizer , total_iters = 8 , # The number of steps that the scheduler decays the learning rate. power = 1 ) # The power of the polynomial. ~CosineAnnealingLR The CosineAnnealingLR reduces learning rate by a cosine function. While you could technically schedule the learning rate adjustments to follow multiple periods, the idea is to decay the learning rate over half a period for the maximum number of iterations. from torch.optim.lr_scheduler import CosineAnnealingLR scheduler = CosineAnnealingLR ( optimizer , T_max = 32 , # Maximum number of iterations. eta_min = 1e-4 ) # Minimum learning rate. ~CosineAnnealingWarmRestartsLR The CosineAnnealingWarmRestarts is similar to the cosine annealing schedule. However, it allows you to restart the LR schedule with the initial LR at, e.g., each epoch. from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts scheduler = CosineAnnealingWarmRestarts ( optimizer , T_0 = 8 , # Number of iterations for the first restart T_mult = 1 , # A factor increases TiTi\u200b after a restart eta_min = 1e-4 ) # Minimum learning rate This is called a warm restart and was introduced in 2017 [1]. Increasing the LR causes the model to diverge. However, this intentional divergence enables the model to escape local minima and find an even better global minimum. ~CyclicLR The CyclicLR adjusted the learning rate according to a cyclical learning rate policy, which is based on the concept of warm restarts which we just discussed in the previous section. In PyTorch there are three built-in policies. from torch.optim.lr_scheduler import CyclicLR scheduler = CyclicLR ( optimizer , base_lr = 0.0001 , # Initial learning rate which is the lower boundary in the cycle for each parameter group max_lr = 1e-3 , # Upper learning rate boundaries in the cycle for each parameter group step_size_up = 4 , # Number of training iterations in the increasing half of a cycle mode = \"triangular\" ) ~OneCycleLR The OneCycleLR reduces learning rate according to the 1cycle learning rate policy. In contrast to many other learning rate schedulers, the learning rate is not only decreased over the training process. Instead, the learning rate increases from an initial learning rate to some maximum learning rate and then decreases again. from torch.optim.lr_scheduler import OneCycleLR scheduler = OneCycleLR ( optimizer , max_lr = 1e-3 , # Upper learning rate boundaries in the cycle for each parameter group steps_per_epoch = 8 , # The number of steps per epoch to train for. epochs = 4 , # The number of epochs to train for. anneal_strategy = 'cos' ) # Specifies the annealing strategy ~ReduceLROnPlateauLR The ReduceLROnPlateau reduces the learning rate by when the metric has stopped improving. As you can guess, this is difficult to visualize because the learning rate reduction timing depends on your model, data, and hyperparameters. ~Custom Learning Rate Schedulers with Lambda Functions If the built-in learning rate schedulers don\u2019t fit your needs, you have the possibility to define a scheduler with lambda functions. The lambda function is a function that returns a multiplicative factor based on the epoch value. The LambdaLR adjusts the learning rate by applying the multiplicative factor from the lambda function to the initial LR. lr_epoch [ t ] = lr_initial * lambda ( epoch )","title":"5.Learning Rate Scheduler"},{"location":"page15/","text":"Model Compression with TensorFlow Lite: A Look into Reducing Model Size Why is Model Compression important? A significant problem in the arms race to produce more accurate models is complexity, which leads to the problem of size. These models are usually huge and resource-intensive, which leads to greater space and time consumption. (Takes up more space in memory and slower in prediction as compared to smaller models) The Problem of Model Size A large model size is a common by product when attempting to push the limits of model accuracy in predicting unseen data in deep learning applications. For example, with more nodes, we can detect subtler features in the dataset. However, for project requirements such as using AI in embedded systems that depend on fast predictions, we are limited by the available computational resources. Furthermore, prevailing edge devices do not have networking capabilities, as such, we are not able to utilize cloud computing. This results in the inability to use massive models which would take too long to get meaningful predictions. As such, we will need to optimize our performance to size, when designing our model. An intuitive understanding of the theory To overly simplify for the gist of understanding machine learning models, a neural network is a set of nodes with weights(W) that connect between nodes. You can think of this as a set of instructions that we optimize to increase our likelihood of generating our desired class. The more specific this set of instructions are, the greater our model size, which is dependent on the size of our parameters (our configuration variables such as weight). TensorFlow Lite to the rescue! TensorFlow Lite deals with the Quantisation and prunning and does a great job in abstracting the hard parts of model compression. TensorFlow Lite covers: * Post-Training Quantization\u2014 Reduce Float16\u2014 Hybrid Quantization\u2014 Integer Quantization * During-Training Quantization * Post-Training Pruning * Post-Training Clustering The most common and easiest to implement method would be post-training quantization. The usage of quantization is the limiting of the bits of precision of our model parameters as such this reduces the amount of data that is needed to be stored. Quantisation Artificial neural networks consist of activation nodes, the connections between the nodes, and a weight parameter associated with each connection. It is these weight parameters and activation node computations that can be quantized. For perspective, running a neural network on hardware can easily result in many millions of multiplication and addition operations. Lower-bit mathematical operations with quantized parameters combined with quantizing intermediate calculations of a neural network results in large computational gains and higher performance. Besides the performance benefit, quantized neural networks also increase power efficiency for two reasons: reduced memory access costs and increased compute efficiency. Using the lower-bit quantized data requires less data movement, both on-chip and off-chip, which reduces memory bandwidth and saves significant energy. Lower-precision mathematical operations, such as an 32-bit integer multiply versus a 64-bit floating point multiply, consume less energy and increase compute efficiency, thus reducing power consumption. In addition, reducing the number of bits for representing the neural network\u2019s parameters results in less memory storage. I will only go through post-training Hybrid/Dynamic range quantization because it is the easiest to implement, has a great amount of impact in size reduction with minimal loss. To reference our earlier neural network diagram, our model parameters(weights) which refer to the lines connecting each note can be seen to represent its literal weight(significance) or the importance of the node to predict our desired outcome. Originally, we gave 64-bits to each weight, known as the tf.float64(64-bit single-precision floating-point), to reduce the size of our model, we would essentially shave off from 64-bits to 32-bits (tf.float32) or 16-bits ( tf.float16) depending on the type of quantization used. We can intuitively see that this poses significant exponential size reductions as with a bigger and more complex the model, the greater the number of nodes and subsequently the greater number of weights which leads to a more significant size reduction especially for fully-connected neural networks, which has each layer of nodes connected to each of the nodes in the next layer. Perks of Model Compression Smaller model sizes \u2014 Models can actually be stored into embedded devices (ESP32 has ~4Mb of Flash Memory) Faster Prediction Rates \u2014 This speeds up actionability, which provides viability for real-time decisions. Lowered power consumption \u2014 An often overlooked feature as environments that train models often come with a constant supply of power, embedded devices usually run on a battery thus this is the most important feature. Hidden Perks of Model Compression You might be quick to think that reducing the amount of information we store for each weight, would always be detrimental to our model, however, quantization promotes generalization which was a huge plus in preventing overfitting \u2014 a common problem with complex models.By Jerome Friedman, the father of gradient boost, empirical evidence shows that lots of small steps in the right direction result in better predictions with test data. By quantization, it is possible to get an improved accuracy due to the decreased sensitivity of the weights. Imagine if, in our dataset, we get lucky, every time we try to detect a cookie our dataset shows us a chocolate chip cookie, our cookie detection would get a high training accuracy, however, if in real-life we only have raisin cookies, it would have a low test accuracy. Generalization is like blurring our chocolate chip so that our model realizes as long as there is this blob, it is a cookie.The same can be said for other compression methods such as pruning. It is in this vein whereby dropout also can improve unseen accuracy as randomly dropping nodes during training promotes generalization as well. Let\u2019s try it out! import os import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Flatten from tensorflow.keras.layers import Dense from tensorflow.keras.losses import SparseCategoricalCrossentropy from sklearn.metrics import accuracy_score print ( tf . __version__ ) def get_file_size(file_path): size = os.path.getsize(file_path) return size def convert_bytes(size, unit=None): if unit == \"KB\": return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes') elif unit == \"MB\": return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes') else: return print('File size: ' + str(size) + ' bytes') Import the fasion MNIST dataset fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] Explore the data train_images.shape len(train_labels) np.unique(train_labels) test_images.shape len(test_labels) Preprocessing plt.figure() plt.imshow(train_images[88]) plt.colorbar() plt.grid(False) plt.show() train_images = train_images / 255.0 test_images = test_images / 255.0 model = keras.Sequential([ Flatten(input_shape=(28, 28)), Dense(128, activation='relu'), Dense(10) ]) model.compile(optimizer='adam', loss= SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) model.fit(train_images, train_labels, epochs=15) KERAS_MODEL_NAME = \"tf_fashion.h5\" model.save(KERAS_MODEL_NAME) Inference Time of tensorflow model model = tf.keras.models.load_model('tf_fashion.h5') _ = model.predict(test_images[:1]) start_time = time.time() output = model.predict(test_images[:1]) end_time = time.time() inference_time = end_time - start_time print('Inference time:', inference_time) Size and Accuracy of the model convert_bytes(get_file_size(KERAS_MODEL_NAME), \"MB\") test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print('\\nTest accuracy:', test_acc) Tflite Model sTF_LITE_MODEL_FILE_NAME = \"tf_lite_model.tflite\" tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model) tflite_model = tf_lite_converter.convert() tflite_model_name = TF_LITE_MODEL_FILE_NAME open(tflite_model_name, \"wb\").write(tflite_model) Size of the Tflite model convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\") Check input tensor shape interpreter = tf.lite.Interpreter(model_path = TF_LITE_MODEL_FILE_NAME) input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() print(\"Input Shape:\", input_details[0]['shape']) print(\"Input Type:\", input_details[0]['dtype']) print(\"Output Shape:\", output_details[0]['shape']) print(\"Output Type:\", output_details[0]['dtype']) Resize tensor shape interpreter.resize_tensor_input(input_details[0]['index'], (10000, 28, 28)) interpreter.resize_tensor_input(output_details[0]['index'], (10000, 10)) interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() print(\"Input Shape:\", input_details[0]['shape']) print(\"Input Type:\", input_details[0]['dtype']) print(\"Output Shape:\", output_details[0]['shape']) print(\"Output Type:\", output_details[0]['dtype']) test_images.dtype test_imgs_numpy = np.array(test_images, dtype=np.float32) interpreter.set_tensor(input_details[0]['index'], test_imgs_numpy) interpreter.invoke() tflite_model_predictions = interpreter.get_tensor(output_details[0]['index']) print(\"Prediction results shape:\", tflite_model_predictions.shape) prediction_classes = np.argmax(tflite_model_predictions, axis=1) interpreter = tf.lite.Interpreter(model_path='tf_lite_model.tflite') interpreter.allocate_tensors() Inference time of the Tflite model start_time = time.time() interpreter.invoke() end_time = time.time() inference_time = end_time - start_time inference_times.append(inference_time) print('Average inference time:', sum(inference_times) / len(inference_times)) Accuracy of the Tflite model acc = accuracy_score(prediction_classes, test_labels) print('Test accuracy TFLITE model :', acc) Analysis of Results Pros and Cons of TensorFlowLite Pros: * Easier to implement model compression * Minimal effect on accuracy (Depending on model) * Major speed up in prediction Cons: * Requires the latest Tensorflow version 2 * Relatively new , Many operations (ops) are not supported yet such as SELU * Requires converting model which can fail * Possible complications when running inference compared to our good friend.predict() as it is more convoluted. Conclusion Despite its cons, TensorFlow Lite serves as a powerful tool with great potential that surpassed my expectations. I foresee in the near future, model compression being more widely used as the demand for AI in embedded devices inevitably grows, which gives TFLite a reason to provide greater operation coverage. With its shortcomings that can be mitigated by custom implementations, TensorFlow Lite for model compression is worth a shot.","title":"Model Compression with TensorFlow Lite: A Look into Reducing Model Size"},{"location":"page15/#model-compression-with-tensorflow-lite-a-look-into-reducing-model-size","text":"","title":"Model Compression with TensorFlow Lite: A Look into Reducing Model Size"},{"location":"page15/#why-is-model-compression-important","text":"A significant problem in the arms race to produce more accurate models is complexity, which leads to the problem of size. These models are usually huge and resource-intensive, which leads to greater space and time consumption. (Takes up more space in memory and slower in prediction as compared to smaller models)","title":"Why is Model Compression important?"},{"location":"page15/#the-problem-of-model-size","text":"A large model size is a common by product when attempting to push the limits of model accuracy in predicting unseen data in deep learning applications. For example, with more nodes, we can detect subtler features in the dataset. However, for project requirements such as using AI in embedded systems that depend on fast predictions, we are limited by the available computational resources. Furthermore, prevailing edge devices do not have networking capabilities, as such, we are not able to utilize cloud computing. This results in the inability to use massive models which would take too long to get meaningful predictions. As such, we will need to optimize our performance to size, when designing our model.","title":"The Problem of Model Size"},{"location":"page15/#an-intuitive-understanding-of-the-theory","text":"To overly simplify for the gist of understanding machine learning models, a neural network is a set of nodes with weights(W) that connect between nodes. You can think of this as a set of instructions that we optimize to increase our likelihood of generating our desired class. The more specific this set of instructions are, the greater our model size, which is dependent on the size of our parameters (our configuration variables such as weight).","title":"An intuitive understanding of the theory"},{"location":"page15/#tensorflow-lite-to-the-rescue","text":"TensorFlow Lite deals with the Quantisation and prunning and does a great job in abstracting the hard parts of model compression. TensorFlow Lite covers: * Post-Training Quantization\u2014 Reduce Float16\u2014 Hybrid Quantization\u2014 Integer Quantization * During-Training Quantization * Post-Training Pruning * Post-Training Clustering The most common and easiest to implement method would be post-training quantization. The usage of quantization is the limiting of the bits of precision of our model parameters as such this reduces the amount of data that is needed to be stored.","title":"TensorFlow Lite to the rescue!"},{"location":"page15/#quantisation","text":"Artificial neural networks consist of activation nodes, the connections between the nodes, and a weight parameter associated with each connection. It is these weight parameters and activation node computations that can be quantized. For perspective, running a neural network on hardware can easily result in many millions of multiplication and addition operations. Lower-bit mathematical operations with quantized parameters combined with quantizing intermediate calculations of a neural network results in large computational gains and higher performance. Besides the performance benefit, quantized neural networks also increase power efficiency for two reasons: reduced memory access costs and increased compute efficiency. Using the lower-bit quantized data requires less data movement, both on-chip and off-chip, which reduces memory bandwidth and saves significant energy. Lower-precision mathematical operations, such as an 32-bit integer multiply versus a 64-bit floating point multiply, consume less energy and increase compute efficiency, thus reducing power consumption. In addition, reducing the number of bits for representing the neural network\u2019s parameters results in less memory storage. I will only go through post-training Hybrid/Dynamic range quantization because it is the easiest to implement, has a great amount of impact in size reduction with minimal loss. To reference our earlier neural network diagram, our model parameters(weights) which refer to the lines connecting each note can be seen to represent its literal weight(significance) or the importance of the node to predict our desired outcome. Originally, we gave 64-bits to each weight, known as the tf.float64(64-bit single-precision floating-point), to reduce the size of our model, we would essentially shave off from 64-bits to 32-bits (tf.float32) or 16-bits ( tf.float16) depending on the type of quantization used. We can intuitively see that this poses significant exponential size reductions as with a bigger and more complex the model, the greater the number of nodes and subsequently the greater number of weights which leads to a more significant size reduction especially for fully-connected neural networks, which has each layer of nodes connected to each of the nodes in the next layer.","title":"Quantisation"},{"location":"page15/#perks-of-model-compression","text":"Smaller model sizes \u2014 Models can actually be stored into embedded devices (ESP32 has ~4Mb of Flash Memory) Faster Prediction Rates \u2014 This speeds up actionability, which provides viability for real-time decisions. Lowered power consumption \u2014 An often overlooked feature as environments that train models often come with a constant supply of power, embedded devices usually run on a battery thus this is the most important feature.","title":"Perks of Model Compression"},{"location":"page15/#hidden-perks-of-model-compression","text":"You might be quick to think that reducing the amount of information we store for each weight, would always be detrimental to our model, however, quantization promotes generalization which was a huge plus in preventing overfitting \u2014 a common problem with complex models.By Jerome Friedman, the father of gradient boost, empirical evidence shows that lots of small steps in the right direction result in better predictions with test data. By quantization, it is possible to get an improved accuracy due to the decreased sensitivity of the weights. Imagine if, in our dataset, we get lucky, every time we try to detect a cookie our dataset shows us a chocolate chip cookie, our cookie detection would get a high training accuracy, however, if in real-life we only have raisin cookies, it would have a low test accuracy. Generalization is like blurring our chocolate chip so that our model realizes as long as there is this blob, it is a cookie.The same can be said for other compression methods such as pruning. It is in this vein whereby dropout also can improve unseen accuracy as randomly dropping nodes during training promotes generalization as well.","title":"Hidden Perks of Model Compression"},{"location":"page15/#lets-try-it-out","text":"import os import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Flatten from tensorflow.keras.layers import Dense from tensorflow.keras.losses import SparseCategoricalCrossentropy from sklearn.metrics import accuracy_score print ( tf . __version__ ) def get_file_size(file_path): size = os.path.getsize(file_path) return size def convert_bytes(size, unit=None): if unit == \"KB\": return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes') elif unit == \"MB\": return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes') else: return print('File size: ' + str(size) + ' bytes')","title":"Let\u2019s try it out!"},{"location":"page15/#import-the-fasion-mnist-dataset","text":"fashion_mnist = keras.datasets.fashion_mnist (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","title":"Import the fasion MNIST dataset"},{"location":"page15/#explore-the-data","text":"train_images.shape len(train_labels) np.unique(train_labels) test_images.shape len(test_labels)","title":"Explore the data"},{"location":"page15/#preprocessing","text":"plt.figure() plt.imshow(train_images[88]) plt.colorbar() plt.grid(False) plt.show() train_images = train_images / 255.0 test_images = test_images / 255.0 model = keras.Sequential([ Flatten(input_shape=(28, 28)), Dense(128, activation='relu'), Dense(10) ]) model.compile(optimizer='adam', loss= SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) model.fit(train_images, train_labels, epochs=15) KERAS_MODEL_NAME = \"tf_fashion.h5\" model.save(KERAS_MODEL_NAME)","title":"Preprocessing"},{"location":"page15/#inference-time-of-tensorflow-model","text":"model = tf.keras.models.load_model('tf_fashion.h5') _ = model.predict(test_images[:1]) start_time = time.time() output = model.predict(test_images[:1]) end_time = time.time() inference_time = end_time - start_time print('Inference time:', inference_time)","title":"Inference Time of tensorflow model"},{"location":"page15/#size-and-accuracy-of-the-model","text":"convert_bytes(get_file_size(KERAS_MODEL_NAME), \"MB\") test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2) print('\\nTest accuracy:', test_acc)","title":"Size and Accuracy of the model"},{"location":"page15/#tflite-model","text":"sTF_LITE_MODEL_FILE_NAME = \"tf_lite_model.tflite\" tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model) tflite_model = tf_lite_converter.convert() tflite_model_name = TF_LITE_MODEL_FILE_NAME open(tflite_model_name, \"wb\").write(tflite_model)","title":"Tflite Model"},{"location":"page15/#size-of-the-tflite-model","text":"convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"KB\")","title":"Size of the Tflite model"},{"location":"page15/#check-input-tensor-shape","text":"interpreter = tf.lite.Interpreter(model_path = TF_LITE_MODEL_FILE_NAME) input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() print(\"Input Shape:\", input_details[0]['shape']) print(\"Input Type:\", input_details[0]['dtype']) print(\"Output Shape:\", output_details[0]['shape']) print(\"Output Type:\", output_details[0]['dtype'])","title":"Check input tensor shape"},{"location":"page15/#resize-tensor-shape","text":"interpreter.resize_tensor_input(input_details[0]['index'], (10000, 28, 28)) interpreter.resize_tensor_input(output_details[0]['index'], (10000, 10)) interpreter.allocate_tensors() input_details = interpreter.get_input_details() output_details = interpreter.get_output_details() print(\"Input Shape:\", input_details[0]['shape']) print(\"Input Type:\", input_details[0]['dtype']) print(\"Output Shape:\", output_details[0]['shape']) print(\"Output Type:\", output_details[0]['dtype']) test_images.dtype test_imgs_numpy = np.array(test_images, dtype=np.float32) interpreter.set_tensor(input_details[0]['index'], test_imgs_numpy) interpreter.invoke() tflite_model_predictions = interpreter.get_tensor(output_details[0]['index']) print(\"Prediction results shape:\", tflite_model_predictions.shape) prediction_classes = np.argmax(tflite_model_predictions, axis=1) interpreter = tf.lite.Interpreter(model_path='tf_lite_model.tflite') interpreter.allocate_tensors()","title":"Resize tensor shape"},{"location":"page15/#inference-time-of-the-tflite-model","text":"start_time = time.time() interpreter.invoke() end_time = time.time() inference_time = end_time - start_time inference_times.append(inference_time) print('Average inference time:', sum(inference_times) / len(inference_times))","title":"Inference time of the Tflite model"},{"location":"page15/#accuracy-of-the-tflite-model","text":"acc = accuracy_score(prediction_classes, test_labels) print('Test accuracy TFLITE model :', acc)","title":"Accuracy of the Tflite model"},{"location":"page15/#analysis-of-results","text":"","title":"Analysis of Results"},{"location":"page15/#pros-and-cons-of-tensorflowlite","text":"Pros: * Easier to implement model compression * Minimal effect on accuracy (Depending on model) * Major speed up in prediction Cons: * Requires the latest Tensorflow version 2 * Relatively new , Many operations (ops) are not supported yet such as SELU * Requires converting model which can fail * Possible complications when running inference compared to our good friend.predict() as it is more convoluted.","title":"Pros and Cons of TensorFlowLite"},{"location":"page15/#conclusion","text":"Despite its cons, TensorFlow Lite serves as a powerful tool with great potential that surpassed my expectations. I foresee in the near future, model compression being more widely used as the demand for AI in embedded devices inevitably grows, which gives TFLite a reason to provide greater operation coverage. With its shortcomings that can be mitigated by custom implementations, TensorFlow Lite for model compression is worth a shot.","title":"Conclusion"},{"location":"page16/","text":"Kernal fusion Kernel fusion refers to the process of combining multiple convolutional kernels or filters into a single kernel in a convolutional neural network (CNN) for more efficient computation. Kernel fusion can be applied to reduce the computational overhead associated with convolutional operations in CNNs. It aims to optimize the model's efficiency by merging multiple convolutional kernels into a single kernel that can perform the same feature extraction in a single operation, rather than applying multiple separate convolutional operations sequentially. This can reduce the number of operations required during the forward pass of the CNN, leading to faster inference times and reduced memory usage. Kernel fusion can be performed in different ways, depending on the specific architecture and requirements of the CNN. One common approach is to combine kernels that perform similar operations, such as those with the same size and stride, into a single kernel. This can be done by adding the weights of the original kernels together and normalizing the resulting weights to maintain the same scale. Another approach is to concatenate the original kernels along an additional dimension, creating a multi-channel kernel that can perform multiple convolutional operations in parallel. However, it's important to note that kernel fusion may also have some trade-offs. Merging multiple kernels into a single kernel can reduce the expressiveness of the model, as it may lose the ability to learn diverse and complex features. Additionally, kernel fusion may increase the risk of overfitting, as the combined kernel may have more parameters and be more prone to overfitting compared to individual kernels. We have applied our techniques for feature tracking on video images captured by a high speed digital video camera where the number of frames captured varies between 600-1000 frames per second. Image processing kernels are composed of multiple simple kernels, which executes on the input image in a given sequence. A set of kernels that can be fused together forms a partition (or fused kernel). Given a set of Kernels and the data dependencies between them, it is difficult to determine the partitions of kernels such that the total performance is maximized (execution time and throughput). We have developed and implemented an optimization model to find such a partition. We also developed an algorithm to fuse multiple kernels based on their data dependencies. Additionally, to further improve performance on GPGPU systems, we have provided methods to distribute data and threads to processors. Our model was able to reduce data traffic, which resulted better performance.The performance (both execution time and throughput) of the proposed method for kernel fusing and its subsequent execution is shown to be 2 to 3 times higher than executing kernels in sequence.","title":"Kernal fusion"},{"location":"page16/#kernal-fusion","text":"Kernel fusion refers to the process of combining multiple convolutional kernels or filters into a single kernel in a convolutional neural network (CNN) for more efficient computation. Kernel fusion can be applied to reduce the computational overhead associated with convolutional operations in CNNs. It aims to optimize the model's efficiency by merging multiple convolutional kernels into a single kernel that can perform the same feature extraction in a single operation, rather than applying multiple separate convolutional operations sequentially. This can reduce the number of operations required during the forward pass of the CNN, leading to faster inference times and reduced memory usage. Kernel fusion can be performed in different ways, depending on the specific architecture and requirements of the CNN. One common approach is to combine kernels that perform similar operations, such as those with the same size and stride, into a single kernel. This can be done by adding the weights of the original kernels together and normalizing the resulting weights to maintain the same scale. Another approach is to concatenate the original kernels along an additional dimension, creating a multi-channel kernel that can perform multiple convolutional operations in parallel. However, it's important to note that kernel fusion may also have some trade-offs. Merging multiple kernels into a single kernel can reduce the expressiveness of the model, as it may lose the ability to learn diverse and complex features. Additionally, kernel fusion may increase the risk of overfitting, as the combined kernel may have more parameters and be more prone to overfitting compared to individual kernels. We have applied our techniques for feature tracking on video images captured by a high speed digital video camera where the number of frames captured varies between 600-1000 frames per second. Image processing kernels are composed of multiple simple kernels, which executes on the input image in a given sequence. A set of kernels that can be fused together forms a partition (or fused kernel). Given a set of Kernels and the data dependencies between them, it is difficult to determine the partitions of kernels such that the total performance is maximized (execution time and throughput). We have developed and implemented an optimization model to find such a partition. We also developed an algorithm to fuse multiple kernels based on their data dependencies. Additionally, to further improve performance on GPGPU systems, we have provided methods to distribute data and threads to processors. Our model was able to reduce data traffic, which resulted better performance.The performance (both execution time and throughput) of the proposed method for kernel fusing and its subsequent execution is shown to be 2 to 3 times higher than executing kernels in sequence.","title":"Kernal fusion"},{"location":"page17/","text":"Embedded hardware JETSON NANO HARDWARE SPECIFICATION: The NVIDIA Jetson Nano is a small, powerful computer designed for embedded applications and AI projects. CPU: Quad-core ARM Cortex-A57 CPU @ 1.43 GHz GPU: 128-core NVIDIA Maxwell GPU Memory: 4 GB 64-bit LPDDR4 RAM @ 1600 MHz Storage: MicroSD card slot for storage expansion Video: 4K @ 30 fps (H.264/H.265) / 4K @ 60 fps (VP9) Interfaces: Gigabit Ethernet, HDMI 2.0, USB 3.0, USB 2.0, Micro-USB (power only) GPIO: 40-pin header with GPIO, I2C, I2S, SPI, UART interfaces Dimensions: 69.6 mm x 45 mm Overall, the Jetson Nano provides a powerful and efficient platform for running AI models and processing data in real-time, while remaining compact and affordable. PROCESSING POWER: The processing power of the NVIDIA Jetson Nano is quite impressive, especially considering its small size and low power consumption. CPU: The Jetson Nano features a quad-core ARM Cortex-A57 CPU running at 1.43 GHz. This provides a lot of processing power for general computing tasks and running the Linux operating system. GPU: The Jetson Nano's GPU is a 128-core NVIDIA Maxwell GPU, which is specifically designed for accelerating deep learning and AI tasks. This GPU provides a significant amount of parallel processing power, which can be used to accelerate tasks such as image recognition and natural language processing. MEMORY: The NVIDIA Jetson Nano has 4 GB of 64-bit LPDDR4 RAM running at 1600 MHz. This is a relatively large amount of memory for an embedded device, and it allows the Jetson Nano to handle complex tasks and large data sets with ease. Having 4 GB of RAM is especially useful for running deep learning models and other memory-intensive tasks, as it allows the Jetson Nano to load large data sets into memory and perform computations on them quickly. Additionally, LPDDR4 RAM is designed to be low-power, which is important for an embedded device that needs to operate on a limited power budget. The Jetson Nano's 4 GB of RAM is a significant amount of memory for an embedded device, and it helps to make the Jetson Nano a powerful platform for running AI models and processing large amounts of data in real-time. BENCH MARK: The benchmark performance of the NVIDIA Jetson Nano varies depending on the specific task and software being used. While running tensorflow, the Jetson Nano achieved a throughput of 1.3 images per second (IPS) while running a MobileNetV2 model for image classification. This is significantly faster than running the same model on a Raspberry Pi, which achieved a throughput of only 0.1 IPS. The Jetson Nano also performs well when running computer vision tasks using OpenCV. It achieved a frame rate of 23.6 frames per second (FPS) while running a facial recognition algorithm using OpenCV's Haar cascades. It is also well-suited for robotics applications, and can be used to run SLAM algorithms, object detection and tracking, and other real-time robotics tasks. In one benchmark, the Jetson Nano achieved a frame rate of 24 FPS while running an object detection and tracking algorithm using the YOLOv3 model. FEW AI APPLICATIONS THAT ARE RUN ON JETSON NANO: Object detection and recognition: Natural language processing Image and video processing: Autonomous navigation: RASPBERRY PI 4B HARDWARE SPECIFICATION: CPU: Broadcom BCM2711, quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz RAM: 2GB, 4GB, or 8GB LPDDR4-3200 SDRAM (depending on model) Connectivity: 2.4 GHz and 5.0 GHz IEEE 802.11b/g/n/ac wireless LAN Gigabit Ethernet Bluetooth 5.0 BLE (Bluetooth Low Energy) 2 USB 3.0 ports and 2 USB 2.0 ports Video and Sound: 2 micro-HDMI ports (up to 4Kp60 supported) 2-lane MIPI DSI display port 2-lane MIPI CSI camera port 4-pole stereo audio and composite video port Multimedia: H.265 (4Kp60 decode); H.264 (1080p60 decode, 1080p30 encode); OpenGL ES, 3.0 graphics Storage: MicroSD card slot for loading operating system and data storage GPIO: Standard 40-pin GPIO header, fully backwards-compatible with previous Raspberry Pi boards Power: 5V DC via USB-C connector (minimum 3A), or GPIO header (minimum 3A) Dimensions: 88 x 58 x 19.5 mm, 46 g The Raspberry Pi 4B is a powerful and versatile single-board computer that is suitable for a wide range of applications, from hobbyist projects to commercial products. Its high processing power, built-in connectivity options, and support for a wide range of software PROCESSING POWER: The Raspberry Pi 4B is a powerful single-board computer that can handle a wide range of computing tasks. CPU: The Raspberry Pi 4B is powered by a Broadcom BCM2711, quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz. This provides a significant amount of processing power for running complex algorithms and data-intensive tasks. GPU: The Raspberry Pi 4B includes a Broadcom VideoCore VI GPU, which is capable of rendering 4K video at 60 frames per second. It also supports OpenGL ES 3.0 graphics, making it well-suited for graphics-intensive applications. Memory: The Raspberry Pi 4B is available with 2GB, 4GB, or 8GB of LPDDR4-3200 SDRAM, which provides high bandwidth and low power consumption. The memory is shared between the CPU and GPU, allowing for efficient data transfer and processing. MEMORY: The Raspberry Pi 4B is available with three different RAM options: 2GB, 4GB, or 8GB LPDDR4-3200 SDRAM. The RAM on the Raspberry Pi 4B is shared between the CPU and the GPU, which allows for efficient data transfer and processing. The LPDDR4-3200 SDRAM is a high-bandwidth, low-power type of memory that helps to maximize the performance and efficiency of the Raspberry Pi 4B. FEW AI APPLICATIONS USED ON RASPBERRY PI 4B: Computer vision: Natural language processing: Robotics Machine learning: ARDUINO UNO HARDWARE SPECIFICATION: Microcontroller: ATmega328P Operating Voltage: 5V Input Voltage (recommended): 7-12V Input Voltage (limit): 6-20V Digital I/O Pins: 14 (of which 6 provide PWM output) PWM Digital I/O Pins: 6 Analog Input Pins: 6 DC Current per I/O Pin: 20 mA DC Current for 3.3V Pin: 50 mA Flash Memory: 32 KB (ATmega328P) SRAM: 2 KB (ATmega328P) EEPROM: 1 KB (ATmega328P) Clock Speed: 16 MHz The Arduino Uno board also includes a USB interface for programming and power, a power jack, an ICSP header, and a reset button. PROCESSING POWER: The processing power of the Arduino Uno is relatively low compared to other microcontroller boards. It is based on the ATmega328P microcontroller which has an 8-bit AVR architecture and operates at a clock speed of 16 MHz. It has 32 KB of flash memory, 2 KB of SRAM, and 1 KB of EEPROM. While the processing power of the Arduino Uno is not suitable for complex tasks like machine learning or image processing, it is capable of handling basic input/output operations and simple control tasks. It is ideal for projects that require a low-power and low-cost microcontroller board, such as controlling sensors or actuators, building simple robots, and creating interactive projects. Flash Memory: 32 KB (ATmega328P microcontroller) SRAM: 2 KB (ATmega328P microcontroller) EEPROM: 1 KB (ATmega328P microcontroller) The flash memory is used to store the program code that is uploaded to the board. The SRAM is used for storing variables and temporary data used during program execution. The EEPROM is used for storing data that needs to be retained even when the board is powered off, such as calibration data or user settings. AI APPLICATIONS THAT ARE RUN ON ARDUINO UNO: Due to its limited processing power and memory, the Arduino Uno is not well-suited for running complex artificial intelligence algorithms or models. However, it can still be used in a variety of AI-related applications for controlling sensors, collecting data, and interfacing with other devices. Here are a few examples: Machine learning on microcontrollers: Sensor data collection and analysis Robotics and automation: Smart home applications ARDUINO NANO HARDWARE SPECIFICATION: Microcontroller: ATmega328P Operating Voltage: 5V or 3.3V (depending on model) Input Voltage (recommended): 7-12V (VIN) or 5V (USB) Input Voltage (limit): 6-20V (VIN) or 5V (USB) Digital I/O Pins: 14 (of which 6 provide PWM output) PWM Digital I/O Pins: 6 Analog Input Pins: 8 DC Current per I/O Pin: 20 mA DC Current for 3.3V Pin: 50 mA Flash Memory: 32 KB (ATmega328P) SRAM: 2 KB (ATmega328P) EEPROM: 1 KB (ATmega328P) Clock Speed: 16 MHz PROCESSING POWER: The Arduino Nano is powered by an ATmega328P microcontroller, which has a clock speed of 16 MHz. This means that the Arduino Nano can execute up to 16 million instructions per second. The exact processing power of the Arduino Nano can vary depending on the specific application and the complexity of the program being run. However, in general, the Arduino Nano is capable of handling simple tasks like reading sensor data, controlling LEDs and motors, and communicating with other devices over serial or I2C protocols. It's important to note that the processing power of the Arduino Nano is relatively limited compared to other microcontrollers or single-board computers, so it may not be suitable for running more complex algorithms or applications. MEMORY: Flash Memory: 32 KB (ATmega328P) SRAM: 2 KB (ATmega328P) EEPROM: 1 KB (ATmega328P) The Flash memory is where the program code is stored. The 32 KB of Flash memory on the Arduino Nano allows for relatively complex programs to be stored on the board. The SRAM (Static Random Access Memory) is where the program data and variables are stored when the program is running. The 2 KB of SRAM on the Arduino Nano limits the amount of data that can be stored in the program memory at any given time. Therefore, it is important to optimize code to avoid excessive use of SRAM. The EEPROM (Electrically Erasable Programmable Read-Only Memory) is a non-volatile memory that can be used to store data that needs to be retained even when the power is turned off. The 1 KB of EEPROM on the Arduino Nano can be used to store data such as calibration values or user settings. AI APPLICATIONS: The Arduino Nano is a relatively low-powered microcontroller and does not have built-in machine learning capabilities. However, it can still be used in AI-related applications as part of a larger system or by using external modules and libraries. Sensor data processing Robotics Edge computing Wearable devices Education","title":"Embedded hardware"},{"location":"page17/#embedded-hardware","text":"","title":"Embedded hardware"},{"location":"page17/#jetson-nano","text":"","title":"JETSON NANO"},{"location":"page17/#hardware-specification","text":"The NVIDIA Jetson Nano is a small, powerful computer designed for embedded applications and AI projects. CPU: Quad-core ARM Cortex-A57 CPU @ 1.43 GHz GPU: 128-core NVIDIA Maxwell GPU Memory: 4 GB 64-bit LPDDR4 RAM @ 1600 MHz Storage: MicroSD card slot for storage expansion Video: 4K @ 30 fps (H.264/H.265) / 4K @ 60 fps (VP9) Interfaces: Gigabit Ethernet, HDMI 2.0, USB 3.0, USB 2.0, Micro-USB (power only) GPIO: 40-pin header with GPIO, I2C, I2S, SPI, UART interfaces Dimensions: 69.6 mm x 45 mm Overall, the Jetson Nano provides a powerful and efficient platform for running AI models and processing data in real-time, while remaining compact and affordable.","title":"HARDWARE SPECIFICATION:"},{"location":"page17/#processing-power","text":"The processing power of the NVIDIA Jetson Nano is quite impressive, especially considering its small size and low power consumption. CPU: The Jetson Nano features a quad-core ARM Cortex-A57 CPU running at 1.43 GHz. This provides a lot of processing power for general computing tasks and running the Linux operating system. GPU: The Jetson Nano's GPU is a 128-core NVIDIA Maxwell GPU, which is specifically designed for accelerating deep learning and AI tasks. This GPU provides a significant amount of parallel processing power, which can be used to accelerate tasks such as image recognition and natural language processing.","title":"PROCESSING POWER:"},{"location":"page17/#memory","text":"The NVIDIA Jetson Nano has 4 GB of 64-bit LPDDR4 RAM running at 1600 MHz. This is a relatively large amount of memory for an embedded device, and it allows the Jetson Nano to handle complex tasks and large data sets with ease. Having 4 GB of RAM is especially useful for running deep learning models and other memory-intensive tasks, as it allows the Jetson Nano to load large data sets into memory and perform computations on them quickly. Additionally, LPDDR4 RAM is designed to be low-power, which is important for an embedded device that needs to operate on a limited power budget. The Jetson Nano's 4 GB of RAM is a significant amount of memory for an embedded device, and it helps to make the Jetson Nano a powerful platform for running AI models and processing large amounts of data in real-time.","title":"MEMORY:"},{"location":"page17/#bench-mark","text":"The benchmark performance of the NVIDIA Jetson Nano varies depending on the specific task and software being used. While running tensorflow, the Jetson Nano achieved a throughput of 1.3 images per second (IPS) while running a MobileNetV2 model for image classification. This is significantly faster than running the same model on a Raspberry Pi, which achieved a throughput of only 0.1 IPS. The Jetson Nano also performs well when running computer vision tasks using OpenCV. It achieved a frame rate of 23.6 frames per second (FPS) while running a facial recognition algorithm using OpenCV's Haar cascades. It is also well-suited for robotics applications, and can be used to run SLAM algorithms, object detection and tracking, and other real-time robotics tasks. In one benchmark, the Jetson Nano achieved a frame rate of 24 FPS while running an object detection and tracking algorithm using the YOLOv3 model.","title":"BENCH MARK:"},{"location":"page17/#few-ai-applications-that-are-run-on-jetson-nano","text":"Object detection and recognition: Natural language processing Image and video processing: Autonomous navigation:","title":"FEW AI APPLICATIONS THAT ARE RUN ON JETSON NANO:"},{"location":"page17/#raspberry-pi-4b","text":"","title":"RASPBERRY PI 4B"},{"location":"page17/#hardware-specification_1","text":"CPU: Broadcom BCM2711, quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz RAM: 2GB, 4GB, or 8GB LPDDR4-3200 SDRAM (depending on model) Connectivity: 2.4 GHz and 5.0 GHz IEEE 802.11b/g/n/ac wireless LAN Gigabit Ethernet Bluetooth 5.0 BLE (Bluetooth Low Energy) 2 USB 3.0 ports and 2 USB 2.0 ports Video and Sound: 2 micro-HDMI ports (up to 4Kp60 supported) 2-lane MIPI DSI display port 2-lane MIPI CSI camera port 4-pole stereo audio and composite video port Multimedia: H.265 (4Kp60 decode); H.264 (1080p60 decode, 1080p30 encode); OpenGL ES, 3.0 graphics Storage: MicroSD card slot for loading operating system and data storage GPIO: Standard 40-pin GPIO header, fully backwards-compatible with previous Raspberry Pi boards Power: 5V DC via USB-C connector (minimum 3A), or GPIO header (minimum 3A) Dimensions: 88 x 58 x 19.5 mm, 46 g The Raspberry Pi 4B is a powerful and versatile single-board computer that is suitable for a wide range of applications, from hobbyist projects to commercial products. Its high processing power, built-in connectivity options, and support for a wide range of software","title":"HARDWARE SPECIFICATION:"},{"location":"page17/#processing-power_1","text":"The Raspberry Pi 4B is a powerful single-board computer that can handle a wide range of computing tasks. CPU: The Raspberry Pi 4B is powered by a Broadcom BCM2711, quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz. This provides a significant amount of processing power for running complex algorithms and data-intensive tasks. GPU: The Raspberry Pi 4B includes a Broadcom VideoCore VI GPU, which is capable of rendering 4K video at 60 frames per second. It also supports OpenGL ES 3.0 graphics, making it well-suited for graphics-intensive applications. Memory: The Raspberry Pi 4B is available with 2GB, 4GB, or 8GB of LPDDR4-3200 SDRAM, which provides high bandwidth and low power consumption. The memory is shared between the CPU and GPU, allowing for efficient data transfer and processing.","title":"PROCESSING POWER:"},{"location":"page17/#memory_1","text":"The Raspberry Pi 4B is available with three different RAM options: 2GB, 4GB, or 8GB LPDDR4-3200 SDRAM. The RAM on the Raspberry Pi 4B is shared between the CPU and the GPU, which allows for efficient data transfer and processing. The LPDDR4-3200 SDRAM is a high-bandwidth, low-power type of memory that helps to maximize the performance and efficiency of the Raspberry Pi 4B.","title":"MEMORY:"},{"location":"page17/#few-ai-applications-used-on-raspberry-pi-4b","text":"Computer vision: Natural language processing: Robotics Machine learning:","title":"FEW AI APPLICATIONS USED ON RASPBERRY PI 4B:"},{"location":"page17/#arduino-uno","text":"","title":"ARDUINO UNO"},{"location":"page17/#hardware-specification_2","text":"Microcontroller: ATmega328P Operating Voltage: 5V Input Voltage (recommended): 7-12V Input Voltage (limit): 6-20V Digital I/O Pins: 14 (of which 6 provide PWM output) PWM Digital I/O Pins: 6 Analog Input Pins: 6 DC Current per I/O Pin: 20 mA DC Current for 3.3V Pin: 50 mA Flash Memory: 32 KB (ATmega328P) SRAM: 2 KB (ATmega328P) EEPROM: 1 KB (ATmega328P) Clock Speed: 16 MHz The Arduino Uno board also includes a USB interface for programming and power, a power jack, an ICSP header, and a reset button.","title":"HARDWARE SPECIFICATION:"},{"location":"page17/#processing-power_2","text":"The processing power of the Arduino Uno is relatively low compared to other microcontroller boards. It is based on the ATmega328P microcontroller which has an 8-bit AVR architecture and operates at a clock speed of 16 MHz. It has 32 KB of flash memory, 2 KB of SRAM, and 1 KB of EEPROM. While the processing power of the Arduino Uno is not suitable for complex tasks like machine learning or image processing, it is capable of handling basic input/output operations and simple control tasks. It is ideal for projects that require a low-power and low-cost microcontroller board, such as controlling sensors or actuators, building simple robots, and creating interactive projects. Flash Memory: 32 KB (ATmega328P microcontroller) SRAM: 2 KB (ATmega328P microcontroller) EEPROM: 1 KB (ATmega328P microcontroller) The flash memory is used to store the program code that is uploaded to the board. The SRAM is used for storing variables and temporary data used during program execution. The EEPROM is used for storing data that needs to be retained even when the board is powered off, such as calibration data or user settings.","title":"PROCESSING POWER:"},{"location":"page17/#ai-applications-that-are-run-on-arduino-uno","text":"Due to its limited processing power and memory, the Arduino Uno is not well-suited for running complex artificial intelligence algorithms or models. However, it can still be used in a variety of AI-related applications for controlling sensors, collecting data, and interfacing with other devices. Here are a few examples: Machine learning on microcontrollers: Sensor data collection and analysis Robotics and automation: Smart home applications","title":"AI APPLICATIONS THAT ARE RUN ON ARDUINO UNO:"},{"location":"page17/#arduino-nano","text":"","title":"ARDUINO NANO"},{"location":"page17/#hardware-specification_3","text":"Microcontroller: ATmega328P Operating Voltage: 5V or 3.3V (depending on model) Input Voltage (recommended): 7-12V (VIN) or 5V (USB) Input Voltage (limit): 6-20V (VIN) or 5V (USB) Digital I/O Pins: 14 (of which 6 provide PWM output) PWM Digital I/O Pins: 6 Analog Input Pins: 8 DC Current per I/O Pin: 20 mA DC Current for 3.3V Pin: 50 mA Flash Memory: 32 KB (ATmega328P) SRAM: 2 KB (ATmega328P) EEPROM: 1 KB (ATmega328P) Clock Speed: 16 MHz","title":"HARDWARE SPECIFICATION:"},{"location":"page17/#processing-power_3","text":"The Arduino Nano is powered by an ATmega328P microcontroller, which has a clock speed of 16 MHz. This means that the Arduino Nano can execute up to 16 million instructions per second. The exact processing power of the Arduino Nano can vary depending on the specific application and the complexity of the program being run. However, in general, the Arduino Nano is capable of handling simple tasks like reading sensor data, controlling LEDs and motors, and communicating with other devices over serial or I2C protocols. It's important to note that the processing power of the Arduino Nano is relatively limited compared to other microcontrollers or single-board computers, so it may not be suitable for running more complex algorithms or applications.","title":"PROCESSING POWER:"},{"location":"page17/#memory_2","text":"Flash Memory: 32 KB (ATmega328P) SRAM: 2 KB (ATmega328P) EEPROM: 1 KB (ATmega328P) The Flash memory is where the program code is stored. The 32 KB of Flash memory on the Arduino Nano allows for relatively complex programs to be stored on the board. The SRAM (Static Random Access Memory) is where the program data and variables are stored when the program is running. The 2 KB of SRAM on the Arduino Nano limits the amount of data that can be stored in the program memory at any given time. Therefore, it is important to optimize code to avoid excessive use of SRAM. The EEPROM (Electrically Erasable Programmable Read-Only Memory) is a non-volatile memory that can be used to store data that needs to be retained even when the power is turned off. The 1 KB of EEPROM on the Arduino Nano can be used to store data such as calibration values or user settings.","title":"MEMORY:"},{"location":"page17/#ai-applications","text":"The Arduino Nano is a relatively low-powered microcontroller and does not have built-in machine learning capabilities. However, it can still be used in AI-related applications as part of a larger system or by using external modules and libraries. Sensor data processing Robotics Edge computing Wearable devices Education","title":"AI APPLICATIONS:"},{"location":"page18/","text":"TENSORFLOW In this session, we will examine several optimisation techniques, such as weight pruning, by training a tf.keras model from scratch for the MNIST dataset. This model will serve as the baseline for conversion to a tflite model. The major goal of this notebook is to comprehend tflite and other model optimisations, hence the modelling portion will be kept straightforward. 1.Importing necessary libraries import os import tempfile import numpy as np import tensorflow as tf from tensorflow import keras 2. LOAD MNIST DATASET mnist = tf.keras.datasets.mnist # the data, split between train and test sets (train_images, train_labels), (test_images, test_labels) = mnist.load_data() Normalize the input image so that each pixel value is between 0 and 1. train_images = train_images / 255.0 test_images = test_images / 255.0 Define the model architecture. def baseline_model(): model = tf.keras.Sequential([ tf.keras.layers.InputLayer(input_shape=(28, 28)), tf.keras.layers.Reshape(target_shape=(28, 28, 1)), tf.keras.layers.Conv2D(filters=12,kernel_size=(3, 3), activation=\"relu\"), tf.keras.layers.MaxPooling2D(pool_size=(2, 2)), tf.keras.layers.Flatten(), tf.keras.layers.Dense(10) ]) Train the digit classification model model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) return model 3.model = baseline_model() train the model for 4 epoch model.fit( train_images, train_labels, epochs=4, validation_split=0.1, ) 4._, baseline_model_accuracy = model.evaluate( test_images, test_labels, verbose=0) print('Baseline test accuracy:', baseline_model_accuracy) _, keras_file = tempfile.mkstemp('.h5') tf.keras.models.save_model(model, keras_file, include_optimizer=False) print('Saved baseline model to:', keras_file) 5.import os import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Flatten from tensorflow.keras.layers import Dense from tensorflow.keras.losses import SparseCategoricalCrossentropy from sklearn.metrics import accuracy_score import time CONVERTION PROCESS 6.######### Convert Keras model to TF Lite format.(32 bit) converter = tf.lite.TFLiteConverter.from_keras_model(model) tflite_float_model = converter.convert() # Show model size in KBs. float_model_size = len(tflite_float_model) / 1024 print('Float model size = %dKBs.' % float_model_size)#base->tflite=437 7.#Re-convert the model to TF Lite using quantization.(32->int 8) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_quantized_model = converter.convert() ## Show model size in KBs. quantized_model_size = len(tflite_quantized_model) / 1024 print('Quantized model size = %dKBs,' % quantized_model_size) print('which is about %d%% of the float model size.'\\ % (quantized_model_size * 100 / float_model_size)) 9.#save your model in the SavedModel format export_dir = 'saved_model/1' tf.saved_model.save(model, export_dir) Convert the model converter = tf.lite.TFLiteConverter.from_saved_model(export_dir) # path to the SavedModel directory tflite_model = converter.convert() Save the model. with open('model.tflite', 'wb') as f: f.write(tflite_model) 8.#Save the keras model after compiling model.save('model_keras.h5') model_keras= tf.keras.models.load_model('model_keras.h5') Converting a tf.Keras model to a TensorFlow Lite model. converter = tf.lite.TFLiteConverter.from_keras_model(model_keras) tflite_model = converter.convert() Save the model. with open('model.tflite', 'wb') as f: f.write(tflite_model) 9.!pip install -q tensorflow-model-optimization 10.import tensorflow_model_optimization as tfmot prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude PRUNING Compute end step to finish pruning after 2 epochs. batch_size = 128 epochs = 2 validation_split = 0.1 ## 10% of training set will be used for validation set. num_images = train_images.shape[0] * (1 - validation_split) end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs Define model for pruning. pruning_params = { 'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.80, begin_step=0, end_step=end_step) } model_for_pruning = prune_low_magnitude(model, **pruning_params) prune_low_magnitude requires a recompile. model_for_pruning.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) model_for_pruning.summary() 11.## fine tuning for 2 epochs logdir = tempfile.mkdtemp() callbacks = [ tfmot.sparsity.keras.UpdatePruningStep(), tfmot.sparsity.keras.PruningSummaries(log_dir=logdir), ] model_for_pruning.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=callbacks) 12.# For this dataset, there is minimal loss in test accuracy after pruning, compared to the baseline model . _, model_for_pruning_accuracy = model_for_pruning.evaluate( test_images, test_labels, verbose=0) print('Baseline test accuracy:', baseline_model_accuracy) print('Pruned test accuracy:', model_for_pruning_accuracy) 12.model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning) _, pruned_keras_file = tempfile.mkstemp('.h5') tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False) print('Saved pruned Keras model to:', pruned_keras_file) 13.converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export) pruned_tflite_model = converter.convert() _, pruned_tflite_file = tempfile.mkstemp('.tflite') with open(pruned_tflite_file, 'wb') as f: f.write(pruned_tflite_model) print('Saved pruned TFLite model to:', pruned_tflite_file) 14.# standard compression def get_gzipped_model_size(file): # Returns size of gzipped model, in bytes. import os import zipfile _, zipped_file = tempfile.mkstemp('.zip') with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f: f.write(file) return os.path.getsize(zipped_file) 15.# from the ouptut we can see that the model size is reduced to 3x from baseline model print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file))) print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file))) print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file))) 16.converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export) converter.optimizations = [tf.lite.Optimize.DEFAULT] quantized_and_pruned_tflite_model = converter.convert() _, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite') with open(quantized_and_pruned_tflite_file, 'wb') as f: f.write(quantized_and_pruned_tflite_model) print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file) print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file))) print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file))) print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file))) 17.#helper function to evaluate tflite model on the test dataset def eval_model(interpreter): input_index = interpreter.get_input_details()[0][\"index\"] output_index = interpreter.get_output_details()[0][\"index\"] # Run predictions on every image in the \"test\" dataset. prediction_digits = [] for i, test_image in enumerate(test_images): if i % 1000 == 0: print('Evaluated on {n} results so far.'.format(n=i)) # Pre-processing: add batch dimension and convert to float32 to match with # the model's input data format. test_image = np.expand_dims(test_image, axis=0).astype(np.float32) interpreter.set_tensor(input_index, test_image) # Run inference. interpreter.invoke() # Post-processing: remove batch dimension and find the digit with highest # probability. output = interpreter.tensor(output_index) digit = np.argmax(output()[0]) prediction_digits.append(digit) output_details = interpreter.get_output_details() print(output_details) print('\\n') # Compare prediction results with ground truth labels to calculate accuracy. prediction_digits = np.array(prediction_digits) accuracy = (prediction_digits == test_labels).mean() return accuracy 18.interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model) interpreter.allocate_tensors() test_accuracy = eval_model(interpreter) # print('Baseline test accuracy:', baseline_model_accuracy) print('Pruned TF test accuracy:', model_for_pruning_accuracy) print('Pruned and quantized TFLite test_accuracy:', test_accuracy)","title":"TENSORFLOW"},{"location":"page18/#tensorflow","text":"In this session, we will examine several optimisation techniques, such as weight pruning, by training a tf.keras model from scratch for the MNIST dataset. This model will serve as the baseline for conversion to a tflite model. The major goal of this notebook is to comprehend tflite and other model optimisations, hence the modelling portion will be kept straightforward. 1.Importing necessary libraries import os import tempfile import numpy as np import tensorflow as tf from tensorflow import keras 2. LOAD MNIST DATASET mnist = tf.keras.datasets.mnist # the data, split between train and test sets (train_images, train_labels), (test_images, test_labels) = mnist.load_data()","title":"TENSORFLOW"},{"location":"page18/#normalize-the-input-image-so-that-each-pixel-value-is-between-0-and-1","text":"train_images = train_images / 255.0 test_images = test_images / 255.0","title":"Normalize the input image so that each pixel value is between 0 and 1."},{"location":"page18/#define-the-model-architecture","text":"def baseline_model(): model = tf.keras.Sequential([ tf.keras.layers.InputLayer(input_shape=(28, 28)), tf.keras.layers.Reshape(target_shape=(28, 28, 1)), tf.keras.layers.Conv2D(filters=12,kernel_size=(3, 3), activation=\"relu\"), tf.keras.layers.MaxPooling2D(pool_size=(2, 2)), tf.keras.layers.Flatten(), tf.keras.layers.Dense(10) ])","title":"Define the model architecture."},{"location":"page18/#train-the-digit-classification-model","text":"model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) return model 3.model = baseline_model()","title":"Train the digit classification model"},{"location":"page18/#train-the-model-for-4-epoch","text":"model.fit( train_images, train_labels, epochs=4, validation_split=0.1, ) 4._, baseline_model_accuracy = model.evaluate( test_images, test_labels, verbose=0) print('Baseline test accuracy:', baseline_model_accuracy) _, keras_file = tempfile.mkstemp('.h5') tf.keras.models.save_model(model, keras_file, include_optimizer=False) print('Saved baseline model to:', keras_file) 5.import os import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.layers import Flatten from tensorflow.keras.layers import Dense from tensorflow.keras.losses import SparseCategoricalCrossentropy from sklearn.metrics import accuracy_score import time CONVERTION PROCESS 6.######### Convert Keras model to TF Lite format.(32 bit) converter = tf.lite.TFLiteConverter.from_keras_model(model) tflite_float_model = converter.convert() # Show model size in KBs. float_model_size = len(tflite_float_model) / 1024 print('Float model size = %dKBs.' % float_model_size)#base->tflite=437 7.#Re-convert the model to TF Lite using quantization.(32->int 8) converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_quantized_model = converter.convert() ## Show model size in KBs. quantized_model_size = len(tflite_quantized_model) / 1024 print('Quantized model size = %dKBs,' % quantized_model_size) print('which is about %d%% of the float model size.'\\ % (quantized_model_size * 100 / float_model_size)) 9.#save your model in the SavedModel format export_dir = 'saved_model/1' tf.saved_model.save(model, export_dir)","title":"train the model for 4 epoch"},{"location":"page18/#convert-the-model","text":"converter = tf.lite.TFLiteConverter.from_saved_model(export_dir) # path to the SavedModel directory tflite_model = converter.convert()","title":"Convert the model"},{"location":"page18/#save-the-model","text":"with open('model.tflite', 'wb') as f: f.write(tflite_model) 8.#Save the keras model after compiling model.save('model_keras.h5') model_keras= tf.keras.models.load_model('model_keras.h5')","title":"Save the model."},{"location":"page18/#converting-a-tfkeras-model-to-a-tensorflow-lite-model","text":"converter = tf.lite.TFLiteConverter.from_keras_model(model_keras) tflite_model = converter.convert()","title":"Converting a tf.Keras model to a TensorFlow Lite model."},{"location":"page18/#save-the-model_1","text":"with open('model.tflite', 'wb') as f: f.write(tflite_model) 9.!pip install -q tensorflow-model-optimization 10.import tensorflow_model_optimization as tfmot prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude PRUNING","title":"Save the model."},{"location":"page18/#compute-end-step-to-finish-pruning-after-2-epochs","text":"batch_size = 128 epochs = 2 validation_split = 0.1 ## 10% of training set will be used for validation set. num_images = train_images.shape[0] * (1 - validation_split) end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs","title":"Compute end step to finish pruning after 2 epochs."},{"location":"page18/#define-model-for-pruning","text":"pruning_params = { 'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.80, begin_step=0, end_step=end_step) } model_for_pruning = prune_low_magnitude(model, **pruning_params)","title":"Define model for pruning."},{"location":"page18/#prune_low_magnitude-requires-a-recompile","text":"model_for_pruning.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy']) model_for_pruning.summary() 11.## fine tuning for 2 epochs logdir = tempfile.mkdtemp() callbacks = [ tfmot.sparsity.keras.UpdatePruningStep(), tfmot.sparsity.keras.PruningSummaries(log_dir=logdir), ] model_for_pruning.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=callbacks) 12.# For this dataset, there is minimal loss in test accuracy after pruning, compared to the baseline model . _, model_for_pruning_accuracy = model_for_pruning.evaluate( test_images, test_labels, verbose=0) print('Baseline test accuracy:', baseline_model_accuracy) print('Pruned test accuracy:', model_for_pruning_accuracy) 12.model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning) _, pruned_keras_file = tempfile.mkstemp('.h5') tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False) print('Saved pruned Keras model to:', pruned_keras_file) 13.converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export) pruned_tflite_model = converter.convert() _, pruned_tflite_file = tempfile.mkstemp('.tflite') with open(pruned_tflite_file, 'wb') as f: f.write(pruned_tflite_model) print('Saved pruned TFLite model to:', pruned_tflite_file) 14.# standard compression def get_gzipped_model_size(file): # Returns size of gzipped model, in bytes. import os import zipfile _, zipped_file = tempfile.mkstemp('.zip') with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f: f.write(file) return os.path.getsize(zipped_file) 15.# from the ouptut we can see that the model size is reduced to 3x from baseline model print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file))) print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file))) print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file))) 16.converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export) converter.optimizations = [tf.lite.Optimize.DEFAULT] quantized_and_pruned_tflite_model = converter.convert() _, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite') with open(quantized_and_pruned_tflite_file, 'wb') as f: f.write(quantized_and_pruned_tflite_model) print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file) print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file))) print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file))) print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file))) 17.#helper function to evaluate tflite model on the test dataset def eval_model(interpreter): input_index = interpreter.get_input_details()[0][\"index\"] output_index = interpreter.get_output_details()[0][\"index\"] # Run predictions on every image in the \"test\" dataset. prediction_digits = [] for i, test_image in enumerate(test_images): if i % 1000 == 0: print('Evaluated on {n} results so far.'.format(n=i)) # Pre-processing: add batch dimension and convert to float32 to match with # the model's input data format. test_image = np.expand_dims(test_image, axis=0).astype(np.float32) interpreter.set_tensor(input_index, test_image) # Run inference. interpreter.invoke() # Post-processing: remove batch dimension and find the digit with highest # probability. output = interpreter.tensor(output_index) digit = np.argmax(output()[0]) prediction_digits.append(digit) output_details = interpreter.get_output_details() print(output_details) print('\\n') # Compare prediction results with ground truth labels to calculate accuracy. prediction_digits = np.array(prediction_digits) accuracy = (prediction_digits == test_labels).mean() return accuracy 18.interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model) interpreter.allocate_tensors() test_accuracy = eval_model(interpreter) # print('Baseline test accuracy:', baseline_model_accuracy) print('Pruned TF test accuracy:', model_for_pruning_accuracy) print('Pruned and quantized TFLite test_accuracy:', test_accuracy)","title":"prune_low_magnitude requires a recompile."},{"location":"page19/","text":"CAFFE Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR) and by community contributors. Yangqing Jia created the project during his PhD at UC Berkeley. Caffe is released under the BSD 2-Clause license. WHY CAFFE? Expressive architecture encourages application and innovation. Models and optimization are defined by configuration without hard-coding. Switch between CPU and GPU by setting a single flag to train on a GPU machine then deploy to commodity clusters or mobile devices. Extensible code fosters active development. In Caffe\u2019s first year, it has been forked by over 1,000 developers and had many significant changes contributed back. Thanks to these contributors the framework tracks the state-of-the-art in both code and models. Speed makes Caffe perfect for research experiments and industry deployment. Caffe can process over 60M images per day with a single NVIDIA K40 GPU*. That\u2019s 1 ms/image for inference and 4 ms/image for learning and more recent library versions and hardware are faster still. We believe that Caffe is among the fastest convnet implementations available. HOW CAFFE WORKS? Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework that is used for various machine learning tasks, including image classification, segmentation, and object detection. Here's a brief overview of how Caffe works: Data preparation: Caffe requires the input data to be in a specific format. The data is typically preprocessed to ensure that it meets the requirements of the framework. Network definition: Caffe uses a domain-specific language called \"Caffe Model Definition\" to define the neural network architecture. This language provides a way to specify the layers of the network, their types, and their parameters. Network training: Once the network is defined, Caffe uses backpropagation to train the model. The optimization algorithm minimizes the error between the predicted output and the actual output. Testing and validation: After training, the network is tested on a separate dataset to evaluate its accuracy and performance. Caffe is designed to be fast and efficient, and it can be used on both CPUs and GPUs. The framework has a large community of users and developers, and it is widely used in both academia and industry for a variety of applications. Some key features of Caffe include: Modularity: Caffe is designed with modularity in mind, making it easy to define and customize the neural network architecture by stacking different layers. Performance: Caffe is optimized for both CPU and GPU usage and is designed to be fast and efficient. It supports parallelization and can handle large datasets. Pre-trained models: Caffe comes with a wide range of pre-trained models that can be used for various tasks, such as image recognition and object detection. Community: Caffe has a large and active community of users and developers who contribute to the development of the framework and provide support and guidance to new users. Caffe has been widely adopted in both academia and industry and is used for a variety of applications, including image and speech recognition, natural language processing, and autonomous driving. Caffe framework provides few optimization techniques to enchance the performance of the model GPU support: Caffe is designed to take advantage of GPUs to speed up training and inference. It uses CUDA (Compute Unified Device Architecture) to parallelize computations across multiple GPU devices. Multi-GPU support: Caffe supports multi-GPU training, allowing users to split the training process across multiple GPUs to reduce the training time. Distributed training: Caffe can also be used for distributed training across multiple machines. This allows for larger models and datasets to be trained. Memory optimization: Caffe provides several memory optimization techniques, such as memory pooling and shared memory, to reduce the memory requirements of the network. Quantization: Caffe supports quantization, which reduces the precision of the network's parameters to reduce memory and computation requirements. Model compression: Caffe provides several techniques for model compression, such as pruning, to reduce the size of the network without sacrificing performance. Low-level optimization: Caffe's C++ implementation is designed to be fast and efficient, with low-level optimizations such as loop unrolling and SSE/AVX vectorization. By leveraging these optimization features, Caffe can train and deploy deep learning models more efficiently and with better performance.","title":"CAFFE"},{"location":"page19/#caffe","text":"Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR) and by community contributors. Yangqing Jia created the project during his PhD at UC Berkeley. Caffe is released under the BSD 2-Clause license.","title":"CAFFE"},{"location":"page19/#why-caffe","text":"Expressive architecture encourages application and innovation. Models and optimization are defined by configuration without hard-coding. Switch between CPU and GPU by setting a single flag to train on a GPU machine then deploy to commodity clusters or mobile devices. Extensible code fosters active development. In Caffe\u2019s first year, it has been forked by over 1,000 developers and had many significant changes contributed back. Thanks to these contributors the framework tracks the state-of-the-art in both code and models. Speed makes Caffe perfect for research experiments and industry deployment. Caffe can process over 60M images per day with a single NVIDIA K40 GPU*. That\u2019s 1 ms/image for inference and 4 ms/image for learning and more recent library versions and hardware are faster still. We believe that Caffe is among the fastest convnet implementations available.","title":"WHY CAFFE?"},{"location":"page19/#how-caffe-works","text":"Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework that is used for various machine learning tasks, including image classification, segmentation, and object detection. Here's a brief overview of how Caffe works: Data preparation: Caffe requires the input data to be in a specific format. The data is typically preprocessed to ensure that it meets the requirements of the framework. Network definition: Caffe uses a domain-specific language called \"Caffe Model Definition\" to define the neural network architecture. This language provides a way to specify the layers of the network, their types, and their parameters. Network training: Once the network is defined, Caffe uses backpropagation to train the model. The optimization algorithm minimizes the error between the predicted output and the actual output. Testing and validation: After training, the network is tested on a separate dataset to evaluate its accuracy and performance. Caffe is designed to be fast and efficient, and it can be used on both CPUs and GPUs. The framework has a large community of users and developers, and it is widely used in both academia and industry for a variety of applications.","title":"HOW CAFFE WORKS?"},{"location":"page19/#some-key-features-of-caffe-include","text":"Modularity: Caffe is designed with modularity in mind, making it easy to define and customize the neural network architecture by stacking different layers. Performance: Caffe is optimized for both CPU and GPU usage and is designed to be fast and efficient. It supports parallelization and can handle large datasets. Pre-trained models: Caffe comes with a wide range of pre-trained models that can be used for various tasks, such as image recognition and object detection. Community: Caffe has a large and active community of users and developers who contribute to the development of the framework and provide support and guidance to new users. Caffe has been widely adopted in both academia and industry and is used for a variety of applications, including image and speech recognition, natural language processing, and autonomous driving. Caffe framework provides few optimization techniques to enchance the performance of the model GPU support: Caffe is designed to take advantage of GPUs to speed up training and inference. It uses CUDA (Compute Unified Device Architecture) to parallelize computations across multiple GPU devices. Multi-GPU support: Caffe supports multi-GPU training, allowing users to split the training process across multiple GPUs to reduce the training time. Distributed training: Caffe can also be used for distributed training across multiple machines. This allows for larger models and datasets to be trained. Memory optimization: Caffe provides several memory optimization techniques, such as memory pooling and shared memory, to reduce the memory requirements of the network. Quantization: Caffe supports quantization, which reduces the precision of the network's parameters to reduce memory and computation requirements. Model compression: Caffe provides several techniques for model compression, such as pruning, to reduce the size of the network without sacrificing performance. Low-level optimization: Caffe's C++ implementation is designed to be fast and efficient, with low-level optimizations such as loop unrolling and SSE/AVX vectorization. By leveraging these optimization features, Caffe can train and deploy deep learning models more efficiently and with better performance.","title":"Some key features of Caffe include:"},{"location":"page20/","text":"RESNET 50 DATASET USED IMAGENET Let us discuss the demonstration on how the techniques allowed us to achieve 81.9% Top 1 Accuracy on ImageNet with ResNet50, outperforming pre-existing SOTA results. Choosing the best neural network training recipe for deep learning models is challenging. A paper called \u201cResnet Strikes Back\u201d demonstrated the significance of the right training for training ResNet50 on ImageNet. They boosted ResNet50 to a top-1 accuracy of 80.4% on ImageNet-1K. The original ResNet50 recipe reached 75.8% accuracy, so this improved. Improving upon resnet strike back: Knowledge Distillation. A training technique that trains small machine learning models to be as accurate as large models by transferring knowledge. Read more about knowledge distillation here. We apply EMA. A method that increases the stability of a model\u2019s convergence and helps it reach a better overall solution by preventing convergence to local minima. Weight averaging. Weight averaging is a post-training method that takes the best model weights across the training and averages them into a single model. We don\u2019t apply stochastic depth. Stochastic depth aims to shrink the depth of a network by randomly removing/deactivating residual blocks during training. We utilized several essential techniques outlined in this previous blog to improve the A1 recipe. Steps for training resnet 50: The training process we utilized comprises two key components: the data pipeline and the training hyperparameters. Combining these data pipeline strategies and training hyperparameters, we could achieve a remarkable Top 1 Accuracy of 81.9% on ImageNet with ResNet50. Reference:[https://deci.ai/blog/resnet50-how-to-achieve-sota-accuracy-on-imagenet/#:~:text=They%20boosted%20ResNet50%20to%20a,%25%20accuracy%2C%20so%20this%20improved.] MOBILENETV1 The original uncompressed MobileNet-v1's top-1 accuracy is 70.89%. We can adopt ChannelPrunedLearner to shrink the number of channels for convolutional layers to reduce the computation complexity. Instead of using the same pruning ratio for all layers, we utilize the DDPG algorithm as the RL agent to iteratively search for the optimal pruning ratio of each layer. After obtaining the optimal pruning ratios, group fine-tuning is adopted to further improve the compressed model's accuracy, as demonstrated below: We can adopt UniformQuantTFLearner to uniformly quantize model weights from 32-bit floating-point numbers to 8-bit fixed-point numbers. The resulting model can be converted into the TensorFlow Lite format for deployment on mobile devices. In the following two tables, we show that 8-bit quantized models can be as accurate as (or even better than) the original 32-bit ones, and the inference time can be significantly reduced after quantization. Reference: [https://pocketflow.github.io/performance/#:~:text=Note%3A%20The%20original%20uncompressed%20MobileNet,%2D1%20accuracy%20is%2070.89%25.] VGG 16 Limitations of VGG 16: ->It is very slow to train (the original VGG model was trained on Nvidia Titan GPU for 2-3 weeks). ->The size of VGG-16 trained imageNet weights is 528 MB. So, it takes quite a lot of disk space and bandwidth which makes it inefficient. ->138 million parameters lead to exploding gradients problem. ->Resnets are introduced to prevent exploding gradients problem that occurred in VGG-16. Challenges of VGG 16: ->It is very slow to train (the original VGG model was trained on the Nvidia Titan GPU for 2\u20133 weeks). ->The size of VGG-16 trained imageNet weights is 528 MB. So, it takes quite a lot of disk space and bandwidth that makes it inefficient. References Reference: 1. [https://pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/#:~:text=Even%20though%20ResNet%20is%20much,down%20to%20102MB%20for%20ResNet50.] 2. [https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918]","title":"RESNET 50"},{"location":"page20/#resnet-50","text":"DATASET USED IMAGENET Let us discuss the demonstration on how the techniques allowed us to achieve 81.9% Top 1 Accuracy on ImageNet with ResNet50, outperforming pre-existing SOTA results. Choosing the best neural network training recipe for deep learning models is challenging. A paper called \u201cResnet Strikes Back\u201d demonstrated the significance of the right training for training ResNet50 on ImageNet. They boosted ResNet50 to a top-1 accuracy of 80.4% on ImageNet-1K. The original ResNet50 recipe reached 75.8% accuracy, so this improved.","title":"RESNET 50"},{"location":"page20/#improving-upon-resnet-strike-back","text":"Knowledge Distillation. A training technique that trains small machine learning models to be as accurate as large models by transferring knowledge. Read more about knowledge distillation here. We apply EMA. A method that increases the stability of a model\u2019s convergence and helps it reach a better overall solution by preventing convergence to local minima. Weight averaging. Weight averaging is a post-training method that takes the best model weights across the training and averages them into a single model. We don\u2019t apply stochastic depth. Stochastic depth aims to shrink the depth of a network by randomly removing/deactivating residual blocks during training. We utilized several essential techniques outlined in this previous blog to improve the A1 recipe.","title":"Improving upon resnet strike back:"},{"location":"page20/#steps-for-training-resnet-50","text":"The training process we utilized comprises two key components: the data pipeline and the training hyperparameters. Combining these data pipeline strategies and training hyperparameters, we could achieve a remarkable Top 1 Accuracy of 81.9% on ImageNet with ResNet50. Reference:[https://deci.ai/blog/resnet50-how-to-achieve-sota-accuracy-on-imagenet/#:~:text=They%20boosted%20ResNet50%20to%20a,%25%20accuracy%2C%20so%20this%20improved.]","title":"Steps for training resnet 50:"},{"location":"page20/#mobilenetv1","text":"The original uncompressed MobileNet-v1's top-1 accuracy is 70.89%. We can adopt ChannelPrunedLearner to shrink the number of channels for convolutional layers to reduce the computation complexity. Instead of using the same pruning ratio for all layers, we utilize the DDPG algorithm as the RL agent to iteratively search for the optimal pruning ratio of each layer. After obtaining the optimal pruning ratios, group fine-tuning is adopted to further improve the compressed model's accuracy, as demonstrated below: We can adopt UniformQuantTFLearner to uniformly quantize model weights from 32-bit floating-point numbers to 8-bit fixed-point numbers. The resulting model can be converted into the TensorFlow Lite format for deployment on mobile devices. In the following two tables, we show that 8-bit quantized models can be as accurate as (or even better than) the original 32-bit ones, and the inference time can be significantly reduced after quantization. Reference: [https://pocketflow.github.io/performance/#:~:text=Note%3A%20The%20original%20uncompressed%20MobileNet,%2D1%20accuracy%20is%2070.89%25.]","title":"MOBILENETV1"},{"location":"page20/#vgg-16","text":"","title":"VGG 16"},{"location":"page20/#limitations-of-vgg-16","text":"->It is very slow to train (the original VGG model was trained on Nvidia Titan GPU for 2-3 weeks). ->The size of VGG-16 trained imageNet weights is 528 MB. So, it takes quite a lot of disk space and bandwidth which makes it inefficient. ->138 million parameters lead to exploding gradients problem. ->Resnets are introduced to prevent exploding gradients problem that occurred in VGG-16.","title":"Limitations of VGG 16:"},{"location":"page20/#challenges-of-vgg-16","text":"->It is very slow to train (the original VGG model was trained on the Nvidia Titan GPU for 2\u20133 weeks). ->The size of VGG-16 trained imageNet weights is 528 MB. So, it takes quite a lot of disk space and bandwidth that makes it inefficient. References Reference: 1. [https://pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/#:~:text=Even%20though%20ResNet%20is%20much,down%20to%20102MB%20for%20ResNet50.] 2. [https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918]","title":"Challenges of VGG 16:"}]}